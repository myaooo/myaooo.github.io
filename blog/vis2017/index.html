<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta property="og:title" class="next-head"/><meta property="og:description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><meta property="og:type" content="website" class="next-head"/><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous" class="next-head"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous" class="next-head"></script><script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous" class="next-head"></script><link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i|Open+Sans" rel="stylesheet" class="next-head"/><style type="text/css" class="next-head">
        body {
          font-family: &#x27;Lato&#x27;, &#x27;Open Sans&#x27;, &#x27;Noto Sans SC&#x27;, &quot;Microsoft YaHei&quot;, sans-serif;
        }
      </style><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-90189261-1" class="next-head"></script><script class="next-head">window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag("js", new Date());

        gtag("config", "UA-90189261-1");</script><title class="next-head">Yao Ming - A Trip to the West World -- Attending VIS 2017</title><meta charSet="utf-8" class="next-head"/><meta name="description" content="" class="next-head"/><meta property="og:title" content="A Trip to the West World -- Attending VIS 2017" class="next-head"/><meta property="og:description" content="" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:creator" content="Yao Ming" class="next-head"/><meta name="twitter:title" content="A Trip to the West World -- Attending VIS 2017" class="next-head"/><meta name="twitter:description" content="" class="next-head"/><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous" class="next-head"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous" class="next-head"></script><script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous" class="next-head"></script><link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i|Open+Sans" rel="stylesheet" class="next-head"/><style type="text/css" class="next-head">
        body {
          font-family: &#x27;Lato&#x27;, &#x27;Open Sans&#x27;, &#x27;Noto Sans SC&#x27;, &quot;Microsoft YaHei&quot;, sans-serif;
        }
      </style><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-90189261-1" class="next-head"></script><script class="next-head">window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag("js", new Date());

        gtag("config", "UA-90189261-1");</script><link rel="preload" href="/_next/static/muuSdRsUVbVhHbSv_LAzR/pages/_page.js" as="script"/><link rel="preload" href="/_next/static/muuSdRsUVbVhHbSv_LAzR/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.c0f58b2928ce9f6db19c.js" as="script"/><link rel="preload" href="/_next/static/chunks/styles.882e1f700f36a193eee0.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-7652b9adf9c4f1fdd86d.js" as="script"/><link rel="stylesheet" href="/_next/static/css/commons.d4b638be.chunk.css"/><link rel="stylesheet" href="/_next/static/css/styles.e5b74096.chunk.css"/><style id="__jsx-3606128814">.loader.jsx-3606128814{width:100%;height:100%;position:absolute;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background:#ffffff;z-index:1700;}.loader.jsx-3606128814 svg.jsx-3606128814{width:20%;height:20%;display:block;margin:auto;}.loader-show.jsx-3606128814{visibility:visible;-webkit-transition:opacity 300ms 300ms,visibility 0s 0s;transition:opacity 300ms 300ms,visibility 0s 0s;opacity:1;}.loader-hide.jsx-3606128814{visibility:hidden;-webkit-transition:opacity 300ms 300ms,visibility 0s 600ms;transition:opacity 300ms 300ms,visibility 0s 600ms;opacity:0;}</style></head><body><div id="__next"><div class="jsx-3606128814 loader loader-show"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" class="jsx-3606128814"><circle transform="translate(8 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle><circle transform="translate(16 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0.3" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle><circle transform="translate(24 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0.6" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle></svg></div><div style="display:none"><div><header id="m-header" class="fixed-top"><div class="m-header-container"><nav class="navbar navbar-expand-md navbar-light m-navbar"><span class="navbar-brand" href="#">Yao Ming <span style="font-size:14px">(明遥)</span></span><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto"><li class="nav-item"><a class="nav-link" data-nav-identifier="home" href="/"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" class="svg-inline--fa fa-home fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" style="margin-right:5px"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></i>Home</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="about" href="/#about"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="user-circle" class="svg-inline--fa fa-user-circle fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" style="margin-right:5px"><path fill="currentColor" d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"></path></svg></i>About</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="research" href="/#research"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="atom" class="svg-inline--fa fa-atom fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="margin-right:5px"><path fill="currentColor" d="M413.03 256c40.13-54.89 41.51-98.62 25.14-128-10.91-19.52-40.54-50.73-116.33-41.88C300.36 34.89 267.64 0 224 0s-76.36 34.89-97.84 86.12C50.43 77.34 20.73 108.48 9.83 128c-16.38 29.4-15 73.09 25.14 128-40.13 54.89-41.51 98.62-25.14 128 29.21 52.34 101.68 43.58 116.33 41.88C147.63 477.1 180.36 512 224 512s76.37-34.9 97.84-86.12c14.64 1.7 87.11 10.46 116.33-41.88 16.38-29.4 15-73.09-25.14-128zM63.38 352c-4.03-7.21-.19-24.8 14.95-48.29 6.96 6.53 14.2 12.89 21.87 19.18 1.71 13.71 4 27.08 6.76 40.08-24.56.89-39.89-4.37-43.58-10.97zm36.82-162.88c-7.66 6.29-14.9 12.65-21.87 19.18-15.13-23.5-18.97-41.09-14.95-48.3 3.41-6.14 16.39-11.47 37.92-11.47 1.71 0 3.87.3 5.69.37a472.191 472.191 0 0 0-6.79 40.22zM224 64c9.47 0 22.2 13.52 33.86 37.26-11.19 3.7-22.44 8-33.86 12.86-11.42-4.86-22.67-9.16-33.86-12.86C201.8 77.52 214.53 64 224 64zm0 384c-9.47 0-22.2-13.52-33.86-37.26 11.19-3.7 22.44-8 33.86-12.86 11.42 4.86 22.67 9.16 33.86 12.86C246.2 434.48 233.47 448 224 448zm62.5-157.33c-26.7 19.08-46.14 29.33-62.5 37.48-16.35-8.14-35.8-18.41-62.5-37.48-1.99-27.79-1.99-41.54 0-69.33 26.67-19.05 46.13-29.32 62.5-37.48 16.39 8.17 35.86 18.44 62.5 37.48 1.98 27.78 1.99 41.53 0 69.33zM384.62 352c-3.67 6.62-19 11.82-43.58 10.95 2.76-13 5.05-26.37 6.76-40.06 7.66-6.29 14.9-12.65 21.87-19.18 15.13 23.49 18.97 41.08 14.95 48.29zm-14.95-143.71c-6.96-6.53-14.2-12.89-21.87-19.18a473.535 473.535 0 0 0-6.79-40.22c1.82-.07 3.97-.37 5.69-.37 21.52 0 34.51 5.34 37.92 11.47 4.02 7.22.18 24.81-14.95 48.3zM224 224c-17.67 0-32 14.33-32 32s14.33 32 32 32 32-14.33 32-32-14.33-32-32-32z"></path></svg></i>Research</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="cv" href="/files/cv-yao.pdf">CV</a></li></ul></div></nav></div></header><div class="mb-4 container page-content" id="content"><main><div class="post-content"><article class="post" itemscope="" itemType="http://schema.org/Article"><header class="post-header"><h1>A Trip to the West World -- Attending VIS 2017</h1><div class="post-meta"><span><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-10-11</span></div></header><div class="post-body"><figure>
<img src="cactus.jpg" title="Eggs-like Cactus" alt="Eggs-like Cactus" /><figcaption>Eggs-like Cactus</figcaption>
</figure>
<p>This year, VIS was held in Phoenix, Arizona. As the name implies, Phoenix is a hot and dry city located in the Sonoran Desert.</p>
<p>This is my second year attending VIS. Unlike last year, I am able to present <a href="http://www.myaooo.com/rnnvis">a conference paper</a> on VAST this year. This is really a great experience for me. More importantly, it's really nice to have the opportunity to learn what others are doing in this community.</p>
<!-- more -->
<figure>
<img src="present.jpg" title="My Presentation" alt="My Presentation" /><figcaption>My Presentation</figcaption>
</figure>
<p>Let's start with some statistics. This year, VIS has accepted (39/170) papers in InfoVis, (37/173) papers in TVCG-track VAST, 15 papers in conference-track VAST (where mine is published), and (23/120) papers in SciVis. From simple math, we see here the acceptance rate for journal papers is (99/463=) 21.4%. Accounting for conference-track VAST papers, the rate is a little higher (24.6%). Though I felt a bit strange about this conference-track, the result that I am able to present my work in front of the community seems to be more important and more "impactful" than just a journal publication.</p>
<div id="acceptance" style="width: 400px; height: 300px; margin: auto;">

</div>
<hr />
<h1 id="visml">VIS+ML</h1>
<p>Time is always limited. I have to say I have the bias towards this topic during the conference, considering that I kind of fixed my research direction on it in the next few years.</p>
<p>Some said this year's VIS is a milestone for VIS+ML. We have VIS+ML tutorial, visual analytics for deep learning (VADL) workshop. We have 3 sessions in VAST and 1 session in InfoVis that titled with "ML". We have a panel discussing the influence of ML to our field. Indeed, the great advances in ML have influenced many other fields, including database, graphics, networks and many non-CS fields. Some even said that computer vision is somewhat like a sub-field of ML now (no offense).</p>
<p>Now, this tide is also coming into our field of visualization. In some aspect, such a huge rise of interest in ML in such a community like ours is not healthy. It restricted the research and development of many other important topics (see below). I totally agree with this point. But in my mind, research field develops and grows as we start to work on promising interdisciplinary new areas. With such powerful techniques coming out, it's not wise to close our doors and say we have all we need in our home (闭关锁国). Are these VIS+ML papers really useful and inspiring? Is VIS+ML really a significant and promising direction? We don't know yet. But I think, to refuse the new is definitely a wrong mindset. Research is about working on new and innovative things that are <strong>potentially</strong> useful. Definitely useful things must have been worked on by more resourceful organizations like companies. I believe we will get to know whether such a direction is non-sense or not in the next few years. But of course, VIS is a conference about visualization anyway. We don't want it to be flooded by "ML" papers (though I also have a contribution to this).</p>
<p>Now back to the conference. A few related events that I attended are listed below.</p>
<h2 id="visualization-the-secret-weapon-for-machine-learning-a-keynote-in-vds">Visualization: The Secret Weapon For Machine Learning (A keynote in VDS)</h2>
<p>The keynote is given by Fernanda Viegas and Martin Wattenberg at Google Inc.</p>
<p>They present several works of them at the Big Picture group at Google Brain and talked a few about the "People + AI" initiative at Google. Actually, I very much agree with their idea of introducing humans in the loop of AI, or the so-called "human-in-the-loop" AI. Autonomous systems have indeed improved the efficiency of many applications. But in the seeable future, a lot more applications cannot be fully automated and require humans to be in the loop. That is why we need VIS and HCI. Interestingly, the closing capstone of the conference is titled "Data Humanism".</p>
<p>One interesting thing is the insights that can be found by simply visualizing the datasets. They present the Facet that they recently released. Though I think the scalability is still an issue of their tool.</p>
<p>A few applications that Vis can be used in ML:</p>
<ul>
<li><p>debugging data set</p></li>
<li><p>debugging/understand models</p></li>
<li><p>education (teach ML models)</p></li>
<li><p>fairness</p></li>
</ul>
<h2 id="visml-symbiosis-of-visualization-and-machine-learning-tutorial">VIS+ML: Symbiosis of Visualization and Machine Learning (Tutorial)</h2>
<p>Something that caught my eye: An in-depth survey on how visualization can be used in augmenting embedding, presented by Yang Wang from Uber research.</p>
<h2 id="visual-analysis-for-deep-learning-workshop">Visual Analysis for Deep Learning (Workshop)</h2>
<p>Shixia Liu at Tsinghua University has given a talk about their recent work on this topic.</p>
<p>She presented the work from three possible applications of visualization for deep learning: Visual Understanding / Visual Diagnosis / Model Refinement. This taxonomy is actually the same as the one presented in their previously published survey paper.</p>
<p>In my mind, the three categories are for experts with DL knowledge. One significant part that this taxonomy is missing is visualization for end-users that are non-experts. Non-expert users of DL techniques can actually be a larger population. I believe visualization can make a larger impact out of this.</p>
<p>A research question that came to my mind during the talk:</p>
<p>How to remove the information of a region of the image more wisely? Current methods: 1) simply fill the region with gray/white 2) replace the region with random noise (uniform distribution)</p>
<h2 id="vis-panel-how-do-recent-ml-advances-impact-the-data-visualization-research-agenda">VIS Panel: How do Recent ML Advances Impact the Data Visualization Research Agenda?</h2>
<p>Q: Endangering the Human-in-the-loop Paradigm?</p>
<p>Q: Extending the Visualization Research Agenda?</p>
<p>Some interesting points that are raised by the panelists.</p>
<h3 id="min-chen-the-space-of-machine-learning">Min Chen: The space of Machine Learning</h3>
<p>I really appreciate his research on the foundation of visualization.</p>
<p>What he discussed on the computation of machine learning based on the scope of Universal Turing Machine is really inspiring. Though I am not sure if the results he presented has any theoretical proofs.</p>
<p>Articulating what current-state machines cannot do is a more solid argument for why we want to include human into the loop.</p>
<ul>
<li>Visually exploring the space of ML? Cannot remember what this is about...</li>
</ul>
<h3 id="visually-supporting-software-engineering-with-ml">Visually supporting software engineering with ML</h3>
<p>Not sure who discussed this idea (maybe Alexandru Telea). In the sense of current IT industry, ML is still in the scope of software engineering. Then, visualization used for supporting software engineering can be naturally extended to support ML. For example, quality assurance, testing...</p>
<h3 id="ross-maciejewski-why-open-the-box">Ross Maciejewski: Why Open the Box?</h3>
<p>Humans always have cognitive biases. So why we want to add bias to our system? Sometimes humans actually worsened the model's prediction.</p>
<p>He also talked about extending the concept of <strong>Algorithmic Aversion</strong> into this topic.</p>
<p>He actually raised a very important (in my mind, the most important) question for the <strong>Vis for ML</strong> topic.</p>
<p>Before we want to add human in the loop, we have to clearly consider why we want to add VIS for ML. Can VIS really be <strong>helpful</strong>? Or will humans make it worse?</p>
<p>I don't have an answer about it. I do agree that sometimes human interventions make things worse. But in many other scenarios, visualizations do help humans get insights. Let's see.</p>
<hr />
<figure>
<img src="sun_flower.jpg" title="Sunflowers in the botanic garden" alt="Sunflowers in the botanic garden" /><figcaption>Sunflowers in the botanic garden</figcaption>
</figure>
<h1 id="important-and-unsolved-problems-in-vast">Important and Unsolved Problems in VAST:</h1>
<p>A list summarized from Prof. Qu's notes and other random discussions:</p>
<ol type="1">
<li><p>How to quantify the visual complexity and cognitive loads?</p></li>
<li><p>How to formalize the framework of visual reasoning in VA?</p></li>
<li><p>How to make the process of visualization design more efficient (like using AI to automate and recommend visualization)?</p></li>
<li><p>How to better integrate the research results of InfoVis and cognition science to our design process using more autonomous systems? (credits to Dongyu)</p></li>
<li><p>*How to scale visualization to support really large dataset (GPU acceleration, distributed system)?</p></li>
</ol>
<hr />
<h1 id="presenting-at-conference">Presenting at Conference</h1>
<p>Presenting your own work at conferences is a good way to make impact. The day before my talk, I suddenly felt so nervous when practicing for the last time accompanied with Qiaomu and Wei. This is kind of normal. In Chinese education environments, shy students like me may not be sufficiently trained for public speaking. Then I practiced about another 4 times to make sure I can go through my presentation fluently. I have to say, this is my first time giving a talk in front of so many people in such a formal event. I believed there are more than 150 people at my talk. Considering "deep learning" is such a buzz word nowadays, it's not a surprise to see this happened. Fortunately, thanks to my labmates and Prof. Qu, I made a talk that at least satisfied myself. Cheers!</p>
<p>Some tips that I learned from conference talks:</p>
<ul>
<li>Conference talk is usually a good way to discuss your work with others. Try to make the most out of it.</li>
<li>Don't fill the presentation with the full content of the paper. Present 30% critical part of the paper clearly can give the audience a deeper impression.</li>
<li>Before the talk, practice at least a few times (for me it's 3) and make sure your talk and slides work together fluently.</li>
<li>Even if you have already done a rehearsal several days before, practice it for another time the day before the talk to make sure your memory is fresh.</li>
<li>Time matters. It's the basic respect that you follows the time schedule in a conference.</li>
<li>When giving the talk, make eye contacts with the audience (you can pick a few people that make you feel comfortable and look at them).</li>
<li>Remind your self to slow down your words. We (non-native speakers) may slur our words when speaking too fast, which may increase the nervousness.</li>
</ul>
<hr />
<h1 id="other-random-notes">Other Random Notes</h1>
<p>Following is some random notes that I took during the conference. Maybe irrelevant to the main idea of this blog.</p>
<h2 id="vahc-visual-analytics-in-healthcare">VAHC: Visual Analytics in Healthcare</h2>
<p>This year, the first event that I attended is VAHC. A keynote was presented by Hadi Kharrazi, a professor in public health at John Hopkins University. He mainly talked about the need for visualization in the field of population health. In a country like U.S., a large amount of funding (over 20% of the GDP) is allocated in healthcare. For example, in hospitals, electronic health records are now pervasive. But the data is not well utilized. The visualization / visual analytics in this field is more practical and domain-specific and can be directly used by or presented to decision makers. Some key points are summarized as follows:</p>
<ul>
<li>Feature reduction</li>
<li>Temporal data (how to deal with zero fills?)</li>
<li>Cognitive issues</li>
<li>Comparing models (models across different sub-populations)</li>
<li>Data quality (how it affects the analytics results/visualization)</li>
<li>Variety and volume of the data</li>
<li>data quality issues</li>
</ul>
<p>People in healthcare domain may prefer simple charts (bar charts) because they don't understand what VA can do.</p>
<p>Different people (clinical/policy makers/medical researcher) have different needs in the visualization.</p>
<h2 id="vds-visualization-in-data-science">VDS: Visualization in Data Science</h2>
<p>Professor Vasant Dhar at NYU gave a keynote talk about decision making with autonomous systems, with a focus on finance data.</p>
<p>Autonomous systems are increasingly used in many areas. In critical areas where human's knowledge are required during decision-making process (e.g. finance), one crucial question is whether we should trust a machine. In finance, dealers or traders may prefer a model that is more predictable (or, stable) with less return revenue. Visualization can be helpful in model evaluation and model monitoring.</p>
<h2 id="vast-sequence">VAST sequence</h2>
<h3 id="yuanzhes-paper">Yuanzhe's paper</h3>
<ul>
<li>Locality sensitive hashing</li>
<li>Presentation tips: embed videos inside ppt.</li>
<li>A good research problem should be general enough to have a broad range of applications.</li>
</ul>
<script type="text/javascript" src="https://unpkg.com/echarts@3.7.2/dist/echarts.min.js"></script>
<script async type="text/javascript" src="/blog/vis2017/bar-chart.js"></script>
</div></article><hr/><ul style="display:flex;flex-wrap:wrap;justify-content:space-between;list-style:none;padding:0"><li></li><li></li></ul></div></main></div><footer id="m-footer" class="bg-light py-2 d-print-none"><div class="container"><div class="row"><div class="col-auto mx-auto mr-sm-0 py-2"><ul class="socials" style="text-align:right"><li><a href="/#"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="font-size:13px"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></i></a></li><li><a href="https://github.com/myaooo"><i><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" style="font-size:13px"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></i></a></li><li><a href="https://scholar.google.com/citations?user=Fh0cwXUAAAAJ&amp;hl=en"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="graduation-cap" class="svg-inline--fa fa-graduation-cap fa-w-20 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" style="font-size:13px"><path fill="currentColor" d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg></i></a></li><li><a href="https://www.linkedin.com/in/yao-ming/"><i><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin" class="svg-inline--fa fa-linkedin fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="font-size:13px"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></i></a></li></ul></div><div class="col-auto copyright mx-auto ml-sm-0 py-2 order-sm-first">Copyright ©<span itemProp="copyrightYear"> <!-- -->2015 - 2020<!-- --> </span><span class="author" itemProp="copyrightHolder">Yao Ming<!-- -->. </span><span> All Rights Reserved.</span></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"page":{"frontmatter":{"title":"A Trip to the West World -- Attending VIS 2017","date":"2017-10-11T15:34:42.000Z","tags":"VIS, research, conference, VIS+ML"},"html":"\u003cfigure\u003e\n\u003cimg src=\"cactus.jpg\" title=\"Eggs-like Cactus\" alt=\"Eggs-like Cactus\" /\u003e\u003cfigcaption\u003eEggs-like Cactus\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThis year, VIS was held in Phoenix, Arizona. As the name implies, Phoenix is a hot and dry city located in the Sonoran Desert.\u003c/p\u003e\n\u003cp\u003eThis is my second year attending VIS. Unlike last year, I am able to present \u003ca href=\"http://www.myaooo.com/rnnvis\"\u003ea conference paper\u003c/a\u003e on VAST this year. This is really a great experience for me. More importantly, it's really nice to have the opportunity to learn what others are doing in this community.\u003c/p\u003e\n\u003c!-- more --\u003e\n\u003cfigure\u003e\n\u003cimg src=\"present.jpg\" title=\"My Presentation\" alt=\"My Presentation\" /\u003e\u003cfigcaption\u003eMy Presentation\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eLet's start with some statistics. This year, VIS has accepted (39/170) papers in InfoVis, (37/173) papers in TVCG-track VAST, 15 papers in conference-track VAST (where mine is published), and (23/120) papers in SciVis. From simple math, we see here the acceptance rate for journal papers is (99/463=) 21.4%. Accounting for conference-track VAST papers, the rate is a little higher (24.6%). Though I felt a bit strange about this conference-track, the result that I am able to present my work in front of the community seems to be more important and more \"impactful\" than just a journal publication.\u003c/p\u003e\n\u003cdiv id=\"acceptance\" style=\"width: 400px; height: 300px; margin: auto;\"\u003e\n\n\u003c/div\u003e\n\u003chr /\u003e\n\u003ch1 id=\"visml\"\u003eVIS+ML\u003c/h1\u003e\n\u003cp\u003eTime is always limited. I have to say I have the bias towards this topic during the conference, considering that I kind of fixed my research direction on it in the next few years.\u003c/p\u003e\n\u003cp\u003eSome said this year's VIS is a milestone for VIS+ML. We have VIS+ML tutorial, visual analytics for deep learning (VADL) workshop. We have 3 sessions in VAST and 1 session in InfoVis that titled with \"ML\". We have a panel discussing the influence of ML to our field. Indeed, the great advances in ML have influenced many other fields, including database, graphics, networks and many non-CS fields. Some even said that computer vision is somewhat like a sub-field of ML now (no offense).\u003c/p\u003e\n\u003cp\u003eNow, this tide is also coming into our field of visualization. In some aspect, such a huge rise of interest in ML in such a community like ours is not healthy. It restricted the research and development of many other important topics (see below). I totally agree with this point. But in my mind, research field develops and grows as we start to work on promising interdisciplinary new areas. With such powerful techniques coming out, it's not wise to close our doors and say we have all we need in our home (闭关锁国). Are these VIS+ML papers really useful and inspiring? Is VIS+ML really a significant and promising direction? We don't know yet. But I think, to refuse the new is definitely a wrong mindset. Research is about working on new and innovative things that are \u003cstrong\u003epotentially\u003c/strong\u003e useful. Definitely useful things must have been worked on by more resourceful organizations like companies. I believe we will get to know whether such a direction is non-sense or not in the next few years. But of course, VIS is a conference about visualization anyway. We don't want it to be flooded by \"ML\" papers (though I also have a contribution to this).\u003c/p\u003e\n\u003cp\u003eNow back to the conference. A few related events that I attended are listed below.\u003c/p\u003e\n\u003ch2 id=\"visualization-the-secret-weapon-for-machine-learning-a-keynote-in-vds\"\u003eVisualization: The Secret Weapon For Machine Learning (A keynote in VDS)\u003c/h2\u003e\n\u003cp\u003eThe keynote is given by Fernanda Viegas and Martin Wattenberg at Google Inc.\u003c/p\u003e\n\u003cp\u003eThey present several works of them at the Big Picture group at Google Brain and talked a few about the \"People + AI\" initiative at Google. Actually, I very much agree with their idea of introducing humans in the loop of AI, or the so-called \"human-in-the-loop\" AI. Autonomous systems have indeed improved the efficiency of many applications. But in the seeable future, a lot more applications cannot be fully automated and require humans to be in the loop. That is why we need VIS and HCI. Interestingly, the closing capstone of the conference is titled \"Data Humanism\".\u003c/p\u003e\n\u003cp\u003eOne interesting thing is the insights that can be found by simply visualizing the datasets. They present the Facet that they recently released. Though I think the scalability is still an issue of their tool.\u003c/p\u003e\n\u003cp\u003eA few applications that Vis can be used in ML:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003edebugging data set\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003edebugging/understand models\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eeducation (teach ML models)\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003efairness\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"visml-symbiosis-of-visualization-and-machine-learning-tutorial\"\u003eVIS+ML: Symbiosis of Visualization and Machine Learning (Tutorial)\u003c/h2\u003e\n\u003cp\u003eSomething that caught my eye: An in-depth survey on how visualization can be used in augmenting embedding, presented by Yang Wang from Uber research.\u003c/p\u003e\n\u003ch2 id=\"visual-analysis-for-deep-learning-workshop\"\u003eVisual Analysis for Deep Learning (Workshop)\u003c/h2\u003e\n\u003cp\u003eShixia Liu at Tsinghua University has given a talk about their recent work on this topic.\u003c/p\u003e\n\u003cp\u003eShe presented the work from three possible applications of visualization for deep learning: Visual Understanding / Visual Diagnosis / Model Refinement. This taxonomy is actually the same as the one presented in their previously published survey paper.\u003c/p\u003e\n\u003cp\u003eIn my mind, the three categories are for experts with DL knowledge. One significant part that this taxonomy is missing is visualization for end-users that are non-experts. Non-expert users of DL techniques can actually be a larger population. I believe visualization can make a larger impact out of this.\u003c/p\u003e\n\u003cp\u003eA research question that came to my mind during the talk:\u003c/p\u003e\n\u003cp\u003eHow to remove the information of a region of the image more wisely? Current methods: 1) simply fill the region with gray/white 2) replace the region with random noise (uniform distribution)\u003c/p\u003e\n\u003ch2 id=\"vis-panel-how-do-recent-ml-advances-impact-the-data-visualization-research-agenda\"\u003eVIS Panel: How do Recent ML Advances Impact the Data Visualization Research Agenda?\u003c/h2\u003e\n\u003cp\u003eQ: Endangering the Human-in-the-loop Paradigm?\u003c/p\u003e\n\u003cp\u003eQ: Extending the Visualization Research Agenda?\u003c/p\u003e\n\u003cp\u003eSome interesting points that are raised by the panelists.\u003c/p\u003e\n\u003ch3 id=\"min-chen-the-space-of-machine-learning\"\u003eMin Chen: The space of Machine Learning\u003c/h3\u003e\n\u003cp\u003eI really appreciate his research on the foundation of visualization.\u003c/p\u003e\n\u003cp\u003eWhat he discussed on the computation of machine learning based on the scope of Universal Turing Machine is really inspiring. Though I am not sure if the results he presented has any theoretical proofs.\u003c/p\u003e\n\u003cp\u003eArticulating what current-state machines cannot do is a more solid argument for why we want to include human into the loop.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVisually exploring the space of ML? Cannot remember what this is about...\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"visually-supporting-software-engineering-with-ml\"\u003eVisually supporting software engineering with ML\u003c/h3\u003e\n\u003cp\u003eNot sure who discussed this idea (maybe Alexandru Telea). In the sense of current IT industry, ML is still in the scope of software engineering. Then, visualization used for supporting software engineering can be naturally extended to support ML. For example, quality assurance, testing...\u003c/p\u003e\n\u003ch3 id=\"ross-maciejewski-why-open-the-box\"\u003eRoss Maciejewski: Why Open the Box?\u003c/h3\u003e\n\u003cp\u003eHumans always have cognitive biases. So why we want to add bias to our system? Sometimes humans actually worsened the model's prediction.\u003c/p\u003e\n\u003cp\u003eHe also talked about extending the concept of \u003cstrong\u003eAlgorithmic Aversion\u003c/strong\u003e into this topic.\u003c/p\u003e\n\u003cp\u003eHe actually raised a very important (in my mind, the most important) question for the \u003cstrong\u003eVis for ML\u003c/strong\u003e topic.\u003c/p\u003e\n\u003cp\u003eBefore we want to add human in the loop, we have to clearly consider why we want to add VIS for ML. Can VIS really be \u003cstrong\u003ehelpful\u003c/strong\u003e? Or will humans make it worse?\u003c/p\u003e\n\u003cp\u003eI don't have an answer about it. I do agree that sometimes human interventions make things worse. But in many other scenarios, visualizations do help humans get insights. Let's see.\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"sun_flower.jpg\" title=\"Sunflowers in the botanic garden\" alt=\"Sunflowers in the botanic garden\" /\u003e\u003cfigcaption\u003eSunflowers in the botanic garden\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch1 id=\"important-and-unsolved-problems-in-vast\"\u003eImportant and Unsolved Problems in VAST:\u003c/h1\u003e\n\u003cp\u003eA list summarized from Prof. Qu's notes and other random discussions:\u003c/p\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eHow to quantify the visual complexity and cognitive loads?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to formalize the framework of visual reasoning in VA?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to make the process of visualization design more efficient (like using AI to automate and recommend visualization)?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to better integrate the research results of InfoVis and cognition science to our design process using more autonomous systems? (credits to Dongyu)\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e*How to scale visualization to support really large dataset (GPU acceleration, distributed system)?\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr /\u003e\n\u003ch1 id=\"presenting-at-conference\"\u003ePresenting at Conference\u003c/h1\u003e\n\u003cp\u003ePresenting your own work at conferences is a good way to make impact. The day before my talk, I suddenly felt so nervous when practicing for the last time accompanied with Qiaomu and Wei. This is kind of normal. In Chinese education environments, shy students like me may not be sufficiently trained for public speaking. Then I practiced about another 4 times to make sure I can go through my presentation fluently. I have to say, this is my first time giving a talk in front of so many people in such a formal event. I believed there are more than 150 people at my talk. Considering \"deep learning\" is such a buzz word nowadays, it's not a surprise to see this happened. Fortunately, thanks to my labmates and Prof. Qu, I made a talk that at least satisfied myself. Cheers!\u003c/p\u003e\n\u003cp\u003eSome tips that I learned from conference talks:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConference talk is usually a good way to discuss your work with others. Try to make the most out of it.\u003c/li\u003e\n\u003cli\u003eDon't fill the presentation with the full content of the paper. Present 30% critical part of the paper clearly can give the audience a deeper impression.\u003c/li\u003e\n\u003cli\u003eBefore the talk, practice at least a few times (for me it's 3) and make sure your talk and slides work together fluently.\u003c/li\u003e\n\u003cli\u003eEven if you have already done a rehearsal several days before, practice it for another time the day before the talk to make sure your memory is fresh.\u003c/li\u003e\n\u003cli\u003eTime matters. It's the basic respect that you follows the time schedule in a conference.\u003c/li\u003e\n\u003cli\u003eWhen giving the talk, make eye contacts with the audience (you can pick a few people that make you feel comfortable and look at them).\u003c/li\u003e\n\u003cli\u003eRemind your self to slow down your words. We (non-native speakers) may slur our words when speaking too fast, which may increase the nervousness.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr /\u003e\n\u003ch1 id=\"other-random-notes\"\u003eOther Random Notes\u003c/h1\u003e\n\u003cp\u003eFollowing is some random notes that I took during the conference. Maybe irrelevant to the main idea of this blog.\u003c/p\u003e\n\u003ch2 id=\"vahc-visual-analytics-in-healthcare\"\u003eVAHC: Visual Analytics in Healthcare\u003c/h2\u003e\n\u003cp\u003eThis year, the first event that I attended is VAHC. A keynote was presented by Hadi Kharrazi, a professor in public health at John Hopkins University. He mainly talked about the need for visualization in the field of population health. In a country like U.S., a large amount of funding (over 20% of the GDP) is allocated in healthcare. For example, in hospitals, electronic health records are now pervasive. But the data is not well utilized. The visualization / visual analytics in this field is more practical and domain-specific and can be directly used by or presented to decision makers. Some key points are summarized as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFeature reduction\u003c/li\u003e\n\u003cli\u003eTemporal data (how to deal with zero fills?)\u003c/li\u003e\n\u003cli\u003eCognitive issues\u003c/li\u003e\n\u003cli\u003eComparing models (models across different sub-populations)\u003c/li\u003e\n\u003cli\u003eData quality (how it affects the analytics results/visualization)\u003c/li\u003e\n\u003cli\u003eVariety and volume of the data\u003c/li\u003e\n\u003cli\u003edata quality issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePeople in healthcare domain may prefer simple charts (bar charts) because they don't understand what VA can do.\u003c/p\u003e\n\u003cp\u003eDifferent people (clinical/policy makers/medical researcher) have different needs in the visualization.\u003c/p\u003e\n\u003ch2 id=\"vds-visualization-in-data-science\"\u003eVDS: Visualization in Data Science\u003c/h2\u003e\n\u003cp\u003eProfessor Vasant Dhar at NYU gave a keynote talk about decision making with autonomous systems, with a focus on finance data.\u003c/p\u003e\n\u003cp\u003eAutonomous systems are increasingly used in many areas. In critical areas where human's knowledge are required during decision-making process (e.g. finance), one crucial question is whether we should trust a machine. In finance, dealers or traders may prefer a model that is more predictable (or, stable) with less return revenue. Visualization can be helpful in model evaluation and model monitoring.\u003c/p\u003e\n\u003ch2 id=\"vast-sequence\"\u003eVAST sequence\u003c/h2\u003e\n\u003ch3 id=\"yuanzhes-paper\"\u003eYuanzhe's paper\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLocality sensitive hashing\u003c/li\u003e\n\u003cli\u003ePresentation tips: embed videos inside ppt.\u003c/li\u003e\n\u003cli\u003eA good research problem should be general enough to have a broad range of applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cscript type=\"text/javascript\" src=\"https://unpkg.com/echarts@3.7.2/dist/echarts.min.js\"\u003e\u003c/script\u003e\n\u003cscript async type=\"text/javascript\" src=\"/blog/vis2017/bar-chart.js\"\u003e\u003c/script\u003e\n"},"asPath":"/blog/vis2017/","userAgent":"server"}},"page":"/_page","query":{"fullUrl":"/blog/vis2017/"},"buildId":"muuSdRsUVbVhHbSv_LAzR","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/_page" src="/_next/static/muuSdRsUVbVhHbSv_LAzR/pages/_page.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/muuSdRsUVbVhHbSv_LAzR/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.c0f58b2928ce9f6db19c.js" async=""></script><script src="/_next/static/chunks/styles.882e1f700f36a193eee0.js" async=""></script><script src="/_next/static/runtime/main-7652b9adf9c4f1fdd86d.js" async=""></script></body></html>