<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><title class="next-head">Yao Ming | PhD in CSE | Researcher in XAI and VIS</title><meta charSet="utf-8" class="next-head"/><meta name="description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><meta property="og:title" class="next-head"/><meta property="og:description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:creator" content="Yao Ming" class="next-head"/><meta name="twitter:title" class="next-head"/><meta name="twitter:description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous" class="next-head"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous" class="next-head"></script><script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous" class="next-head"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript" class="next-head"></script><link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i|Open+Sans" rel="stylesheet" class="next-head"/><style type="text/css" class="next-head">
          body {
            font-family: &#x27;Lato&#x27;, &#x27;Open Sans&#x27;, &#x27;Noto Sans SC&#x27;, &quot;Microsoft YaHei&quot;, sans-serif;
          }
        </style><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-90189261-1" class="next-head"></script><script class="next-head">window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag("js", new Date());

        gtag("config", "UA-90189261-1");</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js" class="next-head"></script><script class="next-head">
AV.initialize("OV1AmD8AzeEzc0LSdxhxsRAj-gzGzoHsz", "oicMtAfktKjQbYrlz6nNWe9h");
function showTime(Counter) {
  var query = new AV.Query(Counter);
  var entries = [];
  var $visitors = $(".leancloud_visitors");

  $visitors.each(function () {
    entries.push( $(this).attr("id").trim() );
  });

  console.log(entries);
  query.containedIn('url', entries);
  query.find()
    .done(function (results) {
      var COUNT_CONTAINER_REF = '.leancloud-visitors-count';
      console.log(results);

      if (results.length === 0) {
        $visitors.find(COUNT_CONTAINER_REF).text(0);
        return;
      }

      for (var i = 0; i < results.length; i++) {
        var item = results[i];
        var url = item.get('url');
        var time = item.get('time');
        var element = document.getElementById(url);

        $(element).find(COUNT_CONTAINER_REF).text(time);
      }
      for(var i = 0; i < entries.length; i++) {
        var url = entries[i];
        var element = document.getElementById(url);
        var countSpan = $(element).find(COUNT_CONTAINER_REF);
        if( countSpan.text() == '') {
          countSpan.text(0);
        }
      }
    })
    .fail(function (object, error) {
      console.log("Error: " + error.code + " " + error.message);
    });
}

function addCount(Counter) {
  var $visitors = $(".leancloud_visitors");
  var url = $visitors.attr('id').trim();
  var title = $visitors.attr('data-flag-title').trim();
  var query = new AV.Query(Counter);

  query.equalTo("url", url);
  query.find({
    success: function(results) {
      if (results.length > 0) {
        var counter = results[0];
        counter.fetchWhenSave(true);
        counter.increment("time");
        counter.save(null, {
          success: function(counter) {
            var $element = $(document.getElementById(url));
            $element.find('.leancloud-visitors-count').text(counter.get('time'));
          },
          error: function(counter, error) {
            console.log('Failed to save Visitor num, with error message: ' + error.message);
          }
        });
      } else {
        var newcounter = new Counter();
        /* Set ACL */
        var acl = new AV.ACL();
        acl.setPublicReadAccess(true);
        acl.setPublicWriteAccess(true);
        newcounter.setACL(acl);
        /* End Set ACL */
        newcounter.set("title", title);
        newcounter.set("url", url);
        newcounter.set("time", 1);
        newcounter.save(null, {
          success: function(newcounter) {
            var $element = $(document.getElementById(url));
            $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
          },
          error: function(newcounter, error) {
            console.log('Failed to create');
          }
        });
      }
    },
    error: function(error) {
      console.log('Error:' + error.code + " " + error.message);
    }
  });
}

$(function() {
  var Counter = AV.Object.extend("Counter");
  if ($('.leancloud_visitors').length == 1) {
    addCount(Counter);
  } else if ($('.post-title-link').length > 1) {
    showTime(Counter);
  }
});
      </script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5fb70e8120f5e1b5" class="next-head"></script><link rel="preload" href="/_next/static/nhMVKDhmaXRt2P_S1C6tN/pages/_page.js" as="script"/><link rel="preload" href="/_next/static/nhMVKDhmaXRt2P_S1C6tN/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.8b1d108f40b66c4ca6b6.js" as="script"/><link rel="preload" href="/_next/static/chunks/styles.a38c7b919a261ed7c094.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-7652b9adf9c4f1fdd86d.js" as="script"/><link rel="stylesheet" href="/_next/static/css/commons.fd000b21.chunk.css"/><link rel="stylesheet" href="/_next/static/css/styles.2c72b905.chunk.css"/><style id="__jsx-3606128814">.loader.jsx-3606128814{width:100%;height:100%;position:absolute;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background:#ffffff;z-index:1700;}.loader.jsx-3606128814 svg.jsx-3606128814{width:20%;height:20%;display:block;margin:auto;}.loader-show.jsx-3606128814{visibility:visible;-webkit-transition:opacity 300ms 300ms,visibility 0s 0s;transition:opacity 300ms 300ms,visibility 0s 0s;opacity:1;}.loader-hide.jsx-3606128814{visibility:hidden;-webkit-transition:opacity 300ms 300ms,visibility 0s 600ms;transition:opacity 300ms 300ms,visibility 0s 600ms;opacity:0;}</style></head><body><div id="__next"><div class="jsx-3606128814 loader loader-show"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" class="jsx-3606128814"><circle transform="translate(8 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle><circle transform="translate(16 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0.3" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle><circle transform="translate(24 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0.6" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle></svg></div><div style="display:none"><div><header id="m-header" class="fixed-top"><div class="m-header-container"><nav class="navbar navbar-expand-md navbar-light m-navbar"><span class="navbar-brand" href="#">Yao Ming <span style="font-size:14px">(明遥)</span></span><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto"><li class="nav-item"><a class="nav-link" data-nav-identifier="home" href="/"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" class="svg-inline--fa fa-home fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" style="margin-right:5px"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></i>Home</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="about" href="/#about"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="user-circle" class="svg-inline--fa fa-user-circle fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" style="margin-right:5px"><path fill="currentColor" d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"></path></svg></i>About</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="research" href="/#research"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="atom" class="svg-inline--fa fa-atom fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="margin-right:5px"><path fill="currentColor" d="M413.03 256c40.13-54.89 41.51-98.62 25.14-128-10.91-19.52-40.54-50.73-116.33-41.88C300.36 34.89 267.64 0 224 0s-76.36 34.89-97.84 86.12C50.43 77.34 20.73 108.48 9.83 128c-16.38 29.4-15 73.09 25.14 128-40.13 54.89-41.51 98.62-25.14 128 29.21 52.34 101.68 43.58 116.33 41.88C147.63 477.1 180.36 512 224 512s76.37-34.9 97.84-86.12c14.64 1.7 87.11 10.46 116.33-41.88 16.38-29.4 15-73.09-25.14-128zM63.38 352c-4.03-7.21-.19-24.8 14.95-48.29 6.96 6.53 14.2 12.89 21.87 19.18 1.71 13.71 4 27.08 6.76 40.08-24.56.89-39.89-4.37-43.58-10.97zm36.82-162.88c-7.66 6.29-14.9 12.65-21.87 19.18-15.13-23.5-18.97-41.09-14.95-48.3 3.41-6.14 16.39-11.47 37.92-11.47 1.71 0 3.87.3 5.69.37a472.191 472.191 0 0 0-6.79 40.22zM224 64c9.47 0 22.2 13.52 33.86 37.26-11.19 3.7-22.44 8-33.86 12.86-11.42-4.86-22.67-9.16-33.86-12.86C201.8 77.52 214.53 64 224 64zm0 384c-9.47 0-22.2-13.52-33.86-37.26 11.19-3.7 22.44-8 33.86-12.86 11.42 4.86 22.67 9.16 33.86 12.86C246.2 434.48 233.47 448 224 448zm62.5-157.33c-26.7 19.08-46.14 29.33-62.5 37.48-16.35-8.14-35.8-18.41-62.5-37.48-1.99-27.79-1.99-41.54 0-69.33 26.67-19.05 46.13-29.32 62.5-37.48 16.39 8.17 35.86 18.44 62.5 37.48 1.98 27.78 1.99 41.53 0 69.33zM384.62 352c-3.67 6.62-19 11.82-43.58 10.95 2.76-13 5.05-26.37 6.76-40.06 7.66-6.29 14.9-12.65 21.87-19.18 15.13 23.49 18.97 41.08 14.95 48.29zm-14.95-143.71c-6.96-6.53-14.2-12.89-21.87-19.18a473.535 473.535 0 0 0-6.79-40.22c1.82-.07 3.97-.37 5.69-.37 21.52 0 34.51 5.34 37.92 11.47 4.02 7.22.18 24.81-14.95 48.3zM224 224c-17.67 0-32 14.33-32 32s14.33 32 32 32 32-14.33 32-32-14.33-32-32-32z"></path></svg></i>Research</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="blog" href="/blog"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="margin-right:5px"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></i>Blog</a></li></ul></div></nav></div></header><main id="main" class="main"><div class="mb-4 container page-content" id="content"><div id="posts" class="post-wrapper"><article class="post" itemscope="" itemType="http://schema.org/Article" tags="EMNLP 2020, Research, Explainable AI, Interpretability, Explainability"><header class="post-header"><h1><a class="post-title-link" href="/blog/emnlp2020/">A Few Interesting Papers on Explainability in EMNLP 2020</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2020-11-19</span><span class="post-meta-divider">|</span><span id="/blog/emnlp2020/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>I was attending EMNLP 2020 this year. It is an interesting experience attending such an academic conference virtually.</p>
<p>For me, one of the most exciting thing about EMNLP 2020 is the track called "Interpretability and Analysis of Models for NLP." According to the opening remarks, this track was introduced earlier in ACL 2020. And the submissions in this track almost doubled in EMNLP 2020. The following lists a few papers related to explainability that I found interesting and inspiring. However, this is nothing close to a complete list of all the good papers related to interpretability and explainability. The paper summaries below are my own understanding and opinion. They may be accurate :)</div><a href="/blog/emnlp2020/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="VIS, research, reflection"><header class="post-header"><h1><a class="post-title-link" href="/blog/vis2018/">Reflecting My Vis 2018 Submission</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2018-04-29</span><span class="post-meta-divider">|</span><span id="/blog/vis2018/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>It is my second time submitting a VIS paper. This year I visited NYU and coauthored the paper with <a href="http://enrico.bertini.io/">Prof. Bertini</a>. It's really exciting to work with Enrico and we are satisfied with what we have done so far. Hope there would be other opportunities to collaborate with him. Though it's not my first time working on the VIS deadline, this is still a fresh experience and I learned a lot from it.</p>
<figure>
<img src="/blog/vis2018/wtc.jpg" title="The West Field of WTC" alt="The West Field of WTC" /><figcaption>The West Field of WTC</figcaption>
</figure></div><a href="/blog/vis2018/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="research"><header class="post-header"><h1><a class="post-title-link" href="/blog/pqe-reflection/">Now, Call Me a Candidate</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-11-06</span><span class="post-meta-divider">|</span><span id="/blog/pqe-reflection/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><blockquote>
<p>Finally, I am a Ph.D. "candidate" now.</p>
</blockquote>
<p>This is the first thought came to my mind after I got passed the PQE.</p>
<blockquote>
<p>I have to throw myself into bed for two days.</p>
</blockquote>
<p>That is my second immediate thought. Honestly speaking, my PQE is far from a good one. My time management is like a disaster. Luckily, the oral presentation is finished as expected.</div><a href="/blog/pqe-reflection/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="VIS, research, conference, VIS+ML"><header class="post-header"><h1><a class="post-title-link" href="/blog/vis2017/">A Trip to the West World -- Attending VIS 2017</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-10-11</span><span class="post-meta-divider">|</span><span id="/blog/vis2017/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><figure>
<img src="/blog/vis2017/cactus.jpg" title="Eggs-like Cactus" alt="Eggs-like Cactus" /><figcaption>Eggs-like Cactus</figcaption>
</figure>
<p>This year, VIS was held in Phoenix, Arizona. As the name implies, Phoenix is a hot and dry city located in the Sonoran Desert.</p>
<p>This is my second year attending VIS. Unlike last year, I am able to present <a href="http://www.myaooo.com/rnnvis">a conference paper</a> on VAST this year. This is really a great experience for me. More importantly, it's really nice to have the opportunity to learn what others are doing in this community.</p></div><a href="/blog/vis2017/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="XAI, research, PQE"><header class="post-header"><h1><a class="post-title-link" href="/blog/PaperListforPQE/">The Paper List for My PQE</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-09-12</span><span class="post-meta-divider">|</span><span id="/blog/PaperListforPQE/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>This is a paper list that I summarized for my PQE.</p></div><a href="/blog/PaperListforPQE/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="XAI, research"><header class="post-header"><h1><a class="post-title-link" href="/blog/Towards/">Explainable AI -- Definition, Motivation and Application</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-08-17</span><span class="post-meta-divider">|</span><span id="/blog/Towards/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>I have been reading papers and articles and searching for ideas of my Ph.D. Qualification Exam (PQE) for a few days. Since I am interested in working on the interdisciplinary field of Visualization and Machine Learning, the idea of "explainable AI" (XAI) seems promising to me. After discussing with my professor, I decided to fixed the survey topic to "Visualization for Explainable Machine Learning". This blog summarizes my understanding on the motivation, scope and application of XAI.</p></div><a href="/blog/Towards/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="reading note, machine learning, research"><header class="post-header"><h1><a class="post-title-link" href="/blog/reading/">Power to the People - The role of Humans in Interactive Machine Learning</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-06-30</span><span class="post-meta-divider">|</span><span id="/blog/reading/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>This paper is written by Amershi, a researcher in MSA, who is kind of a leading researcher in the crossing area of ML + HCI.</p></div><a href="/blog/reading/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="machine learning, RNN"><header class="post-header"><h1><a class="post-title-link" href="/blog/on-rnns/">A summary on Recurrent Neural Networks (RNN)</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-02-10</span><span class="post-meta-divider">|</span><span id="/blog/on-rnns/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>I am working closely on RNNs these days, trying to reveal the ``black box'' and see what RNN learned to use its the hidden states and gates.</p>
<p>After intensively trying to do experiments, I suddenly realize that maybe first analyze them mathematically would give some clues for better visualization</p>
<p>There are already many good articles introducing RNNs and its variants (LSTMs, GRU) on the internet right now. So this is just a post for myself to summarize things up on RNNs.</p>
<h1 id="introduction">Introduction</h1>
<p>So first, what is Recurrent Neural Network (RNN)?</p>
<p>In short, RNN is a type of neural network that deal with sequence data. Classical neural networks, e.g. Multi-layer Perceptron (MLP) or Convolutional Neural Network (CNN) takes a fixed sized input an produce a fixed size output. Although for CNNs you can resize images of different size into a standard size so that the model can work with variable size input, but for the CNN part it still only accept fixed sized input.</div><a href="/blog/on-rnns/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="random"><header class="post-header"><h1><a class="post-title-link" href="/blog/this-is-get-serious/">This is Getting Serious</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-01-11</span><span class="post-meta-divider">|</span><span id="/blog/this-is-get-serious/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>I am thinking of starting a blog for a long time. And this kind of unable-to-finish-my-goals feeling is actually driving me ill.</p>
<p>And here, this blog serves as a FLAG noting I am going to start blogging "seriously". Don't mock me for my strange English, I am trying to make it serious here!</p>
<p>As for the stimulus that drives myself really starting this comes from Yuehan. Her suggestion of writing things down is actually working -- it kinds of saving my presentation today: Writing down things really helps me make sure I really understand what I want to presents. And it helps me clear my thoughts. People who writes logically and clearly must have the clear thoughts in mind, right?</p>
<p>I think I will keeps this habit from now on, for clearing my thoughts, recording my ideas, and exercising my English (Though I am not saying all of them will be in English :D).</p>
</div><a href="/blog/this-is-get-serious/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="course project"><header class="post-header"><h1><a class="post-title-link" href="/blog/TsinghuaJump/">Tsinghua Jump</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2016-02-24</span><span class="post-meta-divider">|</span><span id="/blog/TsinghuaJump/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><h1 id="introduction">Introduction</h1>
<p><a href="https://github.com/szmingyao/THU_Jump">Tsinghua Jump</a> is a doodle jump like game developed by our group Miaow. The game has similar rules as in Doodle Jump, but with many Tsinghua-flavored elements and interesting game items, which increased the game’s creational values. The game mainly has 2 game modes – Single Mode and Multi-Player Mode, a score-ranking system, and a game-item shop system.</p>
<p>The game is developed and maintained via Github. Link for the repository is here. I am the group leader as well as the main contributor. It is still being developed and maintained by myself.</p></div><a href="/blog/TsinghuaJump/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" tags="course-project"><header class="post-header"><h1><a class="post-title-link" href="/blog/RayTracing/">Ray Tracing — a toy project of computer graphics course</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2016-01-23</span><span class="post-meta-divider">|</span><span id="/blog/RayTracing/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><h1 id="introduction">Introduction</h1>
<p>Ray Tracing is a toy project for an undergraduate course: Advanced Computer Graphics in 2015 Fall at Tsinghua University. Ray Tracing is an implementation of classic ray tracing algorithm for photorealistic rendering. It is written in C++11, developed using vs2013 , with dependencies of openmp and opencv. You can view the repo <a href="https://github.com/szmingyao/RayTracing">here</a></p></div><a href="/blog/RayTracing/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" category="misc"><header class="post-header"><h1><a class="post-title-link" href="/blog/animated_poster/">Animated Posters - 2013 Student Festival</a></h1><div class="post-meta"><span class="post-meta-item"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2016-01-21</span><span class="post-meta-divider">|</span><span id="/blog/animated_poster/" class="post-meta-item leancloud_visitors" data-flag-title="[object Object]" title="Views"><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="eye" class="svg-inline--fa fa-eye fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M288 144a110.94 110.94 0 0 0-31.24 5 55.4 55.4 0 0 1 7.24 27 56 56 0 0 1-56 56 55.4 55.4 0 0 1-27-7.24A111.71 111.71 0 1 0 288 144zm284.52 97.4C518.29 135.59 410.93 64 288 64S57.68 135.64 3.48 241.41a32.35 32.35 0 0 0 0 29.19C57.71 376.41 165.07 448 288 448s230.32-71.64 284.52-177.41a32.35 32.35 0 0 0 0-29.19zM288 400c-98.65 0-189.09-55-237.93-144C98.91 167 189.34 112 288 112s189.09 55 237.93 144C477.1 345 386.66 400 288 400z"></path></svg></i>Views: <span class="leancloud-visitors-count"></span></span></div></header><div class="post-body"><p>In the 2013 winter term, I was in charge of the poster design of the student festival of our department. The student festival, "砼年不同Young", and the publicity of the festival have received a lot of attention on campus.</p>
<p>Inspired by famous movies, including V for Revenge, Iron Man, and etc., I designed a list of six animated posters, which have received a lot of views on the Internet. The prototype of these designs is 砼仔 (concrete boy, pronounced [təŋ tsai]).</p>
<p>The six posters are originally posted on <a href="http://page.renren.com/601834083/channel-albumshow-946535872">RenRen</a>. The posters received a total of 500,000 views with more than 50,000 views for each poster.</p>
<p>Guokr.com also invited us to re-post our posters on <a href="https://www.guokr.com/post/543545/">their site</a> and posted them on weibo, where the posters received more than 6,000 re-post and 1,000 likes.</p>
<p>The followings are the animating posters that I designed.</p>
<figure>
<img src="/blog/animated_poster/1900.gif" title="The Legend of 1900" alt="The Legend of 1900" /><figcaption>The Legend of 1900</figcaption>
</figure></div><a href="/blog/animated_poster/">Read more »</a></article></div></div></main><footer id="m-footer" class="bg-light py-2 d-print-none"><div class="container"><div class="row"><div class="col-auto mx-auto mr-sm-0 py-2"><ul class="socials" style="text-align:right"><li><a href="/#"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="font-size:13px"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></i></a></li><li><a href="https://github.com/myaooo"><i><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" style="font-size:13px"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></i></a></li><li><a href="https://scholar.google.com/citations?user=Fh0cwXUAAAAJ&amp;hl=en"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="graduation-cap" class="svg-inline--fa fa-graduation-cap fa-w-20 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" style="font-size:13px"><path fill="currentColor" d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg></i></a></li><li><a href="https://www.linkedin.com/in/yao-ming/"><i><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin" class="svg-inline--fa fa-linkedin fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="font-size:13px"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></i></a></li></ul></div><div class="col-auto copyright mx-auto ml-sm-0 py-2 order-sm-first">Copyright ©<span itemProp="copyrightYear"> <!-- -->2015 - 2020<!-- --> </span><span class="author" itemProp="copyrightHolder">Yao Ming<!-- -->. </span><span> All Rights Reserved.</span></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"page":{"source":"content/blog/index.md","path":"/blog/","url":"/blog/","content":"\n","frontmatter":{},"rootType":"blog","layout":"blog","pages":[{"source":"content/blog/emnlp2020/index.md","path":"/blog/emnlp2020/","url":"/blog/emnlp2020/","content":"\u003cp\u003eI was attending EMNLP 2020 this year. It is an interesting experience attending such an academic conference virtually.\u003c/p\u003e\n\u003cp\u003eFor me, one of the most exciting thing about EMNLP 2020 is the track called \"Interpretability and Analysis of Models for NLP.\" According to the opening remarks, this track was introduced earlier in ACL 2020. And the submissions in this track almost doubled in EMNLP 2020. The following lists a few papers related to explainability that I found interesting and inspiring. However, this is nothing close to a complete list of all the good papers related to interpretability and explainability. The paper summaries below are my own understanding and opinion. They may be accurate :) \u003ca id=\"more\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"a-diagnostic-study-of-explainability-techniques-for-text-classification\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.263/\"\u003eA Diagnostic Study of Explainability Techniques for Text Classification\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein\u003c/p\u003e\n\u003cp\u003eThe paper compiles a list of diagnostic properties with automatic measurement\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHuman Agreement. Degree of overlap between human-annotated saliency scores and the scores computed by an explainability technique \u003cspan class=\"math inline\"\u003e\\(E\\)\u003c/span\u003e for a given model \u003cspan class=\"math inline\"\u003e\\(M\\)\u003c/span\u003e. Mean AP is used for measuring human agreement.\u003c/li\u003e\n\u003cli\u003eConfidence Indication. A measure of the predictive power of a produced explanations for the confidence of a model on its predictions. First, the saliency distance (distance between the saliency scores of a predicted class \u003cspan class=\"math inline\"\u003e\\(k\\)\u003c/span\u003e and the other classes \u003cspan class=\"math inline\"\u003e\\(K/k\\)\u003c/span\u003e). The confidence score is regressed with linear regression to measure how well a model's confidence can be indicated from saliency values. The lower the mean-absolute error (MAE), the better the confidence indication is.\u003c/li\u003e\n\u003cli\u003eFaithfulness. Indicate how well the explanation aligns with the model's inner workings. Measured by masking the most salient words as per an explainability's saliency scores.\u003c/li\u003e\n\u003cli\u003eRationale Consistency. Similarity between the explanations v.s. similarity between the reasoning paths.\u003c/li\u003e\n\u003cli\u003eDataset Consistency. Similarity between the explanations v.s. similarity between the data points.\u003c/li\u003e\n\u003cli\u003eTime for Computing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePersonally, I am more interested in human agreement, confidence indication, and faithfulness. The two consistency metrics relies highly on the similarity measures we choose.\u003c/p\u003e\n\u003cp\u003eThe paper evaluates a set of models (CNN, LSTM, BERT) with different explainability techniques: * gradient-based: Saliency, gradient, guided backprop * perturbation-based: Occlusion, Shapley Value Sampling * simplification based: LIME\u003c/p\u003e\n\u003cp\u003eResults:\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/emnlp2020/./diagnostic-study-of-explainability.png\" alt=\"Experiment Results\" /\u003e\u003cfigcaption\u003eExperiment Results\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIn general, gradient-based methods perform the best for both human agreement and faithfulness. While LIME and perturbation-based methods (e.g., ShapSampl) have the highest scores on confidence indication.\u003c/p\u003e\n\u003ch2 id=\"how-do-decisions-emerge-across-layers-in-neural-models-interpretation-with-differentiable-masking\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.262/\"\u003eHow do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, Ivan Titov\u003c/p\u003e\n\u003cp\u003eObjective: explaining model predictions and internal mechanisms.\u003c/p\u003e\n\u003cp\u003eErasure-based explanation methods are model-agnostic and popular. However, they are often intractable due to the combinatorial computation complexity. The paper proposes a differentiable mask method to extract the faithful input attributions given the hidden states.\u003c/p\u003e\n\u003cp\u003eThe core idea of the method is to train a shallow interpretation mask model (or probe). The input of the masking model is the hidden states of up to l, and the output is a binary mask vector over the input x.\u003c/p\u003e\n\u003cp\u003eTo produce concise explanations (i.e., minimize non-zero masks), uses \u003cspan class=\"math inline\"\u003e\\(L_0\\)\u003c/span\u003e loss to promote sparsity and constrain the margin between model outputs from original input and masked input. Since this non-linear constrained optimization is intractable, the authors used Lagrangian relaxation.\u003c/p\u003e\n\u003cp\u003eTo overcome the challenge where L0 is discontinuous with zero derivative almost everywhere, the paper proposed to use stochastic masks and optimize the objective in expectation.\u003c/p\u003e\n\u003cp\u003eA sparse relaxation to binary variables is used (i.e., the hard concrete distribution, a mixed discrete-continuous distribution in the closed interval [0, 1]).\u003c/p\u003e\n\u003ch3 id=\"experiments\"\u003eExperiments\u003c/h3\u003e\n\u003cp\u003eThe paper conducted experiments on a toy task to validate the correctness of the method. The paper also applies the method to question answering and sentiment classification.\u003c/p\u003e\n\u003ch3 id=\"my-comments\"\u003eMy Comments\u003c/h3\u003e\n\u003cp\u003eErasure-based explanation is not new. The idea of probing is not new either. However, this paper presents a clean and clever solution to make probing-based masking differentiable and tractable. The results also looks decent.\u003c/p\u003e\n\u003cp\u003eOne minor shortcoming of this method is that it produces binary explanations. From human's perception, we would naturally apply weights when explaining things (i.e., part of the inputs would have higher importance than the rest).\u003c/p\u003e\n\u003ch2 id=\"explainable-clinical-decision-support-from-text\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.115/\"\u003eExplainable Clinical Decision Support from Text\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Jinyue Feng, Chantal Shaib, Frank Rudzicz\u003c/p\u003e\n\u003cp\u003eThis paper presents a practical solutions for explainable text classification in clinical decision making.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/emnlp2020/explainable-clinical-decision-support.png\" alt=\"Model\" /\u003e\u003cfigcaption\u003eModel\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe explanations are generated using the latent attention at the patient-level (i.e., the sentence-level). This make the explanations much more usable in practice than token-level ones.\u003c/p\u003e\n\u003ch2 id=\"f1-is-not-enough-models-and-evaluation-towards-user-centered-explainable-question-answering\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.575/\"\u003eF1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Hendrik Schuff, Heike Adel, Ngoc Thang Vu\u003c/p\u003e\n\u003cp\u003eA paper addressing the limitations of current explainable QA methods, including: * Silent facts. Facts are used by the model for the prediction but are not included in the explanation. * Unused facts. Facts are presented in the explanations but are not relevant to the prediction.\u003c/p\u003e\n\u003cp\u003eThe paper proposed a \"Select and Forget\" architecture, which basically forces the model to remove unused facts and only make predictions over related facts.\u003c/p\u003e\n\u003ch2 id=\"learning-variational-word-masks-to-improve-the-interpretability-of-neural-text-classifiers\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.347\"\u003eLearning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Hanjie Chen, Yangfeng Ji\u003c/p\u003e\n\u003ch2 id=\"multimodal-routing-improving-local-and-global-interpretability-of-multimodal-language-analysis\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.143\"\u003eMultimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Yao-Hung Hubert Tsai, Martin Ma, Muqiao Yang, Ruslan Salakhutdinov, Louis-Philippe Morency\u003c/p\u003e\n\u003cp\u003eUses capsule networks to to produce dynamically adjusted weights for different modals (i.e, audio, text, vision).\u003c/p\u003e\n\u003cp\u003eEncoding stage encodes inputs from multi-modal inputs to multimodal features. The routing stage iterates between routing adjustment and concepts update.\u003c/p\u003e\n","frontmatter":{"title":"A Few Interesting Papers on Explainability in EMNLP 2020","date":"2020-11-19T19:42:34.000Z","tags":"EMNLP 2020, Research, Explainable AI, Interpretability, Explainability"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Reflecting My Vis 2018 Submission","date":"2018-04-29T15:34:42.000Z","tags":"VIS, research, reflection"},"path":"/blog/vis2018/"},"next_page":null,"excerpt":"\u003cp\u003eI was attending EMNLP 2020 this year. It is an interesting experience attending such an academic conference virtually.\u003c/p\u003e\n\u003cp\u003eFor me, one of the most exciting thing about EMNLP 2020 is the track called \"Interpretability and Analysis of Models for NLP.\" According to the opening remarks, this track was introduced earlier in ACL 2020. And the submissions in this track almost doubled in EMNLP 2020. The following lists a few papers related to explainability that I found interesting and inspiring. However, this is nothing close to a complete list of all the good papers related to interpretability and explainability. The paper summaries below are my own understanding and opinion. They may be accurate :)","more":"\u003c/p\u003e\n\u003ch2 id=\"a-diagnostic-study-of-explainability-techniques-for-text-classification\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.263/\"\u003eA Diagnostic Study of Explainability Techniques for Text Classification\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein\u003c/p\u003e\n\u003cp\u003eThe paper compiles a list of diagnostic properties with automatic measurement\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHuman Agreement. Degree of overlap between human-annotated saliency scores and the scores computed by an explainability technique \u003cspan class=\"math inline\"\u003e\\(E\\)\u003c/span\u003e for a given model \u003cspan class=\"math inline\"\u003e\\(M\\)\u003c/span\u003e. Mean AP is used for measuring human agreement.\u003c/li\u003e\n\u003cli\u003eConfidence Indication. A measure of the predictive power of a produced explanations for the confidence of a model on its predictions. First, the saliency distance (distance between the saliency scores of a predicted class \u003cspan class=\"math inline\"\u003e\\(k\\)\u003c/span\u003e and the other classes \u003cspan class=\"math inline\"\u003e\\(K/k\\)\u003c/span\u003e). The confidence score is regressed with linear regression to measure how well a model's confidence can be indicated from saliency values. The lower the mean-absolute error (MAE), the better the confidence indication is.\u003c/li\u003e\n\u003cli\u003eFaithfulness. Indicate how well the explanation aligns with the model's inner workings. Measured by masking the most salient words as per an explainability's saliency scores.\u003c/li\u003e\n\u003cli\u003eRationale Consistency. Similarity between the explanations v.s. similarity between the reasoning paths.\u003c/li\u003e\n\u003cli\u003eDataset Consistency. Similarity between the explanations v.s. similarity between the data points.\u003c/li\u003e\n\u003cli\u003eTime for Computing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePersonally, I am more interested in human agreement, confidence indication, and faithfulness. The two consistency metrics relies highly on the similarity measures we choose.\u003c/p\u003e\n\u003cp\u003eThe paper evaluates a set of models (CNN, LSTM, BERT) with different explainability techniques: * gradient-based: Saliency, gradient, guided backprop * perturbation-based: Occlusion, Shapley Value Sampling * simplification based: LIME\u003c/p\u003e\n\u003cp\u003eResults:\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/emnlp2020/./diagnostic-study-of-explainability.png\" alt=\"Experiment Results\" /\u003e\u003cfigcaption\u003eExperiment Results\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eIn general, gradient-based methods perform the best for both human agreement and faithfulness. While LIME and perturbation-based methods (e.g., ShapSampl) have the highest scores on confidence indication.\u003c/p\u003e\n\u003ch2 id=\"how-do-decisions-emerge-across-layers-in-neural-models-interpretation-with-differentiable-masking\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.262/\"\u003eHow do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, Ivan Titov\u003c/p\u003e\n\u003cp\u003eObjective: explaining model predictions and internal mechanisms.\u003c/p\u003e\n\u003cp\u003eErasure-based explanation methods are model-agnostic and popular. However, they are often intractable due to the combinatorial computation complexity. The paper proposes a differentiable mask method to extract the faithful input attributions given the hidden states.\u003c/p\u003e\n\u003cp\u003eThe core idea of the method is to train a shallow interpretation mask model (or probe). The input of the masking model is the hidden states of up to l, and the output is a binary mask vector over the input x.\u003c/p\u003e\n\u003cp\u003eTo produce concise explanations (i.e., minimize non-zero masks), uses \u003cspan class=\"math inline\"\u003e\\(L_0\\)\u003c/span\u003e loss to promote sparsity and constrain the margin between model outputs from original input and masked input. Since this non-linear constrained optimization is intractable, the authors used Lagrangian relaxation.\u003c/p\u003e\n\u003cp\u003eTo overcome the challenge where L0 is discontinuous with zero derivative almost everywhere, the paper proposed to use stochastic masks and optimize the objective in expectation.\u003c/p\u003e\n\u003cp\u003eA sparse relaxation to binary variables is used (i.e., the hard concrete distribution, a mixed discrete-continuous distribution in the closed interval [0, 1]).\u003c/p\u003e\n\u003ch3 id=\"experiments\"\u003eExperiments\u003c/h3\u003e\n\u003cp\u003eThe paper conducted experiments on a toy task to validate the correctness of the method. The paper also applies the method to question answering and sentiment classification.\u003c/p\u003e\n\u003ch3 id=\"my-comments\"\u003eMy Comments\u003c/h3\u003e\n\u003cp\u003eErasure-based explanation is not new. The idea of probing is not new either. However, this paper presents a clean and clever solution to make probing-based masking differentiable and tractable. The results also looks decent.\u003c/p\u003e\n\u003cp\u003eOne minor shortcoming of this method is that it produces binary explanations. From human's perception, we would naturally apply weights when explaining things (i.e., part of the inputs would have higher importance than the rest).\u003c/p\u003e\n\u003ch2 id=\"explainable-clinical-decision-support-from-text\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.115/\"\u003eExplainable Clinical Decision Support from Text\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Jinyue Feng, Chantal Shaib, Frank Rudzicz\u003c/p\u003e\n\u003cp\u003eThis paper presents a practical solutions for explainable text classification in clinical decision making.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/emnlp2020/explainable-clinical-decision-support.png\" alt=\"Model\" /\u003e\u003cfigcaption\u003eModel\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe explanations are generated using the latent attention at the patient-level (i.e., the sentence-level). This make the explanations much more usable in practice than token-level ones.\u003c/p\u003e\n\u003ch2 id=\"f1-is-not-enough-models-and-evaluation-towards-user-centered-explainable-question-answering\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.575/\"\u003eF1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Hendrik Schuff, Heike Adel, Ngoc Thang Vu\u003c/p\u003e\n\u003cp\u003eA paper addressing the limitations of current explainable QA methods, including: * Silent facts. Facts are used by the model for the prediction but are not included in the explanation. * Unused facts. Facts are presented in the explanations but are not relevant to the prediction.\u003c/p\u003e\n\u003cp\u003eThe paper proposed a \"Select and Forget\" architecture, which basically forces the model to remove unused facts and only make predictions over related facts.\u003c/p\u003e\n\u003ch2 id=\"learning-variational-word-masks-to-improve-the-interpretability-of-neural-text-classifiers\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.347\"\u003eLearning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Hanjie Chen, Yangfeng Ji\u003c/p\u003e\n\u003ch2 id=\"multimodal-routing-improving-local-and-global-interpretability-of-multimodal-language-analysis\"\u003e\u003ca href=\"https://www.aclweb.org/anthology/2020.emnlp-main.143\"\u003eMultimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAuthors: Yao-Hung Hubert Tsai, Martin Ma, Muqiao Yang, Ruslan Salakhutdinov, Louis-Philippe Morency\u003c/p\u003e\n\u003cp\u003eUses capsule networks to to produce dynamically adjusted weights for different modals (i.e, audio, text, vision).\u003c/p\u003e\n\u003cp\u003eEncoding stage encodes inputs from multi-modal inputs to multimodal features. The routing stage iterates between routing adjustment and concepts update.\u003c/p\u003e"},{"source":"content/blog/vis2018/index.md","path":"/blog/vis2018/","url":"/blog/vis2018/","content":"\u003cp\u003eIt is my second time submitting a VIS paper. This year I visited NYU and coauthored the paper with \u003ca href=\"http://enrico.bertini.io/\"\u003eProf. Bertini\u003c/a\u003e. It's really exciting to work with Enrico and we are satisfied with what we have done so far. Hope there would be other opportunities to collaborate with him. Though it's not my first time working on the VIS deadline, this is still a fresh experience and I learned a lot from it.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2018/wtc.jpg\" title=\"The West Field of WTC\" alt=\"The West Field of WTC\" /\u003e\u003cfigcaption\u003eThe West Field of WTC\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003ch2 id=\"life-in-new-york\"\u003eLife in New York\u003c/h2\u003e\n\u003cp\u003eNew York is not very different from Hong Kong or Shenzhen, except its horrible subway system. During the stay in NYU, I lived in Jersey City, where I can afford for a good and private room. The neighborhood is actually very nice. There is also a Chinese supermarket in walking distance, where I can buy food and deserts that I enjoy.\u003c/p\u003e\n\u003cp\u003eIt takes about 40 minutes from home to the lab. Every morning, I take a PATH train to World Trade Center, where I then transit for MTA. During the day, the WTC is full of people, some of which are rushing for work, and others are traveling around taking photos. It's also an interesting experience to see how empty it is in 3 a.m.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2018/wtc_at_3.jpg\" title=\"3 a.m. in WTC\" alt=\"3 a.m. in WTC\" /\u003e\u003cfigcaption\u003e3 a.m. in WTC\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"teamwork\"\u003eTeamwork\u003c/h2\u003e\n\u003cp\u003eThis time I did the whole paper with Prof. Enrico and Prof. Qu. At the beginning, I thought it would be a good practice of my research skills if I did the whole paper by myself. However, through this experience, I realized how important teamwork and collaborations are. If I could do the submission another time, I would rather have some other students working together.\u003c/p\u003e\n\u003cp\u003eIt's annoying to switch my mind from coding to writing and then back. It's depressing when the project gets stuck and the meetings with professors are three days later. I was missing how I can discuss freely with someone any time things go wrong.\u003c/p\u003e\n\u003cp\u003eA trick I learned to overcome this negative mind is, when stuck, put down the work in hands and go out for a walk. And sometimes it turns out to be just a minor technical issue that we can bypass. 当局者迷，旁观者清。\u003c/p\u003e\n\u003cp\u003eThough my skills in writing, coding, and idea generations greatly improved after this submission, it's still a pity that my social skills and teamwork skills did not get the same improvements.\u003c/p\u003e\n\u003ch2 id=\"research-in-explainable-ai\"\u003eResearch in Explainable AI\u003c/h2\u003e\n\u003cp\u003eIt's also great that I can dig the field of explainable machine learning further using visualization. During my PQE, Prof. Qu raised a point: selling visualizations to ML/DL experts is hard; while selling it to managers is easy. I think this is a valid point for XAI research, and this is the point that drives my submission this year.\u003c/p\u003e\n\u003cp\u003eThe target users should not be limited to those who develop the system and the models. There is a significant demand to help people without knowledge in ML to understand/trust/use ML techniques. I think this point also accords to part of the goals of massive online education.\u003c/p\u003e\n\u003cp\u003eAutomated solutions powered by ML will be increasingly adopted in our daily life. How the algorithms make things right/wrong? Why? There will always be people interested in these questions, and VIS would be a critical and effective step towards the answers.\u003c/p\u003e\n\u003ch2 id=\"visualization\"\u003eVisualization\u003c/h2\u003e\n\u003cp\u003eI always enjoy making beautiful graphics. I think this is one of my driving powers for research in VIS. Apart from research, I felt a great sense of achievement by making them and having people like them.\u003c/p\u003e\n\u003cp\u003eFrom the past five years (I have been making posters for different student organizations during my undergraduate), I learned that I don't have much talent in design or art. But making graphics can be count as one of my habits. I would not feel tired of creating graphics for a whole day (but would feel tired of writing). I think it's very lucky for me to be able to join Prof. Qu's group to work on VIS.\u003c/p\u003e\n\u003ch2 id=\"coding\"\u003eCoding\u003c/h2\u003e\n\u003cp\u003eBy my own standards, my skill sets in coding improved a lot via this submission.\u003c/p\u003e\n\u003cp\u003eI learned how to write Python C extensions to glue code written in C for performance. I learned React and Typescript in front-end developments. I learned Docker when I need to set up demos to run user studies.\u003c/p\u003e\n\u003cp\u003eAnother lesson I learned is, don't change the framework or refactor the code if the time is limited. During the submission, I wasted about 5 days, changing the code from D3.js to pure React (use React to handle the rendering of SVG nodes) and then back to D3.js. When efficiency is crucial, it's worth sacrificing the readability gained from React.\u003c/p\u003e\n\u003cp\u003eTo build things with my own code is one of the most exciting things that I love in Computer Science.\u003c/p\u003e\n\u003ch2 id=\"things-i-learned-from-enrico-and-his-students\"\u003eThings I learned from Enrico and his students\u003c/h2\u003e\n\u003cp\u003eI learned a lot from Enrico during the collaboration. Enrico has his standards for what is good research. Though he is quite open to my own ideas, his standards helped me a lot in refining the direction and scope of this paper. I think a good measure for the value of an application research can be roughly formulated as: \u003cem\u003ethe potential value added to a user\u003c/em\u003e x \u003cem\u003ethe number of potential users\u003c/em\u003e. Prof. Qu also suggest that we should working on more important problems (而不应该去研究“茴”的四种写法).\u003c/p\u003e\n\u003cp\u003eOne important thing I learn from him is that, don't presume that something is obvious. As researchers, we work on our projects every day. Something clear to us might not be easy-to-understand to others. It's always He tried to understand the most detailed parts of my proposed method and pointed out things that he thought are fragile. This helps a lot for this submission.\u003c/p\u003e\n\u003cp\u003eAnother thing I learned from his group members is to stay critical. During meetings, they always actively raised points they felt confused or ideas they felt relevant. They would start from a question raised from a presentation and discuss freely for half an hour. Presenting our work and discuss it with people is the quickest way to improve it. It turns out these discussions are always valuable and would contribute to one's submission.\u003c/p\u003e\n","frontmatter":{"title":"Reflecting My Vis 2018 Submission","date":"2018-04-29T15:34:42.000Z","tags":"VIS, research, reflection"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Now, Call Me a Candidate","date":"2017-11-06T21:35:10.000Z","tags":"research"},"path":"/blog/pqe-reflection/"},"next_page":{"frontmatter":{"title":"A Few Interesting Papers on Explainability in EMNLP 2020","date":"2020-11-19T19:42:34.000Z","tags":"EMNLP 2020, Research, Explainable AI, Interpretability, Explainability"},"path":"/blog/emnlp2020/"},"excerpt":"\u003cp\u003eIt is my second time submitting a VIS paper. This year I visited NYU and coauthored the paper with \u003ca href=\"http://enrico.bertini.io/\"\u003eProf. Bertini\u003c/a\u003e. It's really exciting to work with Enrico and we are satisfied with what we have done so far. Hope there would be other opportunities to collaborate with him. Though it's not my first time working on the VIS deadline, this is still a fresh experience and I learned a lot from it.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2018/wtc.jpg\" title=\"The West Field of WTC\" alt=\"The West Field of WTC\" /\u003e\u003cfigcaption\u003eThe West Field of WTC\u003c/figcaption\u003e\n\u003c/figure\u003e","more":"\u003ch2 id=\"life-in-new-york\"\u003eLife in New York\u003c/h2\u003e\n\u003cp\u003eNew York is not very different from Hong Kong or Shenzhen, except its horrible subway system. During the stay in NYU, I lived in Jersey City, where I can afford for a good and private room. The neighborhood is actually very nice. There is also a Chinese supermarket in walking distance, where I can buy food and deserts that I enjoy.\u003c/p\u003e\n\u003cp\u003eIt takes about 40 minutes from home to the lab. Every morning, I take a PATH train to World Trade Center, where I then transit for MTA. During the day, the WTC is full of people, some of which are rushing for work, and others are traveling around taking photos. It's also an interesting experience to see how empty it is in 3 a.m.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2018/wtc_at_3.jpg\" title=\"3 a.m. in WTC\" alt=\"3 a.m. in WTC\" /\u003e\u003cfigcaption\u003e3 a.m. in WTC\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"teamwork\"\u003eTeamwork\u003c/h2\u003e\n\u003cp\u003eThis time I did the whole paper with Prof. Enrico and Prof. Qu. At the beginning, I thought it would be a good practice of my research skills if I did the whole paper by myself. However, through this experience, I realized how important teamwork and collaborations are. If I could do the submission another time, I would rather have some other students working together.\u003c/p\u003e\n\u003cp\u003eIt's annoying to switch my mind from coding to writing and then back. It's depressing when the project gets stuck and the meetings with professors are three days later. I was missing how I can discuss freely with someone any time things go wrong.\u003c/p\u003e\n\u003cp\u003eA trick I learned to overcome this negative mind is, when stuck, put down the work in hands and go out for a walk. And sometimes it turns out to be just a minor technical issue that we can bypass. 当局者迷，旁观者清。\u003c/p\u003e\n\u003cp\u003eThough my skills in writing, coding, and idea generations greatly improved after this submission, it's still a pity that my social skills and teamwork skills did not get the same improvements.\u003c/p\u003e\n\u003ch2 id=\"research-in-explainable-ai\"\u003eResearch in Explainable AI\u003c/h2\u003e\n\u003cp\u003eIt's also great that I can dig the field of explainable machine learning further using visualization. During my PQE, Prof. Qu raised a point: selling visualizations to ML/DL experts is hard; while selling it to managers is easy. I think this is a valid point for XAI research, and this is the point that drives my submission this year.\u003c/p\u003e\n\u003cp\u003eThe target users should not be limited to those who develop the system and the models. There is a significant demand to help people without knowledge in ML to understand/trust/use ML techniques. I think this point also accords to part of the goals of massive online education.\u003c/p\u003e\n\u003cp\u003eAutomated solutions powered by ML will be increasingly adopted in our daily life. How the algorithms make things right/wrong? Why? There will always be people interested in these questions, and VIS would be a critical and effective step towards the answers.\u003c/p\u003e\n\u003ch2 id=\"visualization\"\u003eVisualization\u003c/h2\u003e\n\u003cp\u003eI always enjoy making beautiful graphics. I think this is one of my driving powers for research in VIS. Apart from research, I felt a great sense of achievement by making them and having people like them.\u003c/p\u003e\n\u003cp\u003eFrom the past five years (I have been making posters for different student organizations during my undergraduate), I learned that I don't have much talent in design or art. But making graphics can be count as one of my habits. I would not feel tired of creating graphics for a whole day (but would feel tired of writing). I think it's very lucky for me to be able to join Prof. Qu's group to work on VIS.\u003c/p\u003e\n\u003ch2 id=\"coding\"\u003eCoding\u003c/h2\u003e\n\u003cp\u003eBy my own standards, my skill sets in coding improved a lot via this submission.\u003c/p\u003e\n\u003cp\u003eI learned how to write Python C extensions to glue code written in C for performance. I learned React and Typescript in front-end developments. I learned Docker when I need to set up demos to run user studies.\u003c/p\u003e\n\u003cp\u003eAnother lesson I learned is, don't change the framework or refactor the code if the time is limited. During the submission, I wasted about 5 days, changing the code from D3.js to pure React (use React to handle the rendering of SVG nodes) and then back to D3.js. When efficiency is crucial, it's worth sacrificing the readability gained from React.\u003c/p\u003e\n\u003cp\u003eTo build things with my own code is one of the most exciting things that I love in Computer Science.\u003c/p\u003e\n\u003ch2 id=\"things-i-learned-from-enrico-and-his-students\"\u003eThings I learned from Enrico and his students\u003c/h2\u003e\n\u003cp\u003eI learned a lot from Enrico during the collaboration. Enrico has his standards for what is good research. Though he is quite open to my own ideas, his standards helped me a lot in refining the direction and scope of this paper. I think a good measure for the value of an application research can be roughly formulated as: \u003cem\u003ethe potential value added to a user\u003c/em\u003e x \u003cem\u003ethe number of potential users\u003c/em\u003e. Prof. Qu also suggest that we should working on more important problems (而不应该去研究“茴”的四种写法).\u003c/p\u003e\n\u003cp\u003eOne important thing I learn from him is that, don't presume that something is obvious. As researchers, we work on our projects every day. Something clear to us might not be easy-to-understand to others. It's always He tried to understand the most detailed parts of my proposed method and pointed out things that he thought are fragile. This helps a lot for this submission.\u003c/p\u003e\n\u003cp\u003eAnother thing I learned from his group members is to stay critical. During meetings, they always actively raised points they felt confused or ideas they felt relevant. They would start from a question raised from a presentation and discuss freely for half an hour. Presenting our work and discuss it with people is the quickest way to improve it. It turns out these discussions are always valuable and would contribute to one's submission.\u003c/p\u003e"},{"source":"content/blog/pqe-reflection.md","path":"/blog/pqe-reflection/","url":"/blog/pqe-reflection/","content":"\u003cblockquote\u003e\n\u003cp\u003eFinally, I am a Ph.D. \"candidate\" now.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis is the first thought came to my mind after I got passed the PQE.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI have to throw myself into bed for two days.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThat is my second immediate thought. Honestly speaking, my PQE is far from a good one. My time management is like a disaster. Luckily, the oral presentation is finished as expected. \u003ca id=\"more\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"surveying-the-research-gaps\"\u003eSurveying the Research Gaps\u003c/h2\u003e\n\u003cp\u003eThe most rewarding thing that I learned from PQE is how to identify research gaps through surveying. By literature review, we can construct a big picture of the previous and current research in a specific direction. It's like changing the view from multiple trees into a forest, and what is beyond the forest. With such big picture of an area, we are able to learn the specific problems that previous papers trying to solve, the current state-of-the-art of solving it, and which part is still missing (the research gaps). During the survey process, there are two other skills that I learned.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/pqe-reflection/forest.jpg\" title=\"Photo by Markus Spiske on Unsplash\" alt=\"Photo by Markus Spiske on Unsplash\" /\u003e\u003cfigcaption\u003ePhoto by Markus Spiske on Unsplash\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe first one is how to give critical comments in the context of a research area. For a long time, the contribution of some paper looks very vague to me. What kind of contribution is it for solving an old problem with an existing method from another field? It's clearer if we discuss the contribution, the weakness, and the inspiration of a paper considering the big picture. Contribution is a broad concept. Anything that benefits the research area can be count as a contribution. When doing critical thinking, there is more to think about than just discussing whether the techniques a paper used can be improved. They solve one part of the problem, but what other part is still missing? What are other related problems that may benefit from their progress? What can we learn to use in our own research?\u003c/p\u003e\n\u003cp\u003eThe second one is how to organize and summarize batches of papers in a certain field. Common approaches for organizing papers include problem-driven and application-driven. The problem-driven approach is often very clear and understandable for outsiders or newcomers. Given a general problem that a research area is addressing, we then divide the problem into smaller ones according to some taxonomies. This is a very neat way to present the status of a research problem: what are the challenges of this problem, which sub-problems have been attempted and which have not, which have been well tackled and which still have space for improvements.\u003c/p\u003e\n\u003ch2 id=\"presentation\"\u003ePresentation\u003c/h2\u003e\n\u003cp\u003eIn the last two months, I learned a lot for presentation. I think presentation is nothing but a media for conveying ideas, knowledge and all sorts of information. Speeches and diagrams are naturally more friendly for humans than paragraphs of academic papers.\u003c/p\u003e\n\u003cp\u003eSince presentation is an extra media for information distribution, the first question is, what are the receivers of this information? What are your audiences? For PQE, the most important audience is the committee. The committee might not familiar with the specific topic you are doing. So a brief and clear introduction to the background, the importance, and the challenges of the topic is needed. The second question is, how to more effectively convey your message. Prof. Qu has taught us the concept of signal/noise ratio in a presentation. Keep a high signal/noise ratio is the key here. But how to do that? Though PQE is more of a survey, we still need to have depth. Given 30 to 40 minutes, it is very hard to discuss every paper that you think is important in the presentation. Here is the trick that I learned from Prof. Qu: only discuss five papers in detail and five papers as a demonstration, and try to organize a storyline from these papers. And that's the previous work you will discuss in detail in the presentation. Apart from PQE, academic presentations share the same idea. You have done a lot in your work. However, in limited time, it's impossible to present all the details. We have to make decisions on what to kept and what to throw away.\u003c/p\u003e\n\u003ch2 id=\"the-stress-research-and-time-management\"\u003eThe Stress, Research, and Time Management\u003c/h2\u003e\n\u003cp\u003eThe following is more of personal emotions.\u003c/p\u003e\n\u003cp\u003eI felt very stressed for the last three weeks. Sometimes, I even kept thinking about how to better organize different sections when I was eating or having a shower. While at the same time, I felt difficult to calm down to do the writing. The stress actually comes from me myself. I was given a summer to prepare the PQE, but I didn't use the time well and spend it wandering around. My topic is finally fixed around late August. And I only started the writing after I came back from the VIS conference in October.\u003c/p\u003e\n\u003cp\u003eAppropriate stress can be a good thing for self-motivating. That's what I always agreed with. I have not been overwhelmed by stress for quite a long time. Even during the submission of VIS '17, I am not that stressed, since the stress was kind of relieved through the collaboration with Shaozu. And there are many other lab-mates also working on the submission. While this time, I can only face the stress by myself. I am not sure where the stress comes from. PQE is a survey, but it still requires the production of new knowledge -- the big picture, the research gaps, and the future opportunities. Outputting is much harder for most human brains than inputting. For me, this is nothing like in the undergraduate, the things I need to know are either on the books or the Internet. At that time I have the confidence that I am able to master the knowledge of a course in a few days. However, the situation is totally different for research.\u003c/p\u003e\n\u003cp\u003eBefore the PQE, I didn't realize that a serious time management is not only a schedule that helps us get things done. It is actually also a solution for stress. If I had followed step-by-step the time schedules and sub-deadlines for PQE, I would not be that stressed. This is the other lesson that I learned from PQE.\u003c/p\u003e\n\u003cp\u003eAnyway, I am a candidate now. And I will make it a Ph.D.\u003c/p\u003e\n","frontmatter":{"title":"Now, Call Me a Candidate","date":"2017-11-06T21:35:10.000Z","tags":"research"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"A Trip to the West World -- Attending VIS 2017","date":"2017-10-11T15:34:42.000Z","tags":"VIS, research, conference, VIS+ML"},"path":"/blog/vis2017/"},"next_page":{"frontmatter":{"title":"Reflecting My Vis 2018 Submission","date":"2018-04-29T15:34:42.000Z","tags":"VIS, research, reflection"},"path":"/blog/vis2018/"},"excerpt":"\u003cblockquote\u003e\n\u003cp\u003eFinally, I am a Ph.D. \"candidate\" now.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis is the first thought came to my mind after I got passed the PQE.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI have to throw myself into bed for two days.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThat is my second immediate thought. Honestly speaking, my PQE is far from a good one. My time management is like a disaster. Luckily, the oral presentation is finished as expected.","more":"\u003c/p\u003e\n\u003ch2 id=\"surveying-the-research-gaps\"\u003eSurveying the Research Gaps\u003c/h2\u003e\n\u003cp\u003eThe most rewarding thing that I learned from PQE is how to identify research gaps through surveying. By literature review, we can construct a big picture of the previous and current research in a specific direction. It's like changing the view from multiple trees into a forest, and what is beyond the forest. With such big picture of an area, we are able to learn the specific problems that previous papers trying to solve, the current state-of-the-art of solving it, and which part is still missing (the research gaps). During the survey process, there are two other skills that I learned.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/pqe-reflection/forest.jpg\" title=\"Photo by Markus Spiske on Unsplash\" alt=\"Photo by Markus Spiske on Unsplash\" /\u003e\u003cfigcaption\u003ePhoto by Markus Spiske on Unsplash\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThe first one is how to give critical comments in the context of a research area. For a long time, the contribution of some paper looks very vague to me. What kind of contribution is it for solving an old problem with an existing method from another field? It's clearer if we discuss the contribution, the weakness, and the inspiration of a paper considering the big picture. Contribution is a broad concept. Anything that benefits the research area can be count as a contribution. When doing critical thinking, there is more to think about than just discussing whether the techniques a paper used can be improved. They solve one part of the problem, but what other part is still missing? What are other related problems that may benefit from their progress? What can we learn to use in our own research?\u003c/p\u003e\n\u003cp\u003eThe second one is how to organize and summarize batches of papers in a certain field. Common approaches for organizing papers include problem-driven and application-driven. The problem-driven approach is often very clear and understandable for outsiders or newcomers. Given a general problem that a research area is addressing, we then divide the problem into smaller ones according to some taxonomies. This is a very neat way to present the status of a research problem: what are the challenges of this problem, which sub-problems have been attempted and which have not, which have been well tackled and which still have space for improvements.\u003c/p\u003e\n\u003ch2 id=\"presentation\"\u003ePresentation\u003c/h2\u003e\n\u003cp\u003eIn the last two months, I learned a lot for presentation. I think presentation is nothing but a media for conveying ideas, knowledge and all sorts of information. Speeches and diagrams are naturally more friendly for humans than paragraphs of academic papers.\u003c/p\u003e\n\u003cp\u003eSince presentation is an extra media for information distribution, the first question is, what are the receivers of this information? What are your audiences? For PQE, the most important audience is the committee. The committee might not familiar with the specific topic you are doing. So a brief and clear introduction to the background, the importance, and the challenges of the topic is needed. The second question is, how to more effectively convey your message. Prof. Qu has taught us the concept of signal/noise ratio in a presentation. Keep a high signal/noise ratio is the key here. But how to do that? Though PQE is more of a survey, we still need to have depth. Given 30 to 40 minutes, it is very hard to discuss every paper that you think is important in the presentation. Here is the trick that I learned from Prof. Qu: only discuss five papers in detail and five papers as a demonstration, and try to organize a storyline from these papers. And that's the previous work you will discuss in detail in the presentation. Apart from PQE, academic presentations share the same idea. You have done a lot in your work. However, in limited time, it's impossible to present all the details. We have to make decisions on what to kept and what to throw away.\u003c/p\u003e\n\u003ch2 id=\"the-stress-research-and-time-management\"\u003eThe Stress, Research, and Time Management\u003c/h2\u003e\n\u003cp\u003eThe following is more of personal emotions.\u003c/p\u003e\n\u003cp\u003eI felt very stressed for the last three weeks. Sometimes, I even kept thinking about how to better organize different sections when I was eating or having a shower. While at the same time, I felt difficult to calm down to do the writing. The stress actually comes from me myself. I was given a summer to prepare the PQE, but I didn't use the time well and spend it wandering around. My topic is finally fixed around late August. And I only started the writing after I came back from the VIS conference in October.\u003c/p\u003e\n\u003cp\u003eAppropriate stress can be a good thing for self-motivating. That's what I always agreed with. I have not been overwhelmed by stress for quite a long time. Even during the submission of VIS '17, I am not that stressed, since the stress was kind of relieved through the collaboration with Shaozu. And there are many other lab-mates also working on the submission. While this time, I can only face the stress by myself. I am not sure where the stress comes from. PQE is a survey, but it still requires the production of new knowledge -- the big picture, the research gaps, and the future opportunities. Outputting is much harder for most human brains than inputting. For me, this is nothing like in the undergraduate, the things I need to know are either on the books or the Internet. At that time I have the confidence that I am able to master the knowledge of a course in a few days. However, the situation is totally different for research.\u003c/p\u003e\n\u003cp\u003eBefore the PQE, I didn't realize that a serious time management is not only a schedule that helps us get things done. It is actually also a solution for stress. If I had followed step-by-step the time schedules and sub-deadlines for PQE, I would not be that stressed. This is the other lesson that I learned from PQE.\u003c/p\u003e\n\u003cp\u003eAnyway, I am a candidate now. And I will make it a Ph.D.\u003c/p\u003e"},{"source":"content/blog/vis2017/index.md","path":"/blog/vis2017/","url":"/blog/vis2017/","content":"\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2017/cactus.jpg\" title=\"Eggs-like Cactus\" alt=\"Eggs-like Cactus\" /\u003e\u003cfigcaption\u003eEggs-like Cactus\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThis year, VIS was held in Phoenix, Arizona. As the name implies, Phoenix is a hot and dry city located in the Sonoran Desert.\u003c/p\u003e\n\u003cp\u003eThis is my second year attending VIS. Unlike last year, I am able to present \u003ca href=\"http://www.myaooo.com/rnnvis\"\u003ea conference paper\u003c/a\u003e on VAST this year. This is really a great experience for me. More importantly, it's really nice to have the opportunity to learn what others are doing in this community.\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2017/present.jpg\" title=\"My Presentation\" alt=\"My Presentation\" /\u003e\u003cfigcaption\u003eMy Presentation\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eLet's start with some statistics. This year, VIS has accepted (39/170) papers in InfoVis, (37/173) papers in TVCG-track VAST, 15 papers in conference-track VAST (where mine is published), and (23/120) papers in SciVis. From simple math, we see here the acceptance rate for journal papers is (99/463=) 21.4%. Accounting for conference-track VAST papers, the rate is a little higher (24.6%). Though I felt a bit strange about this conference-track, the result that I am able to present my work in front of the community seems to be more important and more \"impactful\" than just a journal publication.\u003c/p\u003e\n\u003cdiv id=\"acceptance\" style=\"width: 400px; height: 300px; margin: auto;\"\u003e\n\n\u003c/div\u003e\n\u003chr /\u003e\n\u003ch1 id=\"visml\"\u003eVIS+ML\u003c/h1\u003e\n\u003cp\u003eTime is always limited. I have to say I have the bias towards this topic during the conference, considering that I kind of fixed my research direction on it in the next few years.\u003c/p\u003e\n\u003cp\u003eSome said this year's VIS is a milestone for VIS+ML. We have VIS+ML tutorial, visual analytics for deep learning (VADL) workshop. We have 3 sessions in VAST and 1 session in InfoVis that titled with \"ML\". We have a panel discussing the influence of ML to our field. Indeed, the great advances in ML have influenced many other fields, including database, graphics, networks and many non-CS fields. Some even said that computer vision is somewhat like a sub-field of ML now (no offense).\u003c/p\u003e\n\u003cp\u003eNow, this tide is also coming into our field of visualization. In some aspect, such a huge rise of interest in ML in such a community like ours is not healthy. It restricted the research and development of many other important topics (see below). I totally agree with this point. But in my mind, research field develops and grows as we start to work on promising interdisciplinary new areas. With such powerful techniques coming out, it's not wise to close our doors and say we have all we need in our home (闭关锁国). Are these VIS+ML papers really useful and inspiring? Is VIS+ML really a significant and promising direction? We don't know yet. But I think, to refuse the new is definitely a wrong mindset. Research is about working on new and innovative things that are \u003cstrong\u003epotentially\u003c/strong\u003e useful. Definitely useful things must have been worked on by more resourceful organizations like companies. I believe we will get to know whether such a direction is non-sense or not in the next few years. But of course, VIS is a conference about visualization anyway. We don't want it to be flooded by \"ML\" papers (though I also have a contribution to this).\u003c/p\u003e\n\u003cp\u003eNow back to the conference. A few related events that I attended are listed below.\u003c/p\u003e\n\u003ch2 id=\"visualization-the-secret-weapon-for-machine-learning-a-keynote-in-vds\"\u003eVisualization: The Secret Weapon For Machine Learning (A keynote in VDS)\u003c/h2\u003e\n\u003cp\u003eThe keynote is given by Fernanda Viegas and Martin Wattenberg at Google Inc.\u003c/p\u003e\n\u003cp\u003eThey present several works of them at the Big Picture group at Google Brain and talked a few about the \"People + AI\" initiative at Google. Actually, I very much agree with their idea of introducing humans in the loop of AI, or the so-called \"human-in-the-loop\" AI. Autonomous systems have indeed improved the efficiency of many applications. But in the seeable future, a lot more applications cannot be fully automated and require humans to be in the loop. That is why we need VIS and HCI. Interestingly, the closing capstone of the conference is titled \"Data Humanism\".\u003c/p\u003e\n\u003cp\u003eOne interesting thing is the insights that can be found by simply visualizing the datasets. They present the Facet that they recently released. Though I think the scalability is still an issue of their tool.\u003c/p\u003e\n\u003cp\u003eA few applications that Vis can be used in ML:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003edebugging data set\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003edebugging/understand models\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eeducation (teach ML models)\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003efairness\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"visml-symbiosis-of-visualization-and-machine-learning-tutorial\"\u003eVIS+ML: Symbiosis of Visualization and Machine Learning (Tutorial)\u003c/h2\u003e\n\u003cp\u003eSomething that caught my eye: An in-depth survey on how visualization can be used in augmenting embedding, presented by Yang Wang from Uber research.\u003c/p\u003e\n\u003ch2 id=\"visual-analysis-for-deep-learning-workshop\"\u003eVisual Analysis for Deep Learning (Workshop)\u003c/h2\u003e\n\u003cp\u003eShixia Liu at Tsinghua University has given a talk about their recent work on this topic.\u003c/p\u003e\n\u003cp\u003eShe presented the work from three possible applications of visualization for deep learning: Visual Understanding / Visual Diagnosis / Model Refinement. This taxonomy is actually the same as the one presented in their previously published survey paper.\u003c/p\u003e\n\u003cp\u003eIn my mind, the three categories are for experts with DL knowledge. One significant part that this taxonomy is missing is visualization for end-users that are non-experts. Non-expert users of DL techniques can actually be a larger population. I believe visualization can make a larger impact out of this.\u003c/p\u003e\n\u003cp\u003eA research question that came to my mind during the talk:\u003c/p\u003e\n\u003cp\u003eHow to remove the information of a region of the image more wisely? Current methods: 1) simply fill the region with gray/white 2) replace the region with random noise (uniform distribution)\u003c/p\u003e\n\u003ch2 id=\"vis-panel-how-do-recent-ml-advances-impact-the-data-visualization-research-agenda\"\u003eVIS Panel: How do Recent ML Advances Impact the Data Visualization Research Agenda?\u003c/h2\u003e\n\u003cp\u003eQ: Endangering the Human-in-the-loop Paradigm?\u003c/p\u003e\n\u003cp\u003eQ: Extending the Visualization Research Agenda?\u003c/p\u003e\n\u003cp\u003eSome interesting points that are raised by the panelists.\u003c/p\u003e\n\u003ch3 id=\"min-chen-the-space-of-machine-learning\"\u003eMin Chen: The space of Machine Learning\u003c/h3\u003e\n\u003cp\u003eI really appreciate his research on the foundation of visualization.\u003c/p\u003e\n\u003cp\u003eWhat he discussed on the computation of machine learning based on the scope of Universal Turing Machine is really inspiring. Though I am not sure if the results he presented has any theoretical proofs.\u003c/p\u003e\n\u003cp\u003eArticulating what current-state machines cannot do is a more solid argument for why we want to include human into the loop.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVisually exploring the space of ML? Cannot remember what this is about...\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"visually-supporting-software-engineering-with-ml\"\u003eVisually supporting software engineering with ML\u003c/h3\u003e\n\u003cp\u003eNot sure who discussed this idea (maybe Alexandru Telea). In the sense of current IT industry, ML is still in the scope of software engineering. Then, visualization used for supporting software engineering can be naturally extended to support ML. For example, quality assurance, testing...\u003c/p\u003e\n\u003ch3 id=\"ross-maciejewski-why-open-the-box\"\u003eRoss Maciejewski: Why Open the Box?\u003c/h3\u003e\n\u003cp\u003eHumans always have cognitive biases. So why we want to add bias to our system? Sometimes humans actually worsened the model's prediction.\u003c/p\u003e\n\u003cp\u003eHe also talked about extending the concept of \u003cstrong\u003eAlgorithmic Aversion\u003c/strong\u003e into this topic.\u003c/p\u003e\n\u003cp\u003eHe actually raised a very important (in my mind, the most important) question for the \u003cstrong\u003eVis for ML\u003c/strong\u003e topic.\u003c/p\u003e\n\u003cp\u003eBefore we want to add human in the loop, we have to clearly consider why we want to add VIS for ML. Can VIS really be \u003cstrong\u003ehelpful\u003c/strong\u003e? Or will humans make it worse?\u003c/p\u003e\n\u003cp\u003eI don't have an answer about it. I do agree that sometimes human interventions make things worse. But in many other scenarios, visualizations do help humans get insights. Let's see.\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2017/sun_flower.jpg\" title=\"Sunflowers in the botanic garden\" alt=\"Sunflowers in the botanic garden\" /\u003e\u003cfigcaption\u003eSunflowers in the botanic garden\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch1 id=\"important-and-unsolved-problems-in-vast\"\u003eImportant and Unsolved Problems in VAST:\u003c/h1\u003e\n\u003cp\u003eA list summarized from Prof. Qu's notes and other random discussions:\u003c/p\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eHow to quantify the visual complexity and cognitive loads?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to formalize the framework of visual reasoning in VA?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to make the process of visualization design more efficient (like using AI to automate and recommend visualization)?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to better integrate the research results of InfoVis and cognition science to our design process using more autonomous systems? (credits to Dongyu)\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e*How to scale visualization to support really large dataset (GPU acceleration, distributed system)?\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr /\u003e\n\u003ch1 id=\"presenting-at-conference\"\u003ePresenting at Conference\u003c/h1\u003e\n\u003cp\u003ePresenting your own work at conferences is a good way to make impact. The day before my talk, I suddenly felt so nervous when practicing for the last time accompanied with Qiaomu and Wei. This is kind of normal. In Chinese education environments, shy students like me may not be sufficiently trained for public speaking. Then I practiced about another 4 times to make sure I can go through my presentation fluently. I have to say, this is my first time giving a talk in front of so many people in such a formal event. I believed there are more than 150 people at my talk. Considering \"deep learning\" is such a buzz word nowadays, it's not a surprise to see this happened. Fortunately, thanks to my labmates and Prof. Qu, I made a talk that at least satisfied myself. Cheers!\u003c/p\u003e\n\u003cp\u003eSome tips that I learned from conference talks:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConference talk is usually a good way to discuss your work with others. Try to make the most out of it.\u003c/li\u003e\n\u003cli\u003eDon't fill the presentation with the full content of the paper. Present 30% critical part of the paper clearly can give the audience a deeper impression.\u003c/li\u003e\n\u003cli\u003eBefore the talk, practice at least a few times (for me it's 3) and make sure your talk and slides work together fluently.\u003c/li\u003e\n\u003cli\u003eEven if you have already done a rehearsal several days before, practice it for another time the day before the talk to make sure your memory is fresh.\u003c/li\u003e\n\u003cli\u003eTime matters. It's the basic respect that you follows the time schedule in a conference.\u003c/li\u003e\n\u003cli\u003eWhen giving the talk, make eye contacts with the audience (you can pick a few people that make you feel comfortable and look at them).\u003c/li\u003e\n\u003cli\u003eRemind your self to slow down your words. We (non-native speakers) may slur our words when speaking too fast, which may increase the nervousness.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr /\u003e\n\u003ch1 id=\"other-random-notes\"\u003eOther Random Notes\u003c/h1\u003e\n\u003cp\u003eFollowing is some random notes that I took during the conference. Maybe irrelevant to the main idea of this blog.\u003c/p\u003e\n\u003ch2 id=\"vahc-visual-analytics-in-healthcare\"\u003eVAHC: Visual Analytics in Healthcare\u003c/h2\u003e\n\u003cp\u003eThis year, the first event that I attended is VAHC. A keynote was presented by Hadi Kharrazi, a professor in public health at John Hopkins University. He mainly talked about the need for visualization in the field of population health. In a country like U.S., a large amount of funding (over 20% of the GDP) is allocated in healthcare. For example, in hospitals, electronic health records are now pervasive. But the data is not well utilized. The visualization / visual analytics in this field is more practical and domain-specific and can be directly used by or presented to decision makers. Some key points are summarized as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFeature reduction\u003c/li\u003e\n\u003cli\u003eTemporal data (how to deal with zero fills?)\u003c/li\u003e\n\u003cli\u003eCognitive issues\u003c/li\u003e\n\u003cli\u003eComparing models (models across different sub-populations)\u003c/li\u003e\n\u003cli\u003eData quality (how it affects the analytics results/visualization)\u003c/li\u003e\n\u003cli\u003eVariety and volume of the data\u003c/li\u003e\n\u003cli\u003edata quality issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePeople in healthcare domain may prefer simple charts (bar charts) because they don't understand what VA can do.\u003c/p\u003e\n\u003cp\u003eDifferent people (clinical/policy makers/medical researcher) have different needs in the visualization.\u003c/p\u003e\n\u003ch2 id=\"vds-visualization-in-data-science\"\u003eVDS: Visualization in Data Science\u003c/h2\u003e\n\u003cp\u003eProfessor Vasant Dhar at NYU gave a keynote talk about decision making with autonomous systems, with a focus on finance data.\u003c/p\u003e\n\u003cp\u003eAutonomous systems are increasingly used in many areas. In critical areas where human's knowledge are required during decision-making process (e.g. finance), one crucial question is whether we should trust a machine. In finance, dealers or traders may prefer a model that is more predictable (or, stable) with less return revenue. Visualization can be helpful in model evaluation and model monitoring.\u003c/p\u003e\n\u003ch2 id=\"vast-sequence\"\u003eVAST sequence\u003c/h2\u003e\n\u003ch3 id=\"yuanzhes-paper\"\u003eYuanzhe's paper\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLocality sensitive hashing\u003c/li\u003e\n\u003cli\u003ePresentation tips: embed videos inside ppt.\u003c/li\u003e\n\u003cli\u003eA good research problem should be general enough to have a broad range of applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cscript type=\"text/javascript\" src=\"https://unpkg.com/echarts@3.7.2/dist/echarts.min.js\"\u003e\u003c/script\u003e\n\u003cscript async type=\"text/javascript\" src=\"/blog/vis2017/bar-chart.js\"\u003e\u003c/script\u003e\n","frontmatter":{"title":"A Trip to the West World -- Attending VIS 2017","date":"2017-10-11T15:34:42.000Z","tags":"VIS, research, conference, VIS+ML"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"The Paper List for My PQE","date":"2017-09-12T19:32:41.000Z","tags":"XAI, research, PQE"},"path":"/blog/PaperListforPQE/"},"next_page":{"frontmatter":{"title":"Now, Call Me a Candidate","date":"2017-11-06T21:35:10.000Z","tags":"research"},"path":"/blog/pqe-reflection/"},"excerpt":"\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2017/cactus.jpg\" title=\"Eggs-like Cactus\" alt=\"Eggs-like Cactus\" /\u003e\u003cfigcaption\u003eEggs-like Cactus\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eThis year, VIS was held in Phoenix, Arizona. As the name implies, Phoenix is a hot and dry city located in the Sonoran Desert.\u003c/p\u003e\n\u003cp\u003eThis is my second year attending VIS. Unlike last year, I am able to present \u003ca href=\"http://www.myaooo.com/rnnvis\"\u003ea conference paper\u003c/a\u003e on VAST this year. This is really a great experience for me. More importantly, it's really nice to have the opportunity to learn what others are doing in this community.\u003c/p\u003e","more":"\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2017/present.jpg\" title=\"My Presentation\" alt=\"My Presentation\" /\u003e\u003cfigcaption\u003eMy Presentation\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eLet's start with some statistics. This year, VIS has accepted (39/170) papers in InfoVis, (37/173) papers in TVCG-track VAST, 15 papers in conference-track VAST (where mine is published), and (23/120) papers in SciVis. From simple math, we see here the acceptance rate for journal papers is (99/463=) 21.4%. Accounting for conference-track VAST papers, the rate is a little higher (24.6%). Though I felt a bit strange about this conference-track, the result that I am able to present my work in front of the community seems to be more important and more \"impactful\" than just a journal publication.\u003c/p\u003e\n\u003cdiv id=\"acceptance\" style=\"width: 400px; height: 300px; margin: auto;\"\u003e\n\n\u003c/div\u003e\n\u003chr /\u003e\n\u003ch1 id=\"visml\"\u003eVIS+ML\u003c/h1\u003e\n\u003cp\u003eTime is always limited. I have to say I have the bias towards this topic during the conference, considering that I kind of fixed my research direction on it in the next few years.\u003c/p\u003e\n\u003cp\u003eSome said this year's VIS is a milestone for VIS+ML. We have VIS+ML tutorial, visual analytics for deep learning (VADL) workshop. We have 3 sessions in VAST and 1 session in InfoVis that titled with \"ML\". We have a panel discussing the influence of ML to our field. Indeed, the great advances in ML have influenced many other fields, including database, graphics, networks and many non-CS fields. Some even said that computer vision is somewhat like a sub-field of ML now (no offense).\u003c/p\u003e\n\u003cp\u003eNow, this tide is also coming into our field of visualization. In some aspect, such a huge rise of interest in ML in such a community like ours is not healthy. It restricted the research and development of many other important topics (see below). I totally agree with this point. But in my mind, research field develops and grows as we start to work on promising interdisciplinary new areas. With such powerful techniques coming out, it's not wise to close our doors and say we have all we need in our home (闭关锁国). Are these VIS+ML papers really useful and inspiring? Is VIS+ML really a significant and promising direction? We don't know yet. But I think, to refuse the new is definitely a wrong mindset. Research is about working on new and innovative things that are \u003cstrong\u003epotentially\u003c/strong\u003e useful. Definitely useful things must have been worked on by more resourceful organizations like companies. I believe we will get to know whether such a direction is non-sense or not in the next few years. But of course, VIS is a conference about visualization anyway. We don't want it to be flooded by \"ML\" papers (though I also have a contribution to this).\u003c/p\u003e\n\u003cp\u003eNow back to the conference. A few related events that I attended are listed below.\u003c/p\u003e\n\u003ch2 id=\"visualization-the-secret-weapon-for-machine-learning-a-keynote-in-vds\"\u003eVisualization: The Secret Weapon For Machine Learning (A keynote in VDS)\u003c/h2\u003e\n\u003cp\u003eThe keynote is given by Fernanda Viegas and Martin Wattenberg at Google Inc.\u003c/p\u003e\n\u003cp\u003eThey present several works of them at the Big Picture group at Google Brain and talked a few about the \"People + AI\" initiative at Google. Actually, I very much agree with their idea of introducing humans in the loop of AI, or the so-called \"human-in-the-loop\" AI. Autonomous systems have indeed improved the efficiency of many applications. But in the seeable future, a lot more applications cannot be fully automated and require humans to be in the loop. That is why we need VIS and HCI. Interestingly, the closing capstone of the conference is titled \"Data Humanism\".\u003c/p\u003e\n\u003cp\u003eOne interesting thing is the insights that can be found by simply visualizing the datasets. They present the Facet that they recently released. Though I think the scalability is still an issue of their tool.\u003c/p\u003e\n\u003cp\u003eA few applications that Vis can be used in ML:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003edebugging data set\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003edebugging/understand models\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eeducation (teach ML models)\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003efairness\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"visml-symbiosis-of-visualization-and-machine-learning-tutorial\"\u003eVIS+ML: Symbiosis of Visualization and Machine Learning (Tutorial)\u003c/h2\u003e\n\u003cp\u003eSomething that caught my eye: An in-depth survey on how visualization can be used in augmenting embedding, presented by Yang Wang from Uber research.\u003c/p\u003e\n\u003ch2 id=\"visual-analysis-for-deep-learning-workshop\"\u003eVisual Analysis for Deep Learning (Workshop)\u003c/h2\u003e\n\u003cp\u003eShixia Liu at Tsinghua University has given a talk about their recent work on this topic.\u003c/p\u003e\n\u003cp\u003eShe presented the work from three possible applications of visualization for deep learning: Visual Understanding / Visual Diagnosis / Model Refinement. This taxonomy is actually the same as the one presented in their previously published survey paper.\u003c/p\u003e\n\u003cp\u003eIn my mind, the three categories are for experts with DL knowledge. One significant part that this taxonomy is missing is visualization for end-users that are non-experts. Non-expert users of DL techniques can actually be a larger population. I believe visualization can make a larger impact out of this.\u003c/p\u003e\n\u003cp\u003eA research question that came to my mind during the talk:\u003c/p\u003e\n\u003cp\u003eHow to remove the information of a region of the image more wisely? Current methods: 1) simply fill the region with gray/white 2) replace the region with random noise (uniform distribution)\u003c/p\u003e\n\u003ch2 id=\"vis-panel-how-do-recent-ml-advances-impact-the-data-visualization-research-agenda\"\u003eVIS Panel: How do Recent ML Advances Impact the Data Visualization Research Agenda?\u003c/h2\u003e\n\u003cp\u003eQ: Endangering the Human-in-the-loop Paradigm?\u003c/p\u003e\n\u003cp\u003eQ: Extending the Visualization Research Agenda?\u003c/p\u003e\n\u003cp\u003eSome interesting points that are raised by the panelists.\u003c/p\u003e\n\u003ch3 id=\"min-chen-the-space-of-machine-learning\"\u003eMin Chen: The space of Machine Learning\u003c/h3\u003e\n\u003cp\u003eI really appreciate his research on the foundation of visualization.\u003c/p\u003e\n\u003cp\u003eWhat he discussed on the computation of machine learning based on the scope of Universal Turing Machine is really inspiring. Though I am not sure if the results he presented has any theoretical proofs.\u003c/p\u003e\n\u003cp\u003eArticulating what current-state machines cannot do is a more solid argument for why we want to include human into the loop.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVisually exploring the space of ML? Cannot remember what this is about...\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"visually-supporting-software-engineering-with-ml\"\u003eVisually supporting software engineering with ML\u003c/h3\u003e\n\u003cp\u003eNot sure who discussed this idea (maybe Alexandru Telea). In the sense of current IT industry, ML is still in the scope of software engineering. Then, visualization used for supporting software engineering can be naturally extended to support ML. For example, quality assurance, testing...\u003c/p\u003e\n\u003ch3 id=\"ross-maciejewski-why-open-the-box\"\u003eRoss Maciejewski: Why Open the Box?\u003c/h3\u003e\n\u003cp\u003eHumans always have cognitive biases. So why we want to add bias to our system? Sometimes humans actually worsened the model's prediction.\u003c/p\u003e\n\u003cp\u003eHe also talked about extending the concept of \u003cstrong\u003eAlgorithmic Aversion\u003c/strong\u003e into this topic.\u003c/p\u003e\n\u003cp\u003eHe actually raised a very important (in my mind, the most important) question for the \u003cstrong\u003eVis for ML\u003c/strong\u003e topic.\u003c/p\u003e\n\u003cp\u003eBefore we want to add human in the loop, we have to clearly consider why we want to add VIS for ML. Can VIS really be \u003cstrong\u003ehelpful\u003c/strong\u003e? Or will humans make it worse?\u003c/p\u003e\n\u003cp\u003eI don't have an answer about it. I do agree that sometimes human interventions make things worse. But in many other scenarios, visualizations do help humans get insights. Let's see.\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/vis2017/sun_flower.jpg\" title=\"Sunflowers in the botanic garden\" alt=\"Sunflowers in the botanic garden\" /\u003e\u003cfigcaption\u003eSunflowers in the botanic garden\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch1 id=\"important-and-unsolved-problems-in-vast\"\u003eImportant and Unsolved Problems in VAST:\u003c/h1\u003e\n\u003cp\u003eA list summarized from Prof. Qu's notes and other random discussions:\u003c/p\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eHow to quantify the visual complexity and cognitive loads?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to formalize the framework of visual reasoning in VA?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to make the process of visualization design more efficient (like using AI to automate and recommend visualization)?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow to better integrate the research results of InfoVis and cognition science to our design process using more autonomous systems? (credits to Dongyu)\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e*How to scale visualization to support really large dataset (GPU acceleration, distributed system)?\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr /\u003e\n\u003ch1 id=\"presenting-at-conference\"\u003ePresenting at Conference\u003c/h1\u003e\n\u003cp\u003ePresenting your own work at conferences is a good way to make impact. The day before my talk, I suddenly felt so nervous when practicing for the last time accompanied with Qiaomu and Wei. This is kind of normal. In Chinese education environments, shy students like me may not be sufficiently trained for public speaking. Then I practiced about another 4 times to make sure I can go through my presentation fluently. I have to say, this is my first time giving a talk in front of so many people in such a formal event. I believed there are more than 150 people at my talk. Considering \"deep learning\" is such a buzz word nowadays, it's not a surprise to see this happened. Fortunately, thanks to my labmates and Prof. Qu, I made a talk that at least satisfied myself. Cheers!\u003c/p\u003e\n\u003cp\u003eSome tips that I learned from conference talks:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConference talk is usually a good way to discuss your work with others. Try to make the most out of it.\u003c/li\u003e\n\u003cli\u003eDon't fill the presentation with the full content of the paper. Present 30% critical part of the paper clearly can give the audience a deeper impression.\u003c/li\u003e\n\u003cli\u003eBefore the talk, practice at least a few times (for me it's 3) and make sure your talk and slides work together fluently.\u003c/li\u003e\n\u003cli\u003eEven if you have already done a rehearsal several days before, practice it for another time the day before the talk to make sure your memory is fresh.\u003c/li\u003e\n\u003cli\u003eTime matters. It's the basic respect that you follows the time schedule in a conference.\u003c/li\u003e\n\u003cli\u003eWhen giving the talk, make eye contacts with the audience (you can pick a few people that make you feel comfortable and look at them).\u003c/li\u003e\n\u003cli\u003eRemind your self to slow down your words. We (non-native speakers) may slur our words when speaking too fast, which may increase the nervousness.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr /\u003e\n\u003ch1 id=\"other-random-notes\"\u003eOther Random Notes\u003c/h1\u003e\n\u003cp\u003eFollowing is some random notes that I took during the conference. Maybe irrelevant to the main idea of this blog.\u003c/p\u003e\n\u003ch2 id=\"vahc-visual-analytics-in-healthcare\"\u003eVAHC: Visual Analytics in Healthcare\u003c/h2\u003e\n\u003cp\u003eThis year, the first event that I attended is VAHC. A keynote was presented by Hadi Kharrazi, a professor in public health at John Hopkins University. He mainly talked about the need for visualization in the field of population health. In a country like U.S., a large amount of funding (over 20% of the GDP) is allocated in healthcare. For example, in hospitals, electronic health records are now pervasive. But the data is not well utilized. The visualization / visual analytics in this field is more practical and domain-specific and can be directly used by or presented to decision makers. Some key points are summarized as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFeature reduction\u003c/li\u003e\n\u003cli\u003eTemporal data (how to deal with zero fills?)\u003c/li\u003e\n\u003cli\u003eCognitive issues\u003c/li\u003e\n\u003cli\u003eComparing models (models across different sub-populations)\u003c/li\u003e\n\u003cli\u003eData quality (how it affects the analytics results/visualization)\u003c/li\u003e\n\u003cli\u003eVariety and volume of the data\u003c/li\u003e\n\u003cli\u003edata quality issues\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePeople in healthcare domain may prefer simple charts (bar charts) because they don't understand what VA can do.\u003c/p\u003e\n\u003cp\u003eDifferent people (clinical/policy makers/medical researcher) have different needs in the visualization.\u003c/p\u003e\n\u003ch2 id=\"vds-visualization-in-data-science\"\u003eVDS: Visualization in Data Science\u003c/h2\u003e\n\u003cp\u003eProfessor Vasant Dhar at NYU gave a keynote talk about decision making with autonomous systems, with a focus on finance data.\u003c/p\u003e\n\u003cp\u003eAutonomous systems are increasingly used in many areas. In critical areas where human's knowledge are required during decision-making process (e.g. finance), one crucial question is whether we should trust a machine. In finance, dealers or traders may prefer a model that is more predictable (or, stable) with less return revenue. Visualization can be helpful in model evaluation and model monitoring.\u003c/p\u003e\n\u003ch2 id=\"vast-sequence\"\u003eVAST sequence\u003c/h2\u003e\n\u003ch3 id=\"yuanzhes-paper\"\u003eYuanzhe's paper\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLocality sensitive hashing\u003c/li\u003e\n\u003cli\u003ePresentation tips: embed videos inside ppt.\u003c/li\u003e\n\u003cli\u003eA good research problem should be general enough to have a broad range of applications.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cscript type=\"text/javascript\" src=\"https://unpkg.com/echarts@3.7.2/dist/echarts.min.js\"\u003e\u003c/script\u003e\n\u003cscript async type=\"text/javascript\" src=\"/blog/vis2017/bar-chart.js\"\u003e\u003c/script\u003e"},{"source":"content/blog/PaperListforPQE.md","path":"/blog/PaperListforPQE/","url":"/blog/PaperListforPQE/","content":"\u003cp\u003eThis is a paper list that I summarized for my PQE.\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003ch1 id=\"resources-for-xai\"\u003eResources for XAI\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIJCAI Workshop on \u003ca href=\"http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/\"\u003eExplainable Artificial Intelligence\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eICML16 Workshop on \u003ca href=\"https://sites.google.com/site/2016whi/\"\u003eHuman Interpretability in Machine Learning\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNIPS16 Workshop on \u003ca href=\"https://sites.google.com/site/nips2016interpretml/\"\u003eInterpretable ML for Complex Systems\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.darpa.mil/program/explainable-artificial-intelligence\"\u003eDARPA Program on XAI\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"/XAI-Researchers/\"\u003eRelated Researchers\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"paper-list\"\u003ePaper List\u003c/h1\u003e\n\u003ch2 id=\"highly-related-papers\"\u003eHighly Related Papers\u003c/h2\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. \u003cstrong\u003e\"Why should I trust you?\": Explaining the predictions of any classifier.\u003c/strong\u003e Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\u003c/p\u003e\n\u003cp\u003eThe paper addressed the important role of human in the field of machine learning. It also stated that \u003cem\u003etrust\u003c/em\u003e is crucial for end-users to utilize the power of a machine learning model. Then the paper proposed to use explanation as a solution to build users' trust.\u003c/p\u003e\n\u003cp\u003eThe paper also described situations where the current quantitative evaluation of ML models might be erroneous. Problems like data leakage and dataset shift are difficult to detect just by observing raw data and prediction. However, they can be efficiently detected if we provide fidel explanations of the model/prediction.\u003c/p\u003e\n\u003cp\u003eOn the other hands, in some situations, human priors (even non-expert) are valuable in improving ML models. There is frequently a mismatch between computable metrics like accuracy and our interested but incomputable metrics like user engagement. In this case, an expert's knowledge may be helpful in choosing a better model using his/her prior.\u003c/p\u003e\n\u003cp\u003eA good theoretical contribution of the paper is the formulation of interpretability and local fidelity of an explanation of the model's prediction (see Section 3).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIdeas\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eSpecific implementations of this framework are ill-argued, many possible improvements can be done. (E.g. the 0-1 formulation of explanation is limited, we have many other visual methods for explanation; the K-LASSO algorithm they proposed seems to be inefficient)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Image, Text\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: General classification model\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance and model-level, model-irrelevant explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: Simulated user experiment, User Experiment: select best classifier, improve classifier, comparison.\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eTodd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. \u003cstrong\u003ePrinciples of Explanatory Debugging to Personalize Interactive Machine Learning\u003c/strong\u003e. In Proceedings of the 20th International Conference on Intelligent User Interfaces (IUI '15). ACM, New York, NY, USA.\u003c/p\u003e\n\u003cp\u003eThis paper presents how explanations can help users build better mental models of an interactive machine learning system so that they can more efficiently personalize their learning systems. This paper was the first that shows the effectiveness of explanations in interactive learning systems.\u003c/p\u003e\n\u003cp\u003eOne important contribution of the paper is the summary of principles for Explanatory Debugging (for interactive learning systems). The basic \"Explaianbility\" principle is a good reference for designing algorithms or interface for general explainable/interpretable ML systems. However, the sub-principles of \"Soundness\" and \"Completeness\", though are necessary for building a faithful mental model, seem to be not necessary under the context of explainable ML. It is acceptable that we can use simpler model to locally approximate and \"explain\" the model-to-be-explained.\u003c/p\u003e\n\u003cp\u003eAnother downside of the paper is that the technique it proposed is only applicable to the Naive Bayes Classifier and can only serve as a illustration of concept.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Text\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Naive Bayes Classifier\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance-level, model-specific explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: A complete end-user study with careful discussion\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eDavid Martens and Foster Provost. 2014. \u003cstrong\u003eExplaining data-driven document classifications\u003c/strong\u003e. MIS Q. 38, 1 (March 2014).\u003c/p\u003e\n\u003cp\u003eThis paper distinguishes 2 types of explanations: Global Explanations and Instance-level Explanations.\u003c/p\u003e\n\u003cp\u003eThe paper claimed to have contribute a new format of explanations for text classifiers: a minimal set of words.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Text (Web pages)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance-level, model-irrelevant explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: General classification model for text\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eBaehrens, David, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert Müller. \u003cstrong\u003eHow to explain individual classification decisions.\u003c/strong\u003e Journal of Machine Learning Research 11.Jun (2010).\u003c/p\u003e\n\u003cp\u003eThis paper proposed a gradient based framework for explaining the decision for each individual data point. The paper defines explanation vector as the class probability gradients w.r.t inputs. The paper also noted that for non-probability based classification models (e.g. SVM), the probability gradient method will fail. For these cases, it uses a probability based model g' to approximate the model g.\u003c/p\u003e\n\u003cp\u003eUsing the gradient of prediction value with respect to input at a local point is an intuitive way for explaining a model. If the prediction is very sensitive w.r.t. a feature at the data point, then it means the this feature is important here, since prediction will changed a lot if this feature is fluctuated. However, gradient-based methods will easily fail for nowadays ML models. (E.g., the local gradient may mean nothing for a very noisy and wavy function) There are also cases that although the gradient of a feature is small, but the feature is actually contribute to the prediction a lot.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Image (USPS digits), Chemical compound structure data (a vector of counts of 142 molecular substructures)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance-level, model-irrelevant explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: General classification model\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eFéraud, Raphael, and Fabrice Clérot. \u003cstrong\u003eA methodology to explain neural network classification.\u003c/strong\u003e Neural Networks 15.2 (2002).\u003c/p\u003e\n\u003cp\u003eThe first paper that I found to explicitly use the term \"explain\" on machine learning models. The paper focused on solving the interpretability of a specific kind of model -- neural networks. In the early days, explaining an ML model is not that difficult. Given the limit computational power and data at that time, most popular ML models out there are actually not complex and have no strong need for explanability.\u003c/p\u003e\n\u003cp\u003eThis paper presents an intuitive methodology to explain neural networks (multi-layer perceptrons):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etrain a neural net\u003c/li\u003e\n\u003cli\u003efeature selection based on the net's prediction/activation.\u003c/li\u003e\n\u003cli\u003etrain a new neural net only based on the selected features\u003c/li\u003e\n\u003cli\u003ecluster the representation of the hidden layer of the new net.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA few problems of this method:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy train a new net? If raw feature results in unexplainable clusters, I don't think a newly trained model can explain the previous one.\u003c/li\u003e\n\u003cli\u003eProblems related to clustering algorithm. E.g., how to choose k? cluster quality?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: categorical data\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: vanilla neural network (MLP)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: Model-level explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: Use cluster result of the hidden representation directly for classification and measure classification accuracy.\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eBach, Sebastian, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. \u003cstrong\u003eOn Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.\u003c/strong\u003e PLOS ONE 10(7) (2015)\u003c/p\u003e\n\u003cp\u003eA technique paper that proposed the LRP method for explaining deep neural networks (CNN) that has layer-wise architectures.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eArras L, Montavon G, Müller KR, Samek W. Explaining recurrent neural network predictions in sentiment analysis. Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. 2017\u003c/p\u003e\n\u003cp\u003eAn extension of the LRP on RNN for sentiment analysis.\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCognitive psychology for deep neural networks: a shape bias case study\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNetwork Dissection: Quantifying Interpretability of Deep Visual Representations.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eUnderstanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"vis-papers\"\u003eVis Papers\u003c/h2\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eLiu, M., Shi, J., Li, Z., Li, C., Zhu, J. and Liu, S., 2017. \u003cstrong\u003eTowards better analysis of deep convolutional neural networks.\u003c/strong\u003e IEEE transactions on visualization and computer graphics, 23(1), pp.91-100.\u003c/p\u003e\n\u003cp\u003eThe good:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFormulate the problem of analyzing CNN, and the requirements of the system very well\u003c/li\u003e\n\u003cli\u003eThe system is complete, well polished. And the paper has argued the visual design in a reasonable manner.\u003c/li\u003e\n\u003cli\u003eBi-clustering-based edge bundling algorithm. Seems inspiring (formulate the CNN as a layer-wised bi-cluster graph)\u003c/li\u003e\n\u003cli\u003eGood and abundant case study to demonstrate the usefulness of their design.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe bad:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe system is somewhat over designed.\u003c/li\u003e\n\u003cli\u003e\u003col type=\"1\"\u003e\n\u003cli\u003ethe activation matrix seems can be better co-designed with the learned feature matrix\u003c/li\u003e\n\u003c/ol\u003e\u003c/li\u003e\n\u003cli\u003e\u003col start=\"2\" type=\"1\"\u003e\n\u003cli\u003eBiclustering-based edge bundling is not intuitive\u003c/li\u003e\n\u003c/ol\u003e\u003c/li\u003e\n\u003cli\u003eThe system is hard to be rebuilt / lack generalization and scalability. e.g. when the network gots too large, the learned feature matrix seems vague and hard understand.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFurther thinking: This paper has already done a lot of work on the CNN, although not perfect. But to improve the system or designing a new one for CNN is not as meaningful in the sense of research. To focus diagnosing and understanding RNN seems to be a better idea. The techniques and ideas of this work can be of reference in RNNVis\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eRNNVis\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eLSTMVis (Harvard)\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"other-related-papers\"\u003eOther Related Papers\u003c/h2\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003eMark W. Craven and Jude W. Shavlik. \u003cstrong\u003eExtracting tree-structured representations of trained networks.\u003c/strong\u003e In Proceedings of the 8th International Conference on Neural Information Processing Systems (NIPS) (1995).\u003c/li\u003e\n\u003c/ol\u003e\n","frontmatter":{"title":"The Paper List for My PQE","date":"2017-09-12T19:32:41.000Z","tags":"XAI, research, PQE"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Explainable AI -- Definition, Motivation and Application","date":"2017-08-17T16:34:42.000Z","tags":"XAI, research"},"path":"/blog/Towards/"},"next_page":{"frontmatter":{"title":"A Trip to the West World -- Attending VIS 2017","date":"2017-10-11T15:34:42.000Z","tags":"VIS, research, conference, VIS+ML"},"path":"/blog/vis2017/"},"excerpt":"\u003cp\u003eThis is a paper list that I summarized for my PQE.\u003c/p\u003e","more":"\u003ch1 id=\"resources-for-xai\"\u003eResources for XAI\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eIJCAI Workshop on \u003ca href=\"http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/\"\u003eExplainable Artificial Intelligence\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eICML16 Workshop on \u003ca href=\"https://sites.google.com/site/2016whi/\"\u003eHuman Interpretability in Machine Learning\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNIPS16 Workshop on \u003ca href=\"https://sites.google.com/site/nips2016interpretml/\"\u003eInterpretable ML for Complex Systems\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.darpa.mil/program/explainable-artificial-intelligence\"\u003eDARPA Program on XAI\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"/XAI-Researchers/\"\u003eRelated Researchers\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"paper-list\"\u003ePaper List\u003c/h1\u003e\n\u003ch2 id=\"highly-related-papers\"\u003eHighly Related Papers\u003c/h2\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. \u003cstrong\u003e\"Why should I trust you?\": Explaining the predictions of any classifier.\u003c/strong\u003e Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\u003c/p\u003e\n\u003cp\u003eThe paper addressed the important role of human in the field of machine learning. It also stated that \u003cem\u003etrust\u003c/em\u003e is crucial for end-users to utilize the power of a machine learning model. Then the paper proposed to use explanation as a solution to build users' trust.\u003c/p\u003e\n\u003cp\u003eThe paper also described situations where the current quantitative evaluation of ML models might be erroneous. Problems like data leakage and dataset shift are difficult to detect just by observing raw data and prediction. However, they can be efficiently detected if we provide fidel explanations of the model/prediction.\u003c/p\u003e\n\u003cp\u003eOn the other hands, in some situations, human priors (even non-expert) are valuable in improving ML models. There is frequently a mismatch between computable metrics like accuracy and our interested but incomputable metrics like user engagement. In this case, an expert's knowledge may be helpful in choosing a better model using his/her prior.\u003c/p\u003e\n\u003cp\u003eA good theoretical contribution of the paper is the formulation of interpretability and local fidelity of an explanation of the model's prediction (see Section 3).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIdeas\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eSpecific implementations of this framework are ill-argued, many possible improvements can be done. (E.g. the 0-1 formulation of explanation is limited, we have many other visual methods for explanation; the K-LASSO algorithm they proposed seems to be inefficient)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Image, Text\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: General classification model\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance and model-level, model-irrelevant explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: Simulated user experiment, User Experiment: select best classifier, improve classifier, comparison.\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eTodd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. \u003cstrong\u003ePrinciples of Explanatory Debugging to Personalize Interactive Machine Learning\u003c/strong\u003e. In Proceedings of the 20th International Conference on Intelligent User Interfaces (IUI '15). ACM, New York, NY, USA.\u003c/p\u003e\n\u003cp\u003eThis paper presents how explanations can help users build better mental models of an interactive machine learning system so that they can more efficiently personalize their learning systems. This paper was the first that shows the effectiveness of explanations in interactive learning systems.\u003c/p\u003e\n\u003cp\u003eOne important contribution of the paper is the summary of principles for Explanatory Debugging (for interactive learning systems). The basic \"Explaianbility\" principle is a good reference for designing algorithms or interface for general explainable/interpretable ML systems. However, the sub-principles of \"Soundness\" and \"Completeness\", though are necessary for building a faithful mental model, seem to be not necessary under the context of explainable ML. It is acceptable that we can use simpler model to locally approximate and \"explain\" the model-to-be-explained.\u003c/p\u003e\n\u003cp\u003eAnother downside of the paper is that the technique it proposed is only applicable to the Naive Bayes Classifier and can only serve as a illustration of concept.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Text\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: Naive Bayes Classifier\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance-level, model-specific explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: A complete end-user study with careful discussion\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eDavid Martens and Foster Provost. 2014. \u003cstrong\u003eExplaining data-driven document classifications\u003c/strong\u003e. MIS Q. 38, 1 (March 2014).\u003c/p\u003e\n\u003cp\u003eThis paper distinguishes 2 types of explanations: Global Explanations and Instance-level Explanations.\u003c/p\u003e\n\u003cp\u003eThe paper claimed to have contribute a new format of explanations for text classifiers: a minimal set of words.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Text (Web pages)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance-level, model-irrelevant explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: General classification model for text\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eBaehrens, David, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert Müller. \u003cstrong\u003eHow to explain individual classification decisions.\u003c/strong\u003e Journal of Machine Learning Research 11.Jun (2010).\u003c/p\u003e\n\u003cp\u003eThis paper proposed a gradient based framework for explaining the decision for each individual data point. The paper defines explanation vector as the class probability gradients w.r.t inputs. The paper also noted that for non-probability based classification models (e.g. SVM), the probability gradient method will fail. For these cases, it uses a probability based model g' to approximate the model g.\u003c/p\u003e\n\u003cp\u003eUsing the gradient of prediction value with respect to input at a local point is an intuitive way for explaining a model. If the prediction is very sensitive w.r.t. a feature at the data point, then it means the this feature is important here, since prediction will changed a lot if this feature is fluctuated. However, gradient-based methods will easily fail for nowadays ML models. (E.g., the local gradient may mean nothing for a very noisy and wavy function) There are also cases that although the gradient of a feature is small, but the feature is actually contribute to the prediction a lot.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: Image (USPS digits), Chemical compound structure data (a vector of counts of 142 molecular substructures)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: instance-level, model-irrelevant explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: General classification model\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eFéraud, Raphael, and Fabrice Clérot. \u003cstrong\u003eA methodology to explain neural network classification.\u003c/strong\u003e Neural Networks 15.2 (2002).\u003c/p\u003e\n\u003cp\u003eThe first paper that I found to explicitly use the term \"explain\" on machine learning models. The paper focused on solving the interpretability of a specific kind of model -- neural networks. In the early days, explaining an ML model is not that difficult. Given the limit computational power and data at that time, most popular ML models out there are actually not complex and have no strong need for explanability.\u003c/p\u003e\n\u003cp\u003eThis paper presents an intuitive methodology to explain neural networks (multi-layer perceptrons):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etrain a neural net\u003c/li\u003e\n\u003cli\u003efeature selection based on the net's prediction/activation.\u003c/li\u003e\n\u003cli\u003etrain a new neural net only based on the selected features\u003c/li\u003e\n\u003cli\u003ecluster the representation of the hidden layer of the new net.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA few problems of this method:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy train a new net? If raw feature results in unexplainable clusters, I don't think a newly trained model can explain the previous one.\u003c/li\u003e\n\u003cli\u003eProblems related to clustering algorithm. E.g., how to choose k? cluster quality?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eData\u003c/strong\u003e: categorical data\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e: vanilla neural network (MLP)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExplanation Type\u003c/strong\u003e: Model-level explanation\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e: Use cluster result of the hidden representation directly for classification and measure classification accuracy.\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eBach, Sebastian, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. \u003cstrong\u003eOn Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.\u003c/strong\u003e PLOS ONE 10(7) (2015)\u003c/p\u003e\n\u003cp\u003eA technique paper that proposed the LRP method for explaining deep neural networks (CNN) that has layer-wise architectures.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eArras L, Montavon G, Müller KR, Samek W. Explaining recurrent neural network predictions in sentiment analysis. Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. 2017\u003c/p\u003e\n\u003cp\u003eAn extension of the LRP on RNN for sentiment analysis.\u003c/p\u003e\n\u003chr /\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCognitive psychology for deep neural networks: a shape bias case study\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eNetwork Dissection: Quantifying Interpretability of Deep Visual Representations.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eUnderstanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"vis-papers\"\u003eVis Papers\u003c/h2\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003e\u003cp\u003eLiu, M., Shi, J., Li, Z., Li, C., Zhu, J. and Liu, S., 2017. \u003cstrong\u003eTowards better analysis of deep convolutional neural networks.\u003c/strong\u003e IEEE transactions on visualization and computer graphics, 23(1), pp.91-100.\u003c/p\u003e\n\u003cp\u003eThe good:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFormulate the problem of analyzing CNN, and the requirements of the system very well\u003c/li\u003e\n\u003cli\u003eThe system is complete, well polished. And the paper has argued the visual design in a reasonable manner.\u003c/li\u003e\n\u003cli\u003eBi-clustering-based edge bundling algorithm. Seems inspiring (formulate the CNN as a layer-wised bi-cluster graph)\u003c/li\u003e\n\u003cli\u003eGood and abundant case study to demonstrate the usefulness of their design.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe bad:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe system is somewhat over designed.\u003c/li\u003e\n\u003cli\u003e\u003col type=\"1\"\u003e\n\u003cli\u003ethe activation matrix seems can be better co-designed with the learned feature matrix\u003c/li\u003e\n\u003c/ol\u003e\u003c/li\u003e\n\u003cli\u003e\u003col start=\"2\" type=\"1\"\u003e\n\u003cli\u003eBiclustering-based edge bundling is not intuitive\u003c/li\u003e\n\u003c/ol\u003e\u003c/li\u003e\n\u003cli\u003eThe system is hard to be rebuilt / lack generalization and scalability. e.g. when the network gots too large, the learned feature matrix seems vague and hard understand.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFurther thinking: This paper has already done a lot of work on the CNN, although not perfect. But to improve the system or designing a new one for CNN is not as meaningful in the sense of research. To focus diagnosing and understanding RNN seems to be a better idea. The techniques and ideas of this work can be of reference in RNNVis\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eRNNVis\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eLSTMVis (Harvard)\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"other-related-papers\"\u003eOther Related Papers\u003c/h2\u003e\n\u003col type=\"1\"\u003e\n\u003cli\u003eMark W. Craven and Jude W. Shavlik. \u003cstrong\u003eExtracting tree-structured representations of trained networks.\u003c/strong\u003e In Proceedings of the 8th International Conference on Neural Information Processing Systems (NIPS) (1995).\u003c/li\u003e\n\u003c/ol\u003e"},{"source":"content/blog/Towards.md","path":"/blog/Towards/","url":"/blog/Towards/","content":"\u003cp\u003eI have been reading papers and articles and searching for ideas of my Ph.D. Qualification Exam (PQE) for a few days. Since I am interested in working on the interdisciplinary field of Visualization and Machine Learning, the idea of \"explainable AI\" (XAI) seems promising to me. After discussing with my professor, I decided to fixed the survey topic to \"Visualization for Explainable Machine Learning\". This blog summarizes my understanding on the motivation, scope and application of XAI.\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003eClearly, the term \"explainable AI\" refers to the \"AI\" that are \"explainable\". With the boom of machine learning and deep learning, today's AI covers a broad range of models and techniques. Thus, the term AI is hard to be clearly defined. Since I am not going to discuss how to better define AI, here the AI refers to the systems or models that utilize modern machine learning techniques rather than the general AI as coined by \u003ca href=\"http://www-formal.stanford.edu/jmc/whatisai/node1.html\"\u003eJohn McCarthy\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe next question is, how we define \"explainable\"?\u003c/p\u003e\n\u003cp\u003eThe Merriam Webster Dictionary defines \"explain\" as \"to give the reason for or cause of\". Based on this definition, I roughly describe \"explainable AI\" as the AI that can provide the reason (of it's prediction/action) to a human so that the human can understand it.\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eNow that we have the definition (or description) of XAI, why we would want to have it? Isn't the black-boxed machine learning models already good enough? Why bother to make them explainable? Indeed, we don't quite need a digit recognizer to be able to explain the reason of its prediction, since we want it fully automatic without costing human labor. But we do have the need for critical situations where fully automatic systems are not available. In these situations, human users need to understand, appropriately trust, and effectively operate with the upcoming AIs (DARPA, 2016). For example, a doctor utilizing AI tools which helps him/her determine whether the patient has caught \u003ca href=\"https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\"\u003ecervical cancer\u003c/a\u003e will need the AI tools to be explainable. In such life-critical situations, explainability can help the doctor build trust on the system and correct wrong diagnosis if necessary, so that the doctor can boost efficiency while keeping the diagnosing quality. More applications with the need of XAI will be discussed below.\u003c/p\u003e\n\u003cp\u003eThe need for XAI, in my mind, results from the recent success of AI technology as well as its limitations. The new AI techniques are believed to be promising in many fields, including finance, hospital and medicine, education, entertainment, and transportation. However, these successful new techniques, including SVM, probabilistic graph model, random forest, deep learning and reinforcement learning, are difficult to interpret. That's why these models are sometimes criticized as black-boxes. In cases where AI are used as a collaborator with a human rather than a completely automatic solution, a monitoring system that can sufficiently explain the AI is crucial.\u003c/p\u003e\n\u003ch3 id=\"application\"\u003eApplication\u003c/h3\u003e\n\u003cp\u003eDoshi-Velez and Kim (2017) argued that not all ML systems require interpretability (or explainability), but explainations may highlight \"incompleteness\" in problem formulation. This incompleteness refers to the biases that we cannot quantified, and thus cannot be optimized. This reminds me of the famous saying of George Box: \"All models are wrong, but some are useful\". Considering that all models are built on the assumptions that we can not fully validate in the real world, explainability is needed when such incompleteness is not neglectable. A list of scenarios that need explainability is provided by Doshi-Velez and Kim (2017). A summarized list is as below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eScientific Understanding. This is for researchers and teachers. Researchers need to gain more understandings on models' behavior and mechanisms. Teachers can utilize explainable AI to help students get essential understanding on AI systems' behavior.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eSafety. In some real tasks, the ML systems are not always testable. Thus explainability is desirable for those who operate or monitor the system. Highly related industries includes military, finance and medical services.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eEthics. E.g. A \"fair\" classifier for loan approval. Sometimes, performance targeted end-to-end classifiers may be biased to gender and race, which is unfair (政治正确).\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMismatched objectives. Some times an ML system may be optimized for one objective (classifying possible engine failures) but used for broader scenarios (building better automatic cars).\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMulti-objective trade-offs. In real tasks, accuracy is not the only objective, we also want reliability, unbiasedness. There is still a lack of effective algorithms that can help optimizing such multi-objectives. Explainable systems can leave this tasks for human.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","frontmatter":{"title":"Explainable AI -- Definition, Motivation and Application","date":"2017-08-17T16:34:42.000Z","tags":"XAI, research"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Power to the People - The role of Humans in Interactive Machine Learning","date":"2017-06-30T10:36:24.000Z","tags":"reading note, machine learning, research"},"path":"/blog/reading/"},"next_page":{"frontmatter":{"title":"The Paper List for My PQE","date":"2017-09-12T19:32:41.000Z","tags":"XAI, research, PQE"},"path":"/blog/PaperListforPQE/"},"excerpt":"\u003cp\u003eI have been reading papers and articles and searching for ideas of my Ph.D. Qualification Exam (PQE) for a few days. Since I am interested in working on the interdisciplinary field of Visualization and Machine Learning, the idea of \"explainable AI\" (XAI) seems promising to me. After discussing with my professor, I decided to fixed the survey topic to \"Visualization for Explainable Machine Learning\". This blog summarizes my understanding on the motivation, scope and application of XAI.\u003c/p\u003e","more":"\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003eClearly, the term \"explainable AI\" refers to the \"AI\" that are \"explainable\". With the boom of machine learning and deep learning, today's AI covers a broad range of models and techniques. Thus, the term AI is hard to be clearly defined. Since I am not going to discuss how to better define AI, here the AI refers to the systems or models that utilize modern machine learning techniques rather than the general AI as coined by \u003ca href=\"http://www-formal.stanford.edu/jmc/whatisai/node1.html\"\u003eJohn McCarthy\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe next question is, how we define \"explainable\"?\u003c/p\u003e\n\u003cp\u003eThe Merriam Webster Dictionary defines \"explain\" as \"to give the reason for or cause of\". Based on this definition, I roughly describe \"explainable AI\" as the AI that can provide the reason (of it's prediction/action) to a human so that the human can understand it.\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eNow that we have the definition (or description) of XAI, why we would want to have it? Isn't the black-boxed machine learning models already good enough? Why bother to make them explainable? Indeed, we don't quite need a digit recognizer to be able to explain the reason of its prediction, since we want it fully automatic without costing human labor. But we do have the need for critical situations where fully automatic systems are not available. In these situations, human users need to understand, appropriately trust, and effectively operate with the upcoming AIs (DARPA, 2016). For example, a doctor utilizing AI tools which helps him/her determine whether the patient has caught \u003ca href=\"https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\"\u003ecervical cancer\u003c/a\u003e will need the AI tools to be explainable. In such life-critical situations, explainability can help the doctor build trust on the system and correct wrong diagnosis if necessary, so that the doctor can boost efficiency while keeping the diagnosing quality. More applications with the need of XAI will be discussed below.\u003c/p\u003e\n\u003cp\u003eThe need for XAI, in my mind, results from the recent success of AI technology as well as its limitations. The new AI techniques are believed to be promising in many fields, including finance, hospital and medicine, education, entertainment, and transportation. However, these successful new techniques, including SVM, probabilistic graph model, random forest, deep learning and reinforcement learning, are difficult to interpret. That's why these models are sometimes criticized as black-boxes. In cases where AI are used as a collaborator with a human rather than a completely automatic solution, a monitoring system that can sufficiently explain the AI is crucial.\u003c/p\u003e\n\u003ch3 id=\"application\"\u003eApplication\u003c/h3\u003e\n\u003cp\u003eDoshi-Velez and Kim (2017) argued that not all ML systems require interpretability (or explainability), but explainations may highlight \"incompleteness\" in problem formulation. This incompleteness refers to the biases that we cannot quantified, and thus cannot be optimized. This reminds me of the famous saying of George Box: \"All models are wrong, but some are useful\". Considering that all models are built on the assumptions that we can not fully validate in the real world, explainability is needed when such incompleteness is not neglectable. A list of scenarios that need explainability is provided by Doshi-Velez and Kim (2017). A summarized list is as below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eScientific Understanding. This is for researchers and teachers. Researchers need to gain more understandings on models' behavior and mechanisms. Teachers can utilize explainable AI to help students get essential understanding on AI systems' behavior.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eSafety. In some real tasks, the ML systems are not always testable. Thus explainability is desirable for those who operate or monitor the system. Highly related industries includes military, finance and medical services.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eEthics. E.g. A \"fair\" classifier for loan approval. Sometimes, performance targeted end-to-end classifiers may be biased to gender and race, which is unfair (政治正确).\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMismatched objectives. Some times an ML system may be optimized for one objective (classifying possible engine failures) but used for broader scenarios (building better automatic cars).\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMulti-objective trade-offs. In real tasks, accuracy is not the only objective, we also want reliability, unbiasedness. There is still a lack of effective algorithms that can help optimizing such multi-objectives. Explainable systems can leave this tasks for human.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e"},{"source":"content/blog/reading.md","path":"/blog/reading/","url":"/blog/reading/","content":"\u003cp\u003eThis paper is written by Amershi, a researcher in MSA, who is kind of a leading researcher in the crossing area of ML + HCI.\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003cp\u003eThe main issue that the paper addressed is the important role of end-users in \u003cstrong\u003einteractive machine learning\u003c/strong\u003e, which is characterized by the \"rapid train-feedback-correct cycles\". The paper also summarized common problems that users introduced when applying interactive machine learning: * Users are not machines. Users may get frustrated if mechanically interact with the system (example: active learning); Users naturally don't want to be just data labelers. * Users have bias. For example, in reinforcement learning, the reward function provided by users are often positively biased, which caused the learner to pursue short-term reward. * Users value transparency in learning systems, and transparency can help users provide better labels.\u003c/p\u003e\n\u003ch3 id=\"my-takeaways\"\u003eMy takeaways:\u003c/h3\u003e\n\u003cp\u003eThe keys of a general interactive machine learning system: timing (feedback time), trust (transparency of the algorithms and the system), and user friendly interactions (avoid misleading and frustration).\u003c/p\u003e\n\u003ch3 id=\"discussions\"\u003eDiscussions:\u003c/h3\u003e\n\u003cp\u003eIf we say that \"Why should I trust you\" mainly emphasized end-users' need to understand and trust ML models, then this paper mainly emphasized end-users' need to efficiently develop ML models for their own needs.\u003c/p\u003e\n\u003cp\u003eInteractive machine learning is still a growing field, lacks widely accepted taxonomy and foundations. This also means that there is large space for collaboration across the fields of HCI/Vis and ML. HCI/Vis can leverage advances in ML to develop more powerful tools to users. ML also faces more practical issues brought by potential users and new opportunities to develop new frameworks that supports realistic assumptions about users.\u003c/p\u003e\n","frontmatter":{"title":"Power to the People - The role of Humans in Interactive Machine Learning","date":"2017-06-30T10:36:24.000Z","tags":"reading note, machine learning, research"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"A summary on Recurrent Neural Networks (RNN)","date":"2017-02-10T11:57:34.000Z","tags":"machine learning, RNN"},"path":"/blog/on-rnns/"},"next_page":{"frontmatter":{"title":"Explainable AI -- Definition, Motivation and Application","date":"2017-08-17T16:34:42.000Z","tags":"XAI, research"},"path":"/blog/Towards/"},"excerpt":"\u003cp\u003eThis paper is written by Amershi, a researcher in MSA, who is kind of a leading researcher in the crossing area of ML + HCI.\u003c/p\u003e","more":"\u003cp\u003eThe main issue that the paper addressed is the important role of end-users in \u003cstrong\u003einteractive machine learning\u003c/strong\u003e, which is characterized by the \"rapid train-feedback-correct cycles\". The paper also summarized common problems that users introduced when applying interactive machine learning: * Users are not machines. Users may get frustrated if mechanically interact with the system (example: active learning); Users naturally don't want to be just data labelers. * Users have bias. For example, in reinforcement learning, the reward function provided by users are often positively biased, which caused the learner to pursue short-term reward. * Users value transparency in learning systems, and transparency can help users provide better labels.\u003c/p\u003e\n\u003ch3 id=\"my-takeaways\"\u003eMy takeaways:\u003c/h3\u003e\n\u003cp\u003eThe keys of a general interactive machine learning system: timing (feedback time), trust (transparency of the algorithms and the system), and user friendly interactions (avoid misleading and frustration).\u003c/p\u003e\n\u003ch3 id=\"discussions\"\u003eDiscussions:\u003c/h3\u003e\n\u003cp\u003eIf we say that \"Why should I trust you\" mainly emphasized end-users' need to understand and trust ML models, then this paper mainly emphasized end-users' need to efficiently develop ML models for their own needs.\u003c/p\u003e\n\u003cp\u003eInteractive machine learning is still a growing field, lacks widely accepted taxonomy and foundations. This also means that there is large space for collaboration across the fields of HCI/Vis and ML. HCI/Vis can leverage advances in ML to develop more powerful tools to users. ML also faces more practical issues brought by potential users and new opportunities to develop new frameworks that supports realistic assumptions about users.\u003c/p\u003e"},{"source":"content/blog/on-rnns.md","path":"/blog/on-rnns/","url":"/blog/on-rnns/","content":"\u003cp\u003eI am working closely on RNNs these days, trying to reveal the ``black box'' and see what RNN learned to use its the hidden states and gates.\u003c/p\u003e\n\u003cp\u003eAfter intensively trying to do experiments, I suddenly realize that maybe first analyze them mathematically would give some clues for better visualization\u003c/p\u003e\n\u003cp\u003eThere are already many good articles introducing RNNs and its variants (LSTMs, GRU) on the internet right now. So this is just a post for myself to summarize things up on RNNs.\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eSo first, what is Recurrent Neural Network (RNN)?\u003c/p\u003e\n\u003cp\u003eIn short, RNN is a type of neural network that deal with sequence data. Classical neural networks, e.g. Multi-layer Perceptron (MLP) or Convolutional Neural Network (CNN) takes a fixed sized input an produce a fixed size output. Although for CNNs you can resize images of different size into a standard size so that the model can work with variable size input, but for the CNN part it still only accept fixed sized input. \u003ca id=\"more\"\u003e\u003c/a\u003e So RNN is a more flexible form of neural networks that kind of extends the computation power of neural networks. Indeed, some research has proofed that \u003ca href=\"http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf\"\u003eRNN is Turing-Complete\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"model-definition\"\u003eModel Definition\u003c/h1\u003e\n\u003cp\u003eNow, let's dive into the detail of how RNN works.\u003c/p\u003e\n\u003cp\u003eThe basic idea that differentiate RNN with CNN or Classical NN is that it introduce information and dependency of precious inputs when predicting output based on the current input. At each time step \u003cspan class=\"math inline\"\u003e\\(t\\)\u003c/span\u003e, given previous inputs \u003cspan class=\"math inline\"\u003e\\([x_0, \\cdots, x_{t-1}]\\)\u003c/span\u003e and current input \u003cspan class=\"math inline\"\u003e\\(x_t\\)\u003c/span\u003e, we can express the output of the model as conditional probability:\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[\nP(y_t \\mid x_t, x_{t-1}, \\cdots, x_0 ) = P (y_t \\mid x_t, i_t)\n\\]\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003ewhere \u003cspan class=\"math inline\"\u003e\\(i_t\\)\u003c/span\u003e is the representation of previous information learned by the model.\u003c/p\u003e\n\u003ch2 id=\"vanilla-rnn\"\u003eVanilla RNN\u003c/h2\u003e\n\u003cp\u003eThe simplest version of RNN model the previous information as a vector of \u003cstrong\u003ehidden states\u003c/strong\u003e,\u003c/p\u003e\n","frontmatter":{"title":"A summary on Recurrent Neural Networks (RNN)","date":"2017-02-10T11:57:34.000Z","tags":"machine learning, RNN"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"This is Getting Serious","date":"2017-01-11T19:18:17.000Z","tags":"random"},"path":"/blog/this-is-get-serious/"},"next_page":{"frontmatter":{"title":"Power to the People - The role of Humans in Interactive Machine Learning","date":"2017-06-30T10:36:24.000Z","tags":"reading note, machine learning, research"},"path":"/blog/reading/"},"excerpt":"\u003cp\u003eI am working closely on RNNs these days, trying to reveal the ``black box'' and see what RNN learned to use its the hidden states and gates.\u003c/p\u003e\n\u003cp\u003eAfter intensively trying to do experiments, I suddenly realize that maybe first analyze them mathematically would give some clues for better visualization\u003c/p\u003e\n\u003cp\u003eThere are already many good articles introducing RNNs and its variants (LSTMs, GRU) on the internet right now. So this is just a post for myself to summarize things up on RNNs.\u003c/p\u003e\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eSo first, what is Recurrent Neural Network (RNN)?\u003c/p\u003e\n\u003cp\u003eIn short, RNN is a type of neural network that deal with sequence data. Classical neural networks, e.g. Multi-layer Perceptron (MLP) or Convolutional Neural Network (CNN) takes a fixed sized input an produce a fixed size output. Although for CNNs you can resize images of different size into a standard size so that the model can work with variable size input, but for the CNN part it still only accept fixed sized input.","more":"So RNN is a more flexible form of neural networks that kind of extends the computation power of neural networks. Indeed, some research has proofed that \u003ca href=\"http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf\"\u003eRNN is Turing-Complete\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"model-definition\"\u003eModel Definition\u003c/h1\u003e\n\u003cp\u003eNow, let's dive into the detail of how RNN works.\u003c/p\u003e\n\u003cp\u003eThe basic idea that differentiate RNN with CNN or Classical NN is that it introduce information and dependency of precious inputs when predicting output based on the current input. At each time step \u003cspan class=\"math inline\"\u003e\\(t\\)\u003c/span\u003e, given previous inputs \u003cspan class=\"math inline\"\u003e\\([x_0, \\cdots, x_{t-1}]\\)\u003c/span\u003e and current input \u003cspan class=\"math inline\"\u003e\\(x_t\\)\u003c/span\u003e, we can express the output of the model as conditional probability:\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[\nP(y_t \\mid x_t, x_{t-1}, \\cdots, x_0 ) = P (y_t \\mid x_t, i_t)\n\\]\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003ewhere \u003cspan class=\"math inline\"\u003e\\(i_t\\)\u003c/span\u003e is the representation of previous information learned by the model.\u003c/p\u003e\n\u003ch2 id=\"vanilla-rnn\"\u003eVanilla RNN\u003c/h2\u003e\n\u003cp\u003eThe simplest version of RNN model the previous information as a vector of \u003cstrong\u003ehidden states\u003c/strong\u003e,\u003c/p\u003e"},{"source":"content/blog/this-is-get-serious.md","path":"/blog/this-is-get-serious/","url":"/blog/this-is-get-serious/","content":"\u003cp\u003eI am thinking of starting a blog for a long time. And this kind of unable-to-finish-my-goals feeling is actually driving me ill.\u003c/p\u003e\n\u003cp\u003eAnd here, this blog serves as a FLAG noting I am going to start blogging \"seriously\". Don't mock me for my strange English, I am trying to make it serious here!\u003c/p\u003e\n\u003cp\u003eAs for the stimulus that drives myself really starting this comes from Yuehan. Her suggestion of writing things down is actually working -- it kinds of saving my presentation today: Writing down things really helps me make sure I really understand what I want to presents. And it helps me clear my thoughts. People who writes logically and clearly must have the clear thoughts in mind, right?\u003c/p\u003e\n\u003cp\u003eI think I will keeps this habit from now on, for clearing my thoughts, recording my ideas, and exercising my English (Though I am not saying all of them will be in English :D).\u003c/p\u003e\n","frontmatter":{"title":"This is Getting Serious","date":"2017-01-11T19:18:17.000Z","tags":"random"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Tsinghua Jump","date":"2016-02-24T17:19:35.000Z","tags":"course project"},"path":"/blog/TsinghuaJump/"},"next_page":{"frontmatter":{"title":"A summary on Recurrent Neural Networks (RNN)","date":"2017-02-10T11:57:34.000Z","tags":"machine learning, RNN"},"path":"/blog/on-rnns/"},"excerpt":"","more":"\u003cp\u003eI am thinking of starting a blog for a long time. And this kind of unable-to-finish-my-goals feeling is actually driving me ill.\u003c/p\u003e\n\u003cp\u003eAnd here, this blog serves as a FLAG noting I am going to start blogging \"seriously\". Don't mock me for my strange English, I am trying to make it serious here!\u003c/p\u003e\n\u003cp\u003eAs for the stimulus that drives myself really starting this comes from Yuehan. Her suggestion of writing things down is actually working -- it kinds of saving my presentation today: Writing down things really helps me make sure I really understand what I want to presents. And it helps me clear my thoughts. People who writes logically and clearly must have the clear thoughts in mind, right?\u003c/p\u003e\n\u003cp\u003eI think I will keeps this habit from now on, for clearing my thoughts, recording my ideas, and exercising my English (Though I am not saying all of them will be in English :D).\u003c/p\u003e\n"},{"source":"content/blog/TsinghuaJump/index.md","path":"/blog/TsinghuaJump/","url":"/blog/TsinghuaJump/","content":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/szmingyao/THU_Jump\"\u003eTsinghua Jump\u003c/a\u003e is a doodle jump like game developed by our group Miaow. The game has similar rules as in Doodle Jump, but with many Tsinghua-flavored elements and interesting game items, which increased the game’s creational values. The game mainly has 2 game modes – Single Mode and Multi-Player Mode, a score-ranking system, and a game-item shop system.\u003c/p\u003e\n\u003cp\u003eThe game is developed and maintained via Github. Link for the repository is here. I am the group leader as well as the main contributor. It is still being developed and maintained by myself.\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003ch1 id=\"award\"\u003eAward\u003c/h1\u003e\n\u003cp\u003eI am so happy and proud to be informed that this project has been awarded the best 3 software projects in the Software Engineering Course: SE Fall 2015, \u003cspan class=\"citation\" data-cites=\"Tsinghua\"\u003e@Tsinghua\u003c/span\u003e.\u003c/p\u003e\n\u003ch1 id=\"screenshots\"\u003eScreenshots\u003c/h1\u003e\n\u003cp\u003eBelow are several screenshots for the game.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDemo:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJdemo.png\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMain Screen:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJmainscreen.png\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMulti-Player Mode:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJmultiplayer.png\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStart Screen:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJstartingscreen.png\" /\u003e\u003c/p\u003e\n","frontmatter":{"title":"Tsinghua Jump","date":"2016-02-24T17:19:35.000Z","tags":"course project"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Ray Tracing — a toy project of computer graphics course","date":"2016-01-23T17:09:51.000Z","tags":"course-project"},"path":"/blog/RayTracing/"},"next_page":{"frontmatter":{"title":"This is Getting Serious","date":"2017-01-11T19:18:17.000Z","tags":"random"},"path":"/blog/this-is-get-serious/"},"excerpt":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/szmingyao/THU_Jump\"\u003eTsinghua Jump\u003c/a\u003e is a doodle jump like game developed by our group Miaow. The game has similar rules as in Doodle Jump, but with many Tsinghua-flavored elements and interesting game items, which increased the game’s creational values. The game mainly has 2 game modes – Single Mode and Multi-Player Mode, a score-ranking system, and a game-item shop system.\u003c/p\u003e\n\u003cp\u003eThe game is developed and maintained via Github. Link for the repository is here. I am the group leader as well as the main contributor. It is still being developed and maintained by myself.\u003c/p\u003e","more":"\u003ch1 id=\"award\"\u003eAward\u003c/h1\u003e\n\u003cp\u003eI am so happy and proud to be informed that this project has been awarded the best 3 software projects in the Software Engineering Course: SE Fall 2015, \u003cspan class=\"citation\" data-cites=\"Tsinghua\"\u003e@Tsinghua\u003c/span\u003e.\u003c/p\u003e\n\u003ch1 id=\"screenshots\"\u003eScreenshots\u003c/h1\u003e\n\u003cp\u003eBelow are several screenshots for the game.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDemo:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJdemo.png\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMain Screen:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJmainscreen.png\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMulti-Player Mode:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJmultiplayer.png\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStart Screen:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/blog/TsinghuaJump/./TJstartingscreen.png\" /\u003e\u003c/p\u003e"},{"source":"content/blog/RayTracing/index.md","path":"/blog/RayTracing/","url":"/blog/RayTracing/","content":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eRay Tracing is a toy project for an undergraduate course: Advanced Computer Graphics in 2015 Fall at Tsinghua University. Ray Tracing is an implementation of classic ray tracing algorithm for photorealistic rendering. It is written in C++11, developed using vs2013 , with dependencies of openmp and opencv. You can view the repo \u003ca href=\"https://github.com/szmingyao/RayTracing\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003ch1 id=\"functions\"\u003eFunctions\u003c/h1\u003e\n\u003cp\u003eFunctions of the project include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePhong Model for local light computing.\u003c/li\u003e\n\u003cli\u003eGeometric ray intersection computing.\u003c/li\u003e\n\u003cli\u003eK-d tree.\u003c/li\u003e\n\u003cli\u003eBeer-Lambert for distance related luminance damping.\u003c/li\u003e\n\u003cli\u003eSoft shadow.\u003c/li\u003e\n\u003cli\u003eMesh Simplification.\u003c/li\u003e\n\u003cli\u003eUser-customized scene editing with simple script editing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSupported Model type: simple triangle, cube, sphere, infinite plane, 3D mesh(.obj).\u003c/p\u003e\n\u003ch1 id=\"demo\"\u003eDemo\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/blog/RayTracing/./RTdemo.png\" /\u003e\u003c/p\u003e\n","frontmatter":{"title":"Ray Tracing — a toy project of computer graphics course","date":"2016-01-23T17:09:51.000Z","tags":"course-project"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":{"frontmatter":{"title":"Animated Posters - 2013 Student Festival","date":"2016-01-22T00:00:00.000Z","category":"misc"},"path":"/blog/animated_poster/"},"next_page":{"frontmatter":{"title":"Tsinghua Jump","date":"2016-02-24T17:19:35.000Z","tags":"course project"},"path":"/blog/TsinghuaJump/"},"excerpt":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eRay Tracing is a toy project for an undergraduate course: Advanced Computer Graphics in 2015 Fall at Tsinghua University. Ray Tracing is an implementation of classic ray tracing algorithm for photorealistic rendering. It is written in C++11, developed using vs2013 , with dependencies of openmp and opencv. You can view the repo \u003ca href=\"https://github.com/szmingyao/RayTracing\"\u003ehere\u003c/a\u003e\u003c/p\u003e","more":"\u003ch1 id=\"functions\"\u003eFunctions\u003c/h1\u003e\n\u003cp\u003eFunctions of the project include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePhong Model for local light computing.\u003c/li\u003e\n\u003cli\u003eGeometric ray intersection computing.\u003c/li\u003e\n\u003cli\u003eK-d tree.\u003c/li\u003e\n\u003cli\u003eBeer-Lambert for distance related luminance damping.\u003c/li\u003e\n\u003cli\u003eSoft shadow.\u003c/li\u003e\n\u003cli\u003eMesh Simplification.\u003c/li\u003e\n\u003cli\u003eUser-customized scene editing with simple script editing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSupported Model type: simple triangle, cube, sphere, infinite plane, 3D mesh(.obj).\u003c/p\u003e\n\u003ch1 id=\"demo\"\u003eDemo\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/blog/RayTracing/./RTdemo.png\" /\u003e\u003c/p\u003e"},{"source":"content/blog/animated_poster/index.md","path":"/blog/animated_poster/","url":"/blog/animated_poster/","content":"\u003cp\u003eIn the 2013 winter term, I was in charge of the poster design of the student festival of our department. The student festival, \"砼年不同Young\", and the publicity of the festival have received a lot of attention on campus.\u003c/p\u003e\n\u003cp\u003eInspired by famous movies, including V for Revenge, Iron Man, and etc., I designed a list of six animated posters, which have received a lot of views on the Internet. The prototype of these designs is 砼仔 (concrete boy, pronounced [təŋ tsai]).\u003c/p\u003e\n\u003cp\u003eThe six posters are originally posted on \u003ca href=\"http://page.renren.com/601834083/channel-albumshow-946535872\"\u003eRenRen\u003c/a\u003e. The posters received a total of 500,000 views with more than 50,000 views for each poster.\u003c/p\u003e\n\u003cp\u003eGuokr.com also invited us to re-post our posters on \u003ca href=\"https://www.guokr.com/post/543545/\"\u003etheir site\u003c/a\u003e and posted them on weibo, where the posters received more than 6,000 re-post and 1,000 likes.\u003c/p\u003e\n\u003cp\u003eThe followings are the animating posters that I designed.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/1900.gif\" title=\"The Legend of 1900\" alt=\"The Legend of 1900\" /\u003e\u003cfigcaption\u003eThe Legend of 1900\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eThe Legend of 1900\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/v_for_construction.gif\" title=\"V for Construction\" alt=\"V for Construction\" /\u003e\u003cfigcaption\u003eV for Construction\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eV for Revenge\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/3_idiots.gif\" title=\"Three Idiots and Concrete\" alt=\"Three Idiots and Concrete\" /\u003e\u003cfigcaption\u003eThree Idiots and Concrete\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eThree Idiots\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/iron_man.gif\" title=\"钢筋侠\" alt=\"钢筋侠\" /\u003e\u003cfigcaption\u003e钢筋侠\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eIron Man\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/edward_sheerwall.gif\" title=\"Edward Sheerwall\" alt=\"Edward Sheerwall\" /\u003e\u003cfigcaption\u003eEdward Sheerwall\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eEdward Scissorhands\u003c/em\u003e.\u003c/p\u003e\n","frontmatter":{"title":"Animated Posters - 2013 Student Festival","date":"2016-01-22T00:00:00.000Z","category":"misc"},"rootType":"blog","layout":"blog","parent_path":"/blog/","prev_page":null,"next_page":{"frontmatter":{"title":"Ray Tracing — a toy project of computer graphics course","date":"2016-01-23T17:09:51.000Z","tags":"course-project"},"path":"/blog/RayTracing/"},"excerpt":"\u003cp\u003eIn the 2013 winter term, I was in charge of the poster design of the student festival of our department. The student festival, \"砼年不同Young\", and the publicity of the festival have received a lot of attention on campus.\u003c/p\u003e\n\u003cp\u003eInspired by famous movies, including V for Revenge, Iron Man, and etc., I designed a list of six animated posters, which have received a lot of views on the Internet. The prototype of these designs is 砼仔 (concrete boy, pronounced [təŋ tsai]).\u003c/p\u003e\n\u003cp\u003eThe six posters are originally posted on \u003ca href=\"http://page.renren.com/601834083/channel-albumshow-946535872\"\u003eRenRen\u003c/a\u003e. The posters received a total of 500,000 views with more than 50,000 views for each poster.\u003c/p\u003e\n\u003cp\u003eGuokr.com also invited us to re-post our posters on \u003ca href=\"https://www.guokr.com/post/543545/\"\u003etheir site\u003c/a\u003e and posted them on weibo, where the posters received more than 6,000 re-post and 1,000 likes.\u003c/p\u003e\n\u003cp\u003eThe followings are the animating posters that I designed.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/1900.gif\" title=\"The Legend of 1900\" alt=\"The Legend of 1900\" /\u003e\u003cfigcaption\u003eThe Legend of 1900\u003c/figcaption\u003e\n\u003c/figure\u003e","more":"\u003cp\u003eInspired by the movie \u003cem\u003eThe Legend of 1900\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/v_for_construction.gif\" title=\"V for Construction\" alt=\"V for Construction\" /\u003e\u003cfigcaption\u003eV for Construction\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eV for Revenge\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/3_idiots.gif\" title=\"Three Idiots and Concrete\" alt=\"Three Idiots and Concrete\" /\u003e\u003cfigcaption\u003eThree Idiots and Concrete\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eThree Idiots\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/iron_man.gif\" title=\"钢筋侠\" alt=\"钢筋侠\" /\u003e\u003cfigcaption\u003e钢筋侠\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eIron Man\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/blog/animated_poster/edward_sheerwall.gif\" title=\"Edward Sheerwall\" alt=\"Edward Sheerwall\" /\u003e\u003cfigcaption\u003eEdward Sheerwall\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003eInspired by the movie \u003cem\u003eEdward Scissorhands\u003c/em\u003e.\u003c/p\u003e"}],"parent_path":"/","prev_page":null,"next_page":null,"excerpt":"","more":"\n"},"layout":"blog","asPath":"/blog/","userAgent":"server"}},"page":"/_page","query":{"fullUrl":"/blog/","layout":"blog"},"buildId":"nhMVKDhmaXRt2P_S1C6tN","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/_page" src="/_next/static/nhMVKDhmaXRt2P_S1C6tN/pages/_page.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/nhMVKDhmaXRt2P_S1C6tN/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.8b1d108f40b66c4ca6b6.js" async=""></script><script src="/_next/static/chunks/styles.a38c7b919a261ed7c094.js" async=""></script><script src="/_next/static/runtime/main-7652b9adf9c4f1fdd86d.js" async=""></script></body></html>