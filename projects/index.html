<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><title class="next-head">Yao Ming | PhD in CSE | Researcher in XAI and VIS</title><meta charSet="utf-8" class="next-head"/><meta name="description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><meta property="og:title" class="next-head"/><meta property="og:description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:creator" content="Yao Ming" class="next-head"/><meta name="twitter:title" class="next-head"/><meta name="twitter:description" content="Yao Ming, Ph.D. in computer science and engineering at HKUST. My research interests are explainable and interpretable machine learning (or some called XAI), data visualization and visual analytics." class="next-head"/><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous" class="next-head"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous" class="next-head"></script><script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous" class="next-head"></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6" class="next-head"></script><script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" class="next-head"></script><link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i|Open+Sans" rel="stylesheet" class="next-head"/><style type="text/css" class="next-head">
        body {
          font-family: &#x27;Lato&#x27;, &#x27;Open Sans&#x27;, &#x27;Noto Sans SC&#x27;, &quot;Microsoft YaHei&quot;, sans-serif;
        }
      </style><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-90189261-1" class="next-head"></script><script class="next-head">window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag("js", new Date());

        gtag("config", "UA-90189261-1");</script><link rel="preload" href="/_next/static/0CJFptho6U289zpXUkwJ9/pages/_page.js" as="script"/><link rel="preload" href="/_next/static/0CJFptho6U289zpXUkwJ9/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.7f89317f958d91eef0e9.js" as="script"/><link rel="preload" href="/_next/static/chunks/styles.2471293ecff49027e156.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-7652b9adf9c4f1fdd86d.js" as="script"/><link rel="stylesheet" href="/_next/static/css/commons.fd000b21.chunk.css"/><link rel="stylesheet" href="/_next/static/css/styles.57322401.chunk.css"/><style id="__jsx-3606128814">.loader.jsx-3606128814{width:100%;height:100%;position:absolute;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background:#ffffff;z-index:1700;}.loader.jsx-3606128814 svg.jsx-3606128814{width:20%;height:20%;display:block;margin:auto;}.loader-show.jsx-3606128814{visibility:visible;-webkit-transition:opacity 300ms 300ms,visibility 0s 0s;transition:opacity 300ms 300ms,visibility 0s 0s;opacity:1;}.loader-hide.jsx-3606128814{visibility:hidden;-webkit-transition:opacity 300ms 300ms,visibility 0s 600ms;transition:opacity 300ms 300ms,visibility 0s 600ms;opacity:0;}</style></head><body><div id="__next"><div class="jsx-3606128814 loader loader-show"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" class="jsx-3606128814"><circle transform="translate(8 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle><circle transform="translate(16 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0.3" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle><circle transform="translate(24 0)" cx="0" cy="16" r="0" class="jsx-3606128814"><animate attributeName="r" values="0; 4; 0; 0" dur="1.2s" repeatCount="indefinite" begin="0.6" keyTimes="0;0.2;0.7;1" keySplines="0.2 0.2 0.4 0.8;0.2 0.6 0.4 0.8;0.2 0.6 0.4 0.8" calcMode="spline" class="jsx-3606128814"></animate></circle></svg></div><div style="display:none"><div><header id="m-header" class="fixed-top"><div class="m-header-container"><nav class="navbar navbar-expand-md navbar-light m-navbar"><span class="navbar-brand" href="#">Yao Ming <span style="font-size:14px">(明遥)</span></span><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto"><li class="nav-item"><a class="nav-link" data-nav-identifier="home" href="/"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" class="svg-inline--fa fa-home fa-w-18 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" style="margin-right:5px"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></i>Home</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="about" href="/#about"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="user-circle" class="svg-inline--fa fa-user-circle fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" style="margin-right:5px"><path fill="currentColor" d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"></path></svg></i>About</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="research" href="/#research"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="atom" class="svg-inline--fa fa-atom fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="margin-right:5px"><path fill="currentColor" d="M413.03 256c40.13-54.89 41.51-98.62 25.14-128-10.91-19.52-40.54-50.73-116.33-41.88C300.36 34.89 267.64 0 224 0s-76.36 34.89-97.84 86.12C50.43 77.34 20.73 108.48 9.83 128c-16.38 29.4-15 73.09 25.14 128-40.13 54.89-41.51 98.62-25.14 128 29.21 52.34 101.68 43.58 116.33 41.88C147.63 477.1 180.36 512 224 512s76.37-34.9 97.84-86.12c14.64 1.7 87.11 10.46 116.33-41.88 16.38-29.4 15-73.09-25.14-128zM63.38 352c-4.03-7.21-.19-24.8 14.95-48.29 6.96 6.53 14.2 12.89 21.87 19.18 1.71 13.71 4 27.08 6.76 40.08-24.56.89-39.89-4.37-43.58-10.97zm36.82-162.88c-7.66 6.29-14.9 12.65-21.87 19.18-15.13-23.5-18.97-41.09-14.95-48.3 3.41-6.14 16.39-11.47 37.92-11.47 1.71 0 3.87.3 5.69.37a472.191 472.191 0 0 0-6.79 40.22zM224 64c9.47 0 22.2 13.52 33.86 37.26-11.19 3.7-22.44 8-33.86 12.86-11.42-4.86-22.67-9.16-33.86-12.86C201.8 77.52 214.53 64 224 64zm0 384c-9.47 0-22.2-13.52-33.86-37.26 11.19-3.7 22.44-8 33.86-12.86 11.42 4.86 22.67 9.16 33.86 12.86C246.2 434.48 233.47 448 224 448zm62.5-157.33c-26.7 19.08-46.14 29.33-62.5 37.48-16.35-8.14-35.8-18.41-62.5-37.48-1.99-27.79-1.99-41.54 0-69.33 26.67-19.05 46.13-29.32 62.5-37.48 16.39 8.17 35.86 18.44 62.5 37.48 1.98 27.78 1.99 41.53 0 69.33zM384.62 352c-3.67 6.62-19 11.82-43.58 10.95 2.76-13 5.05-26.37 6.76-40.06 7.66-6.29 14.9-12.65 21.87-19.18 15.13 23.49 18.97 41.08 14.95 48.29zm-14.95-143.71c-6.96-6.53-14.2-12.89-21.87-19.18a473.535 473.535 0 0 0-6.79-40.22c1.82-.07 3.97-.37 5.69-.37 21.52 0 34.51 5.34 37.92 11.47 4.02 7.22.18 24.81-14.95 48.3zM224 224c-17.67 0-32 14.33-32 32s14.33 32 32 32 32-14.33 32-32-14.33-32-32-32z"></path></svg></i>Research</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="blog" href="/blog"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pencil-alt" class="svg-inline--fa fa-pencil-alt fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="margin-right:5px"><path fill="currentColor" d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></i>Blog</a></li><li class="nav-item"><a class="nav-link" data-nav-identifier="cv" href="/files/cv-yao.pdf">CV</a></li></ul></div></nav></div></header><main id="main" class="main"><div class="mb-4 container page-content" id="content"><div id="posts" class="post-wrapper"><article class="post" itemscope="" itemType="http://schema.org/Article" layout="project" tags="sequence learning, interpretable machine learning, xai, kdd"><header class="post-header"><h1><a href="/projects/prosenet/">ProSeNet: Interpretable and Steerable Sequence Learning via Prototypes</a></h1><div class="post-meta"><span><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2019-07-23</span></div></header><div class="post-body"><p style="text-align:center;">
Yao Ming, Panpan Xu, Huamin Qu, and Liu Ren
</p>
<p><img src="/projects/prosenet/./architecture.png" /></p>
<h2 id="a-brief-introduction">A Brief Introduction</h2>
<p>The <strong><em>interpretability</em></strong> of machine learning models is becoming increasingly important for crucial decision-making scenarios. It is especially challenging for deep learning models which consist of massive parameters and complicated architectures.</p>
<p><strong>ProSeNet</strong> (<strong>Pro</strong>totype <strong>Se</strong>quence <strong>Ne</strong>twork) is a sequence model that is <strong>interpretable</strong> while retaining the accuracy of sequence neural networks like RNN and LSTM.</p>
<p>ProSeNet is interpretable in the sense that its predictions are produced under a case-based reasoning framework, which naturally generates explanations by comparing the input to the typical cases. For each input sequence, the model computes its similarity with the <strong><em>prototype</em></strong> sequences that we learned form the training data, and computes the final prediction by consulting the most similar prototypes. For example, the prediction and explanation of a sentiment classifier for text based on ProSeNet would be something like:</p>
<p><img src="/projects/prosenet/./example-yelp.png" /></p>
<p>Here the numbers (0.69, 0.30) indicates the similarity between the inputs and the prototypes. You may find such a framework is similar to k-nearest neighbor models. And yes, the idea of the model originates in classical k-nearest neighbors and metrics learning!</p>
<h2 id="prosenet">ProSeNet</h2>
<p>The architecture of the model is illustrated as in the upmost figure. It uses an LSTM encoder <em>r</em> to map sequences to a fixed embedding space, and learns a set of <em>k</em> prototype vectors that are used as a basis for inference. The embedding of the sequence is compared with the prototype vectors and produces k similarity scores. Then a fully connected layer <em>f</em> is used to produce the final output. Here the weight of the layer <em>f</em> assigns the relation between the prototypes and the final classes.</p>
<p>However, the model is still not interpretable, cuz the prototypes are vectors in the embedding space! Thus, we use a projection technique to replace the prototype vectors by its closest embedding vector during training, which associates each prototype vector with a "real" readable sequence. For more details, please check our paper listed below.</p>
<h2 id="publication">Publication</h2>
<p>To appear in Proceedings of KDD 19. [<a href="https://arxiv.org/abs/1907.09728">preprint</a>]</p>
<h2 id="video-preview">Video Preview</h2>
<div class="video-container">
<iframe src="//www.youtube.com/embed/VSl33psbd1g" frameborder="0" allowfullscreen>
</iframe>
</div>
<h2 id="code">Code</h2>
<p>The code of ProSeNet will be hosted on github at <a href="https://github.com/myaooo/prosenet">here</a>.</p>
<p><strong>We are actively working on the code review to meet legal and copyright requirements of Bosch. The code will be release once the review is finished.</strong></p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>The major idea of the paper, and most of the experiments are done during Yao's internship at Bosch Research North America.</p>
</div><a href="/projects/prosenet/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" layout="project" tags="vast, classification, xai"><header class="post-header"><h1><a href="/projects/rule-matrix/">RuleMatrix: Visualizing and Understanding Classifiers using Rules</a></h1><div class="post-meta"><span><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2018-07-17</span></div></header><div class="post-body"><p style="text-align:center;">
Yao Ming, Huamin Qu, and Enrico Bertini
</p>
<hr />
<figure>
<img src="/projects/rule-matrix/./teaser.jpg" alt="&quot;teaser&quot;" /><figcaption>"teaser"</figcaption>
</figure>
<h2 id="summary">Summary</h2>
<p>RuleMatrix is an interactive visualization technique that helps users understand the input-output behavior of machine learning models. By viewing a classifier as a black box, we extract a standardized rule-based knowledge representation (a rule list) from its input-output behavior. The extracted rule list is presented as RuleMatrix, a matrix-based visualization, to help users navigate and verify the rules and the black-box model.</p>
<h2 id="software">Software</h2>
<p>The software RuleMatrix and its documentation can be found on <a href="https://github.com/rulematrix/rule-matrix-py">Github</a>.</p>
<h2 id="video-tutorial">Video Tutorial</h2>
<div class="video-container">
<iframe src="//www.youtube.com/embed/r3waU2AhWKE" frameborder="0" allowfullscreen>
</iframe>
</div>
<h2 id="publication">Publication</h2>
<p>Yao Ming, Huamin Qu, Enrico Bertini. <strong>RuleMatrix: Visualizing and Understanding Classifiers using Rules</strong>. <em>IEEE Transactions on Visualization and Computer Graphic, 2018 (to appear)</em>.</p>
<h2 id="materials">Materials</h2>
<p>[<a href="/projects/rule-matrix/./rule-matrix-preprint-jul17.pdf">preprint</a>] | [<a href="https://github.com/rulematrix/rule-matrix-py">code</a>]</p>
</div><a href="/projects/rule-matrix/">Read more »</a></article><article class="post" itemscope="" itemType="http://schema.org/Article" layout="project" tags="vast, rnn, xai" paper="[object Object]"><header class="post-header"><h1><a href="/projects/rnnvis/">RNNVis: Understanding Hidden Memories of Recurrent Neural Networks</a></h1><div class="post-meta"><span><i><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="calendar" class="svg-inline--fa fa-calendar fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M400 64h-48V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H160V12c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v52H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zm-6 400H54c-3.3 0-6-2.7-6-6V160h352v298c0 3.3-2.7 6-6 6z"></path></svg></i>Posted on <!-- -->2017-10-31</span></div></header><div class="post-body"><p style="text-align:center;">
Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, and Huamin Qu
</p>
<hr />
<figure>
<img src="/projects/rnnvis/./teaser-web.png" alt="&quot;teaser&quot;" /><figcaption>"teaser"</figcaption>
</figure>
<h2 id="summary">SUMMARY</h2>
<p>RNNVis is a visual analytics tool for understanding and comparing recurrent neural networks (RNNs) for text-based applications. The functions of hidden state units are explained using their expected response to the input texts (words). It allows users to gain more comprehensive understandings on the RNN's hidden mechanism through various visual techniques.</p>
<h2 id="publication">PUBLICATION</h2>
<p>To appear in Proceedings of VAST 17. [<a href="https://arxiv.org/pdf/1710.10777">preprint</a>]</p>
<h2 id="videos">VIDEOS</h2>
<p>[<strong>VIS17 Preview</strong>]</p>
<div class="video-container">
<iframe src="//player.vimeo.com/video/230830054" frameborder="0" allowfullscreen>
</iframe>
</div>
<p>[<strong>Introduction</strong>]</p>
<div class="video-container">
<iframe src="//www.youtube.com/embed/0QFDNLdQ6_w" frameborder="0" allowfullscreen>
</iframe>
</div>
<h2 id="code">CODE</h2>
<p>RNNVis is under <a href="https://github.com/myaooo/rnnvis">development</a>. A working demo can be found <a href="http://rnnvis.hkustvis.org">here</a>. If you have any comments or suggestions, feel free to open an issue.</p>
<h2 id="data">DATA</h2>
<p>This project has used the following dataset:</p>
<ul>
<li>Penn Tree Bank: [<a href="https://catalog.ldc.upenn.edu/ldc99t42">data source</a>] [<a href="http://dl.acm.org/citation.cfm?id=972475">paper</a>]</li>
<li>Yelp Data Challenge: [<a href="https://www.yelp.com/dataset/challenge">data source</a>]</li>
<li>A collection of Shakespeare’s work (used in supplement case study): [<a href="http://www.gutenberg.org/ebooks/100">data source</a>]</li>
</ul>
</div><a href="/projects/rnnvis/">Read more »</a></article></div></div></main><footer id="m-footer" class="bg-light py-2 d-print-none"><div class="container"><div class="row"><div class="col-auto mx-auto mr-sm-0 py-2"><ul class="socials" style="text-align:right"><li><a href="/#"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="font-size:13px"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></i></a></li><li><a href="https://github.com/myaooo"><i><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" style="font-size:13px"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></i></a></li><li><a href="https://scholar.google.com/citations?user=Fh0cwXUAAAAJ&amp;hl=en"><i><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="graduation-cap" class="svg-inline--fa fa-graduation-cap fa-w-20 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" style="font-size:13px"><path fill="currentColor" d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg></i></a></li><li><a href="https://www.linkedin.com/in/yao-ming/"><i><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin" class="svg-inline--fa fa-linkedin fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="font-size:13px"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></i></a></li></ul></div><div class="col-auto copyright mx-auto ml-sm-0 py-2 order-sm-first">Copyright ©<span itemProp="copyrightYear"> <!-- -->2015 - 2020<!-- --> </span><span class="author" itemProp="copyrightHolder">Yao Ming<!-- -->. </span><span> All Rights Reserved.</span></div></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"page":{"source":"content/projects/index.md","path":"/projects/","url":"/projects/","content":"\n","frontmatter":{"title":"Projects","date":"2016-08-19T22:53:32.000Z","layout":"projects","projects":[{"name":"xai","title":"Explainable Machine Learning","type":"research","publications":[{"name":"rnnvis"}]},{"name":"va","title":"Visual Analytics of Scocial Networks","type":"research","publications":[{"name":"mmorpg"}]}]},"rootType":"projects","layout":"projects","pages":[{"source":"content/projects/prosenet/index.md","path":"/projects/prosenet/","url":"/projects/prosenet/","content":"\u003cp style=\"text-align:center;\"\u003e\nYao Ming, Panpan Xu, Huamin Qu, and Liu Ren\n\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/projects/prosenet/./architecture.png\" /\u003e\u003c/p\u003e\n\u003ch2 id=\"a-brief-introduction\"\u003eA Brief Introduction\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003einterpretability\u003c/em\u003e\u003c/strong\u003e of machine learning models is becoming increasingly important for crucial decision-making scenarios. It is especially challenging for deep learning models which consist of massive parameters and complicated architectures.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProSeNet\u003c/strong\u003e (\u003cstrong\u003ePro\u003c/strong\u003etotype \u003cstrong\u003eSe\u003c/strong\u003equence \u003cstrong\u003eNe\u003c/strong\u003etwork) is a sequence model that is \u003cstrong\u003einterpretable\u003c/strong\u003e while retaining the accuracy of sequence neural networks like RNN and LSTM.\u003c/p\u003e\n\u003cp\u003eProSeNet is interpretable in the sense that its predictions are produced under a case-based reasoning framework, which naturally generates explanations by comparing the input to the typical cases. For each input sequence, the model computes its similarity with the \u003cstrong\u003e\u003cem\u003eprototype\u003c/em\u003e\u003c/strong\u003e sequences that we learned form the training data, and computes the final prediction by consulting the most similar prototypes. For example, the prediction and explanation of a sentiment classifier for text based on ProSeNet would be something like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/projects/prosenet/./example-yelp.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eHere the numbers (0.69, 0.30) indicates the similarity between the inputs and the prototypes. You may find such a framework is similar to k-nearest neighbor models. And yes, the idea of the model originates in classical k-nearest neighbors and metrics learning!\u003c/p\u003e\n\u003ch2 id=\"prosenet\"\u003eProSeNet\u003c/h2\u003e\n\u003cp\u003eThe architecture of the model is illustrated as in the upmost figure. It uses an LSTM encoder \u003cem\u003er\u003c/em\u003e to map sequences to a fixed embedding space, and learns a set of \u003cem\u003ek\u003c/em\u003e prototype vectors that are used as a basis for inference. The embedding of the sequence is compared with the prototype vectors and produces k similarity scores. Then a fully connected layer \u003cem\u003ef\u003c/em\u003e is used to produce the final output. Here the weight of the layer \u003cem\u003ef\u003c/em\u003e assigns the relation between the prototypes and the final classes.\u003c/p\u003e\n\u003cp\u003eHowever, the model is still not interpretable, cuz the prototypes are vectors in the embedding space! Thus, we use a projection technique to replace the prototype vectors by its closest embedding vector during training, which associates each prototype vector with a \"real\" readable sequence. For more details, please check our paper listed below.\u003c/p\u003e\n\u003ch2 id=\"publication\"\u003ePublication\u003c/h2\u003e\n\u003cp\u003eTo appear in Proceedings of KDD 19. [\u003ca href=\"https://arxiv.org/abs/1907.09728\"\u003epreprint\u003c/a\u003e]\u003c/p\u003e\n\u003ch2 id=\"video-preview\"\u003eVideo Preview\u003c/h2\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//www.youtube.com/embed/VSl33psbd1g\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003ch2 id=\"code\"\u003eCode\u003c/h2\u003e\n\u003cp\u003eThe code of ProSeNet will be hosted on github at \u003ca href=\"https://github.com/myaooo/prosenet\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWe are actively working on the code review to meet legal and copyright requirements of Bosch. The code will be release once the review is finished.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"acknowledgements\"\u003eAcknowledgements\u003c/h2\u003e\n\u003cp\u003eThe major idea of the paper, and most of the experiments are done during Yao's internship at Bosch Research North America.\u003c/p\u003e\n","frontmatter":{"layout":"project","title":"ProSeNet: Interpretable and Steerable Sequence Learning via Prototypes","date":"2019-07-23T23:40:38.000Z","tags":"sequence learning, interpretable machine learning, xai, kdd","has_page":true},"rootType":"projects","layout":"project","parent_path":"/projects/","prev_page":{"frontmatter":{"layout":"project","title":"RuleMatrix: Visualizing and Understanding Classifiers using Rules","date":"2018-07-17T12:40:38.000Z","tags":"vast, classification, xai","has_page":true},"path":"/projects/rule-matrix/"},"next_page":null,"excerpt":"","more":"\u003cp style=\"text-align:center;\"\u003e\nYao Ming, Panpan Xu, Huamin Qu, and Liu Ren\n\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/projects/prosenet/./architecture.png\" /\u003e\u003c/p\u003e\n\u003ch2 id=\"a-brief-introduction\"\u003eA Brief Introduction\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003einterpretability\u003c/em\u003e\u003c/strong\u003e of machine learning models is becoming increasingly important for crucial decision-making scenarios. It is especially challenging for deep learning models which consist of massive parameters and complicated architectures.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProSeNet\u003c/strong\u003e (\u003cstrong\u003ePro\u003c/strong\u003etotype \u003cstrong\u003eSe\u003c/strong\u003equence \u003cstrong\u003eNe\u003c/strong\u003etwork) is a sequence model that is \u003cstrong\u003einterpretable\u003c/strong\u003e while retaining the accuracy of sequence neural networks like RNN and LSTM.\u003c/p\u003e\n\u003cp\u003eProSeNet is interpretable in the sense that its predictions are produced under a case-based reasoning framework, which naturally generates explanations by comparing the input to the typical cases. For each input sequence, the model computes its similarity with the \u003cstrong\u003e\u003cem\u003eprototype\u003c/em\u003e\u003c/strong\u003e sequences that we learned form the training data, and computes the final prediction by consulting the most similar prototypes. For example, the prediction and explanation of a sentiment classifier for text based on ProSeNet would be something like:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/projects/prosenet/./example-yelp.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eHere the numbers (0.69, 0.30) indicates the similarity between the inputs and the prototypes. You may find such a framework is similar to k-nearest neighbor models. And yes, the idea of the model originates in classical k-nearest neighbors and metrics learning!\u003c/p\u003e\n\u003ch2 id=\"prosenet\"\u003eProSeNet\u003c/h2\u003e\n\u003cp\u003eThe architecture of the model is illustrated as in the upmost figure. It uses an LSTM encoder \u003cem\u003er\u003c/em\u003e to map sequences to a fixed embedding space, and learns a set of \u003cem\u003ek\u003c/em\u003e prototype vectors that are used as a basis for inference. The embedding of the sequence is compared with the prototype vectors and produces k similarity scores. Then a fully connected layer \u003cem\u003ef\u003c/em\u003e is used to produce the final output. Here the weight of the layer \u003cem\u003ef\u003c/em\u003e assigns the relation between the prototypes and the final classes.\u003c/p\u003e\n\u003cp\u003eHowever, the model is still not interpretable, cuz the prototypes are vectors in the embedding space! Thus, we use a projection technique to replace the prototype vectors by its closest embedding vector during training, which associates each prototype vector with a \"real\" readable sequence. For more details, please check our paper listed below.\u003c/p\u003e\n\u003ch2 id=\"publication\"\u003ePublication\u003c/h2\u003e\n\u003cp\u003eTo appear in Proceedings of KDD 19. [\u003ca href=\"https://arxiv.org/abs/1907.09728\"\u003epreprint\u003c/a\u003e]\u003c/p\u003e\n\u003ch2 id=\"video-preview\"\u003eVideo Preview\u003c/h2\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//www.youtube.com/embed/VSl33psbd1g\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003ch2 id=\"code\"\u003eCode\u003c/h2\u003e\n\u003cp\u003eThe code of ProSeNet will be hosted on github at \u003ca href=\"https://github.com/myaooo/prosenet\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWe are actively working on the code review to meet legal and copyright requirements of Bosch. The code will be release once the review is finished.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"acknowledgements\"\u003eAcknowledgements\u003c/h2\u003e\n\u003cp\u003eThe major idea of the paper, and most of the experiments are done during Yao's internship at Bosch Research North America.\u003c/p\u003e\n"},{"source":"content/projects/rule-matrix/index.md","path":"/projects/rule-matrix/","url":"/projects/rule-matrix/","content":"\u003cp style=\"text-align:center;\"\u003e\nYao Ming, Huamin Qu, and Enrico Bertini\n\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/projects/rule-matrix/./teaser.jpg\" alt=\"\u0026quot;teaser\u0026quot;\" /\u003e\u003cfigcaption\u003e\"teaser\"\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRuleMatrix is an interactive visualization technique that helps users understand the input-output behavior of machine learning models. By viewing a classifier as a black box, we extract a standardized rule-based knowledge representation (a rule list) from its input-output behavior. The extracted rule list is presented as RuleMatrix, a matrix-based visualization, to help users navigate and verify the rules and the black-box model.\u003c/p\u003e\n\u003ch2 id=\"software\"\u003eSoftware\u003c/h2\u003e\n\u003cp\u003eThe software RuleMatrix and its documentation can be found on \u003ca href=\"https://github.com/rulematrix/rule-matrix-py\"\u003eGithub\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"video-tutorial\"\u003eVideo Tutorial\u003c/h2\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//www.youtube.com/embed/r3waU2AhWKE\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003ch2 id=\"publication\"\u003ePublication\u003c/h2\u003e\n\u003cp\u003eYao Ming, Huamin Qu, Enrico Bertini. \u003cstrong\u003eRuleMatrix: Visualizing and Understanding Classifiers using Rules\u003c/strong\u003e. \u003cem\u003eIEEE Transactions on Visualization and Computer Graphic, 2018 (to appear)\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id=\"materials\"\u003eMaterials\u003c/h2\u003e\n\u003cp\u003e[\u003ca href=\"/projects/rule-matrix/./rule-matrix-preprint-jul17.pdf\"\u003epreprint\u003c/a\u003e] | [\u003ca href=\"https://github.com/rulematrix/rule-matrix-py\"\u003ecode\u003c/a\u003e]\u003c/p\u003e\n","frontmatter":{"layout":"project","title":"RuleMatrix: Visualizing and Understanding Classifiers using Rules","date":"2018-07-17T12:40:38.000Z","tags":"vast, classification, xai","has_page":true},"rootType":"projects","layout":"project","parent_path":"/projects/","prev_page":{"frontmatter":{"layout":"project","title":"RNNVis: Understanding Hidden Memories of Recurrent Neural Networks","date":"2017-10-31T12:40:38.000Z","tags":"vast, rnn, xai","has_page":true,"paper":{"title":"Understanding Hidden Memories of Recurrent Neural Networks","authors":["Yao Ming","Shaozu Cao","Ruixiang Zhang","Zhen Li","Yuanzhe Chen","Yangqiu Song","Huamin Qu"],"venue":{"tag":"VAST","name":"IEEE Conference on Visual Analytics Science and Technology","title":"Proceedings of IEEE Conference on Visual Analytics Science and Technology"},"year":2017,"category":"xai","resources":{"pdf":"https://arxiv.org/pdf/1710.10777","video":"https://www.youtube.com/watch?v=0QFDNLdQ6_w","slides":"/rnnvis/rnnvis-vis2017-slides.pdf","code":"https//github.com/myaooo/rnnvis"},"teaser":"/rnnvis/teaser-web.png","short":"RNNVis"}},"path":"/projects/rnnvis/"},"next_page":{"frontmatter":{"layout":"project","title":"ProSeNet: Interpretable and Steerable Sequence Learning via Prototypes","date":"2019-07-23T23:40:38.000Z","tags":"sequence learning, interpretable machine learning, xai, kdd","has_page":true},"path":"/projects/prosenet/"},"excerpt":"","more":"\u003cp style=\"text-align:center;\"\u003e\nYao Ming, Huamin Qu, and Enrico Bertini\n\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/projects/rule-matrix/./teaser.jpg\" alt=\"\u0026quot;teaser\u0026quot;\" /\u003e\u003cfigcaption\u003e\"teaser\"\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eRuleMatrix is an interactive visualization technique that helps users understand the input-output behavior of machine learning models. By viewing a classifier as a black box, we extract a standardized rule-based knowledge representation (a rule list) from its input-output behavior. The extracted rule list is presented as RuleMatrix, a matrix-based visualization, to help users navigate and verify the rules and the black-box model.\u003c/p\u003e\n\u003ch2 id=\"software\"\u003eSoftware\u003c/h2\u003e\n\u003cp\u003eThe software RuleMatrix and its documentation can be found on \u003ca href=\"https://github.com/rulematrix/rule-matrix-py\"\u003eGithub\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"video-tutorial\"\u003eVideo Tutorial\u003c/h2\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//www.youtube.com/embed/r3waU2AhWKE\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003ch2 id=\"publication\"\u003ePublication\u003c/h2\u003e\n\u003cp\u003eYao Ming, Huamin Qu, Enrico Bertini. \u003cstrong\u003eRuleMatrix: Visualizing and Understanding Classifiers using Rules\u003c/strong\u003e. \u003cem\u003eIEEE Transactions on Visualization and Computer Graphic, 2018 (to appear)\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id=\"materials\"\u003eMaterials\u003c/h2\u003e\n\u003cp\u003e[\u003ca href=\"/projects/rule-matrix/./rule-matrix-preprint-jul17.pdf\"\u003epreprint\u003c/a\u003e] | [\u003ca href=\"https://github.com/rulematrix/rule-matrix-py\"\u003ecode\u003c/a\u003e]\u003c/p\u003e\n"},{"source":"content/projects/rnnvis/index.md","path":"/projects/rnnvis/","url":"/projects/rnnvis/","content":"\u003cp style=\"text-align:center;\"\u003e\nYao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, and Huamin Qu\n\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/projects/rnnvis/./teaser-web.png\" alt=\"\u0026quot;teaser\u0026quot;\" /\u003e\u003cfigcaption\u003e\"teaser\"\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"summary\"\u003eSUMMARY\u003c/h2\u003e\n\u003cp\u003eRNNVis is a visual analytics tool for understanding and comparing recurrent neural networks (RNNs) for text-based applications. The functions of hidden state units are explained using their expected response to the input texts (words). It allows users to gain more comprehensive understandings on the RNN's hidden mechanism through various visual techniques.\u003c/p\u003e\n\u003ch2 id=\"publication\"\u003ePUBLICATION\u003c/h2\u003e\n\u003cp\u003eTo appear in Proceedings of VAST 17. [\u003ca href=\"https://arxiv.org/pdf/1710.10777\"\u003epreprint\u003c/a\u003e]\u003c/p\u003e\n\u003ch2 id=\"videos\"\u003eVIDEOS\u003c/h2\u003e\n\u003cp\u003e[\u003cstrong\u003eVIS17 Preview\u003c/strong\u003e]\u003c/p\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//player.vimeo.com/video/230830054\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003cp\u003e[\u003cstrong\u003eIntroduction\u003c/strong\u003e]\u003c/p\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//www.youtube.com/embed/0QFDNLdQ6_w\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003ch2 id=\"code\"\u003eCODE\u003c/h2\u003e\n\u003cp\u003eRNNVis is under \u003ca href=\"https://github.com/myaooo/rnnvis\"\u003edevelopment\u003c/a\u003e. A working demo can be found \u003ca href=\"http://rnnvis.hkustvis.org\"\u003ehere\u003c/a\u003e. If you have any comments or suggestions, feel free to open an issue.\u003c/p\u003e\n\u003ch2 id=\"data\"\u003eDATA\u003c/h2\u003e\n\u003cp\u003eThis project has used the following dataset:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePenn Tree Bank: [\u003ca href=\"https://catalog.ldc.upenn.edu/ldc99t42\"\u003edata source\u003c/a\u003e] [\u003ca href=\"http://dl.acm.org/citation.cfm?id=972475\"\u003epaper\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003eYelp Data Challenge: [\u003ca href=\"https://www.yelp.com/dataset/challenge\"\u003edata source\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003eA collection of Shakespeare’s work (used in supplement case study): [\u003ca href=\"http://www.gutenberg.org/ebooks/100\"\u003edata source\u003c/a\u003e]\u003c/li\u003e\n\u003c/ul\u003e\n","frontmatter":{"layout":"project","title":"RNNVis: Understanding Hidden Memories of Recurrent Neural Networks","date":"2017-10-31T12:40:38.000Z","tags":"vast, rnn, xai","has_page":true,"paper":{"title":"Understanding Hidden Memories of Recurrent Neural Networks","authors":["Yao Ming","Shaozu Cao","Ruixiang Zhang","Zhen Li","Yuanzhe Chen","Yangqiu Song","Huamin Qu"],"venue":{"tag":"VAST","name":"IEEE Conference on Visual Analytics Science and Technology","title":"Proceedings of IEEE Conference on Visual Analytics Science and Technology"},"year":2017,"category":"xai","resources":{"pdf":"https://arxiv.org/pdf/1710.10777","video":"https://www.youtube.com/watch?v=0QFDNLdQ6_w","slides":"/rnnvis/rnnvis-vis2017-slides.pdf","code":"https//github.com/myaooo/rnnvis"},"teaser":"/rnnvis/teaser-web.png","short":"RNNVis"}},"rootType":"projects","layout":"project","parent_path":"/projects/","prev_page":null,"next_page":{"frontmatter":{"layout":"project","title":"RuleMatrix: Visualizing and Understanding Classifiers using Rules","date":"2018-07-17T12:40:38.000Z","tags":"vast, classification, xai","has_page":true},"path":"/projects/rule-matrix/"},"excerpt":"","more":"\u003cp style=\"text-align:center;\"\u003e\nYao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, and Huamin Qu\n\u003c/p\u003e\n\u003chr /\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/projects/rnnvis/./teaser-web.png\" alt=\"\u0026quot;teaser\u0026quot;\" /\u003e\u003cfigcaption\u003e\"teaser\"\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2 id=\"summary\"\u003eSUMMARY\u003c/h2\u003e\n\u003cp\u003eRNNVis is a visual analytics tool for understanding and comparing recurrent neural networks (RNNs) for text-based applications. The functions of hidden state units are explained using their expected response to the input texts (words). It allows users to gain more comprehensive understandings on the RNN's hidden mechanism through various visual techniques.\u003c/p\u003e\n\u003ch2 id=\"publication\"\u003ePUBLICATION\u003c/h2\u003e\n\u003cp\u003eTo appear in Proceedings of VAST 17. [\u003ca href=\"https://arxiv.org/pdf/1710.10777\"\u003epreprint\u003c/a\u003e]\u003c/p\u003e\n\u003ch2 id=\"videos\"\u003eVIDEOS\u003c/h2\u003e\n\u003cp\u003e[\u003cstrong\u003eVIS17 Preview\u003c/strong\u003e]\u003c/p\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//player.vimeo.com/video/230830054\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003cp\u003e[\u003cstrong\u003eIntroduction\u003c/strong\u003e]\u003c/p\u003e\n\u003cdiv class=\"video-container\"\u003e\n\u003ciframe src=\"//www.youtube.com/embed/0QFDNLdQ6_w\" frameborder=\"0\" allowfullscreen\u003e\n\u003c/iframe\u003e\n\u003c/div\u003e\n\u003ch2 id=\"code\"\u003eCODE\u003c/h2\u003e\n\u003cp\u003eRNNVis is under \u003ca href=\"https://github.com/myaooo/rnnvis\"\u003edevelopment\u003c/a\u003e. A working demo can be found \u003ca href=\"http://rnnvis.hkustvis.org\"\u003ehere\u003c/a\u003e. If you have any comments or suggestions, feel free to open an issue.\u003c/p\u003e\n\u003ch2 id=\"data\"\u003eDATA\u003c/h2\u003e\n\u003cp\u003eThis project has used the following dataset:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePenn Tree Bank: [\u003ca href=\"https://catalog.ldc.upenn.edu/ldc99t42\"\u003edata source\u003c/a\u003e] [\u003ca href=\"http://dl.acm.org/citation.cfm?id=972475\"\u003epaper\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003eYelp Data Challenge: [\u003ca href=\"https://www.yelp.com/dataset/challenge\"\u003edata source\u003c/a\u003e]\u003c/li\u003e\n\u003cli\u003eA collection of Shakespeare’s work (used in supplement case study): [\u003ca href=\"http://www.gutenberg.org/ebooks/100\"\u003edata source\u003c/a\u003e]\u003c/li\u003e\n\u003c/ul\u003e\n"}],"parent_path":"/","prev_page":null,"next_page":null,"excerpt":"","more":"\n"},"layout":"projects","asPath":"/projects/","userAgent":"server"}},"page":"/_page","query":{"fullUrl":"/projects/","layout":"projects"},"buildId":"0CJFptho6U289zpXUkwJ9","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/_page" src="/_next/static/0CJFptho6U289zpXUkwJ9/pages/_page.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/0CJFptho6U289zpXUkwJ9/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.7f89317f958d91eef0e9.js" async=""></script><script src="/_next/static/chunks/styles.2471293ecff49027e156.js" async=""></script><script src="/_next/static/runtime/main-7652b9adf9c4f1fdd86d.js" async=""></script></body></html>